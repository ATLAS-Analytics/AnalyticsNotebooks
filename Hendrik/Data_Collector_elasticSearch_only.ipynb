{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import sys\n",
    "import dateutil\n",
    "import zipfile\n",
    "es = Elasticsearch(['atlas-kibana.mwt2.org:9200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is legacy code for collect_and_clean_Scroll and collect_and_cleanRawData_RealTime\n",
    "# Newer functions directly give back the data and don't store it globaly\n",
    "raw_data_pool = {}\n",
    "\n",
    "def make_sure(src_site, dest_site):\n",
    "    if src_site not in raw_data_pool:\n",
    "        raw_data_pool[src_site] = {}\n",
    "    if dest_site not in raw_data_pool[src_site]:\n",
    "        raw_data_pool[src_site][dest_site] = pd.DataFrame()\n",
    "\n",
    "def put_data(src_site, dest_site, timestamp_epoch, column_type, value):\n",
    "    make_sure(src_site, dest_site)\n",
    "    raw_data_pool[src_site][dest_site].set_value(timestamp_epoch, column_type, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define search functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function collects data from elastic search \n",
    "def collect_and_clean_Scroll(es, src_site, dest_site, timeframe):\n",
    "    print(\"--- Starting data collection for {} to {}\".format(src_site, dest_site))\n",
    "    es_query = \\\n",
    "    {\n",
    "        \"fielddata_fields\":[\n",
    "            \"timestamp\"\n",
    "        ],\n",
    "        \"query\": {\n",
    "            \"filtered\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\":[\n",
    "                            { 'term': { 'srcSite': src_site } },\n",
    "                            { 'term': { 'destSite': dest_site } },\n",
    "                            { 'range': { 'timestamp': timeframe } }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es_index = \"network_weather_2-*\"\n",
    "    \n",
    "    # Collect latency raw data\n",
    "    event_type = 'latency'\n",
    "    scroll = list(helpers.scan(client=es, query=es_query, index=es_index, doc_type=event_type, request_timeout=6000))\n",
    "    count = 0\n",
    "    for dataPoint in scroll:\n",
    "        count += 1\n",
    "        timestamp_epoch = dataPoint['fields']['timestamp'][0]\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'delay_median', dataPoint['_source']['delay_median'])\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'delay_mean', dataPoint['_source']['delay_mean'])\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'delay_sd', dataPoint['_source']['delay_sd'])\n",
    "    print('Number of raw records of {} is {}'.format(event_type, count))\n",
    "    \n",
    "    # Collect packet_loss_rate raw data\n",
    "    event_type = 'packet_loss_rate'\n",
    "    scroll = list(helpers.scan(client=es, query=es_query, index=es_index, doc_type=event_type, request_timeout=6000))\n",
    "    count = 0\n",
    "    for dataPoint in scroll:\n",
    "        count += 1\n",
    "        timestamp_epoch = dataPoint['fields']['timestamp'][0]\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'packet_loss', dataPoint['_source']['packet_loss'])\n",
    "    print('Number of raw records of {} is {}'.format(event_type, count))\n",
    "    \n",
    "    # Collect throughput raw data\n",
    "    event_type = 'throughput'\n",
    "    scroll = list(helpers.scan(client=es, query=es_query, index=es_index, doc_type=event_type, request_timeout=6000))\n",
    "    count = 0\n",
    "    for dataPoint in scroll:\n",
    "        count += 1\n",
    "        timestamp_epoch = dataPoint['fields']['timestamp'][0]\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "        put_data(src_site, dest_site, timestamp_epoch, 'throughput', dataPoint['_source']['throughput'])\n",
    "    print('Number of raw records of {} is {}'.format(event_type, count))\n",
    "    \n",
    "    # View statistics of raw_data_pool (already de-duplicated)\n",
    "    print()\n",
    "    print('De-duplication result:')\n",
    "    print(raw_data_pool[src_site][dest_site].count(axis='index'))\n",
    "    \n",
    "    # Sort in-place\n",
    "    raw_data_pool[src_site][dest_site].sort_index(inplace=True, ascending=False)\n",
    "    \n",
    "    # Store this DataFrame to disk file\n",
    "    raw_data_pool[src_site][dest_site].to_pickle('raw_data/raw_data_from_{}_to_{}_and_time_{}_to_{}_sorted.pkl'.format(src_site, dest_site, timeframe[\"gte\"], timeframe[\"lt\"]))\n",
    "    \n",
    "    # How to get this DataFrame\n",
    "    # raw_data_pool[src_site][dest_site]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# this method should only be used for \"small\" timeframes containig less than 10.000 results\n",
    "# otherwise it will crash\n",
    "# a good cache size could be 1000\n",
    "def collect_and_cleanRawData_RealTime(es, src_site, dest_site, timeframe, cache_size):\n",
    "    print(\"--- Starting data collection for {} to {}\".format(src_site, dest_site))\n",
    "    es_sorting = \"timestamp:desc\"\n",
    "    es_size = cache_size\n",
    "    es_query = \\\n",
    "    {\n",
    "        \"query\": {\n",
    "            \"filtered\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\":[\n",
    "                            { 'term': { 'srcSite': src_site } },\n",
    "                            { 'term': { 'destSite': dest_site } },\n",
    "                            { 'range': { 'timestamp': timeframe } }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es_index = \"network_weather_2-*\"\n",
    "    \n",
    "    # Collect latency raw data\n",
    "    print(\"--- Collectiong latency data\")\n",
    "    event_type = 'latency'\n",
    "    es_from = 0\n",
    "    response = es.search(index=es_index, body=es_query, size=10, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "    numDataPoints = response['hits']['total']\n",
    "    while (es_from <= numDataPoints):\n",
    "        response = es.search(index=es_index, body=es_query, size=es_size, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "        for dataPoint in response['hits']['hits']:\n",
    "            timestamp_epoch = dataPoint['_source']['timestamp']\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'delay_median', dataPoint['_source']['delay_median'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'delay_mean', dataPoint['_source']['delay_mean'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'delay_sd', dataPoint['_source']['delay_sd'])\n",
    "        es_from = es_from + es_size\n",
    "    print('Number of raw records of {} is {}'.format(event_type, numDataPoints))\n",
    "        \n",
    "    # Collect packet_loss_rate raw data\n",
    "    print(\"--- Collectiong packet_loss_rate data\")\n",
    "    event_type = 'packet_loss_rate'\n",
    "    es_from = 0\n",
    "    response = es.search(index=es_index, body=es_query, size=10, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "    numDataPoints = response['hits']['total']\n",
    "    while (es_from <= numDataPoints):\n",
    "        response = es.search(index=es_index, body=es_query, size=es_size, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "        for dataPoint in response['hits']['hits']:\n",
    "            timestamp_epoch = dataPoint['_source']['timestamp']\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'packet_loss', dataPoint['_source']['packet_loss'])\n",
    "        es_from = es_from + es_size\n",
    "    print('Number of raw records of {} is {}'.format(event_type, numDataPoints))\n",
    "    \n",
    "    # Collect throughput raw data\n",
    "    print(\"--- Collectiong throughput data\")\n",
    "    event_type = 'throughput'\n",
    "    es_from = 0\n",
    "    response = es.search(index=es_index, body=es_query, size=10, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "    numDataPoints = response['hits']['total']\n",
    "    while (es_from <= numDataPoints):\n",
    "        response = es.search(index=es_index, body=es_query, size=es_size, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "        for dataPoint in response['hits']['hits']:\n",
    "            timestamp_epoch = dataPoint['_source']['timestamp']\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'throughput', dataPoint['_source']['throughput'])\n",
    "        es_from = es_from + es_size\n",
    "    \n",
    "    print('Number of raw records of {} is {}'.format(event_type, numDataPoints))\n",
    "    # View statistics of raw_data_pool (already de-duplicated)\n",
    "    print(\"\")\n",
    "    print('De-duplication result:')\n",
    "    print(raw_data_pool[src_site][dest_site].count(axis='index'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_and_clean_byIP_Scroll(es, src_data, dest_data, timeframe, saveToDisk = True):\n",
    "    '''\n",
    "    This will collect raw data from elastic search for the connection from a source, to a given destination.\n",
    "    Within the given timeframe.\n",
    "    The resulting data is returned and by can be as well wirtten to disk (default).\n",
    "    '''\n",
    "    print(\"--- Starting data collection for {} to {}, in the time of {} to {}\".format(\n",
    "            src_data['name'], dest_data['name'], timeframe[\"gte\"], timeframe[\"lt\"]))\n",
    "    data = pd.DataFrame()\n",
    "    es_query = \\\n",
    "    {\n",
    "        \"query\": {\n",
    "            \"filtered\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\":[\n",
    "                            { 'term': { 'src': src_data[\"latency\"] } },\n",
    "                            { 'term': { 'dest': dest_data[\"latency\"] } },\n",
    "                            { 'range': { 'timestamp': timeframe } }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es_index = \"network_weather_2-*\"\n",
    "    \n",
    "    # Collect latency raw data\n",
    "    event_type = 'latency'\n",
    "    scroll = list(helpers.scan(client=es, query=es_query, index=es_index, doc_type=event_type, request_timeout=6000))\n",
    "    count = 0\n",
    "    for dataPoint in scroll:\n",
    "        count += 1\n",
    "        time = dateutil.parser.parse(dataPoint['_source']['timestamp'])\n",
    "        timestamp_epoch = calendar.timegm(time.timetuple())\n",
    "        data.set_value(timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "        data.set_value(timestamp_epoch, 'delay_median', dataPoint['_source']['delay_median'])\n",
    "        data.set_value(timestamp_epoch, 'delay_mean', dataPoint['_source']['delay_mean'])\n",
    "        data.set_value(timestamp_epoch, 'delay_sd', dataPoint['_source']['delay_sd'])\n",
    "    print('Number of raw records of {} is {}'.format(event_type, count))\n",
    "    \n",
    "    # Collect packet_loss_rate raw data\n",
    "    event_type = 'packet_loss_rate'\n",
    "    scroll = list(helpers.scan(client=es, query=es_query, index=es_index, doc_type=event_type, request_timeout=6000))\n",
    "    count = 0\n",
    "    for dataPoint in scroll:\n",
    "        count += 1\n",
    "        time = dateutil.parser.parse(dataPoint['_source']['timestamp'])\n",
    "        timestamp_epoch = calendar.timegm(time.timetuple())\n",
    "        data.set_value(timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "        data.set_value(timestamp_epoch, 'packet_loss', dataPoint['_source']['packet_loss'])\n",
    "    print('Number of raw records of {} is {}'.format(event_type, count))\n",
    "    \n",
    "    # rebuilding our query\n",
    "    es_query = \\\n",
    "    {\n",
    "        \"query\": {\n",
    "            \"filtered\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\":[\n",
    "                            { 'term': { 'src': src_data[\"throughput\"] } },\n",
    "                            { 'term': { 'dest': dest_data[\"throughput\"] } },\n",
    "                            { 'range': { 'timestamp': timeframe } }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Collect throughput raw data\n",
    "    event_type = 'throughput'\n",
    "    scroll = list(helpers.scan(client=es, query=es_query, index=es_index, doc_type=event_type, request_timeout=6000))\n",
    "    count = 0\n",
    "    for dataPoint in scroll:\n",
    "        count += 1\n",
    "        time = dateutil.parser.parse(dataPoint['_source']['timestamp'])\n",
    "        timestamp_epoch = calendar.timegm(time.timetuple())\n",
    "        data.set_value(timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "        data.set_value(timestamp_epoch, 'throughput', dataPoint['_source']['throughput'])\n",
    "    print('Number of raw records of {} is {}'.format(event_type, count))\n",
    "    print(\" \")\n",
    "    \n",
    "    # View statistics of raw_data_pool (already de-duplicated)\n",
    "    print()\n",
    "    print('De-duplication result:')\n",
    "    print(data.count(axis='index'))\n",
    "    \n",
    "    # Sort in-place\n",
    "    data.sort_index(inplace=True, ascending=False)\n",
    "    \n",
    "    # Store this DataFrame to disk file\n",
    "    if saveToDisk:\n",
    "        print(\"Saving results to disk.\")\n",
    "        print(\"\")\n",
    "        data.to_pickle('raw_data/raw_data_by_IP_from_{}_to_{}_and_time_{}_to_{}_sorted.pkl'.format(src_data['name'], dest_data['name'], timeframe[\"gte\"], timeframe[\"lt\"]))\n",
    "    \n",
    "    # return the data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def starlikeConnector(es, center_point_data, out_points_list, timeframe, saveToDisk=saveToDisk):\n",
    "    for dest_data in out_points_list:\n",
    "        collect_and_clean_byIP_Scroll(es, center_point_data, dest_data, timeframe, saveToDisk=saveToDisk)\n",
    "        collect_and_clean_byIP_Scroll(es, dest_data, center_point_data, timeframe, saveToDisk=saveToDisk)\n",
    "\n",
    "        \n",
    "def zipFolder(path=\"raw_data/\"):\n",
    "    zipf = zipfile.ZipFile('folder_zipped.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file))\n",
    "    zipf.close()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting data collection for CERN to RAL, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 18810\n",
      "Number of raw records of packet_loss_rate is 18908\n",
      "Number of raw records of throughput is 661\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        13164\n",
      "delay_median    12314\n",
      "delay_mean      12314\n",
      "delay_sd        12314\n",
      "packet_loss     12345\n",
      "throughput        639\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for RAL to CERN, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 17954\n",
      "Number of raw records of packet_loss_rate is 17906\n",
      "Number of raw records of throughput is 665\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        12521\n",
      "delay_median    11688\n",
      "delay_mean      11688\n",
      "delay_sd        11688\n",
      "packet_loss     11704\n",
      "throughput        646\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for CERN to PIC, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 16611\n",
      "Number of raw records of packet_loss_rate is 16315\n",
      "Number of raw records of throughput is 1249\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        13469\n",
      "delay_median    12093\n",
      "delay_mean      12093\n",
      "delay_sd        12093\n",
      "packet_loss     12123\n",
      "throughput       1175\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for PIC to CERN, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 17159\n",
      "Number of raw records of packet_loss_rate is 17127\n",
      "Number of raw records of throughput is 1244\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        13391\n",
      "delay_median    11989\n",
      "delay_mean      11989\n",
      "delay_sd        11989\n",
      "packet_loss     12020\n",
      "throughput       1183\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for CERN to TRIUMF, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 16408\n",
      "Number of raw records of packet_loss_rate is 16019\n",
      "Number of raw records of throughput is 699\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        12095\n",
      "delay_median    11298\n",
      "delay_mean      11298\n",
      "delay_sd        11298\n",
      "packet_loss     11256\n",
      "throughput        676\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for TRIUMF to CERN, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 16140\n",
      "Number of raw records of packet_loss_rate is 16089\n",
      "Number of raw records of throughput is 704\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        12166\n",
      "delay_median    11365\n",
      "delay_mean      11365\n",
      "delay_sd        11365\n",
      "packet_loss     11335\n",
      "throughput        671\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for CERN to BNL, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 17408\n",
      "Number of raw records of packet_loss_rate is 17070\n",
      "Number of raw records of throughput is 693\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        13282\n",
      "delay_median    12390\n",
      "delay_mean      12390\n",
      "delay_sd        12390\n",
      "packet_loss     12417\n",
      "throughput        681\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for BNL to CERN, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 16765\n",
      "Number of raw records of packet_loss_rate is 16817\n",
      "Number of raw records of throughput is 1838\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        12622\n",
      "delay_median    11792\n",
      "delay_mean      11792\n",
      "delay_sd        11792\n",
      "packet_loss     11815\n",
      "throughput        650\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for CERN to KIT, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 17429\n",
      "Number of raw records of packet_loss_rate is 17231\n",
      "Number of raw records of throughput is 1362\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        13940\n",
      "delay_median    12436\n",
      "delay_mean      12436\n",
      "delay_sd        12436\n",
      "packet_loss     12452\n",
      "throughput       1303\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n",
      "--- Starting data collection for KIT to CERN, in the time of 2016-05-01 to 2016-09-06\n",
      "Number of raw records of latency is 16953\n",
      "Number of raw records of packet_loss_rate is 16964\n",
      "Number of raw records of throughput is 1364\n",
      " \n",
      "\n",
      "De-duplication result:\n",
      "iso_8601        13808\n",
      "delay_median    12317\n",
      "delay_mean      12317\n",
      "delay_sd        12317\n",
      "packet_loss     12352\n",
      "throughput       1279\n",
      "dtype: int64\n",
      "Saving results to disk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: extract traceroute data, which is in the throughput <-> throughput connections\n",
    "# it is typed as: \"traceroute\"\n",
    "CERN_data = {'name': \"CERN\", \"latency\": \"128.142.223.247\", \"throughput\": \"128.142.223.246\"}\n",
    "RAL_data = {'name': \"RAL\", \"latency\": \"130.246.176.109\", \"throughput\": \"130.246.176.110\"}\n",
    "PIC_data = {'name': \"PIC\", \"latency\": \"193.109.172.188\", \"throughput\": \"193.109.172.187\"}\n",
    "TRIUMF_data = {'name': \"TRIUMF\", \"latency\": \"206.12.9.2\", \"throughput\": \"206.12.9.1\"}\n",
    "BNL_data = {'name': \"BNL\", \"latency\": \"192.12.15.26\", \"throughput\": \"192.12.15.23\"}\n",
    "KIT_data = {'name': \"KIT\", \"latency\": \"192.108.47.12\", \"throughput\": \"192.108.47.6\"}\n",
    "\n",
    "out_points_list = [RAL_data, PIC_data, TRIUMF_data, BNL_data, KIT_data]\n",
    "\n",
    "\n",
    "\n",
    "timeframe = { 'gte': '2016-05-01', 'lt': '2016-09-06' }\n",
    "starlikeConnector(es, CERN_data, out_points_list, timeframe, saveToDisk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ipykernel_py2)",
   "language": "python",
   "name": "ipykernel_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
