{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import sys\n",
    "import dateutil\n",
    "import zipfile\n",
    "es = Elasticsearch(['atlas-kibana.mwt2.org:9200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets get the latest packetloss and latency data\n",
    "# it seems that elastic search is not really close to realtime, but rather a few hours behind?\n",
    "# Or maybe the timezone for the timestamps is off?\n",
    "\n",
    "CERN_data = {'name': \"CERN\", \"latency\": \"128.142.223.247\", \"throughput\": \"128.142.223.246\"}\n",
    "TRIUMF_data = {'name': \"TRIUMF\", \"latency\": \"206.12.9.2\", \"throughput\": \"206.12.9.1\"}\n",
    "src_data = CERN_data\n",
    "dest_data = TRIUMF_data\n",
    "data = pd.DataFrame()\n",
    "response_size = 10\n",
    "es_sorting = \"timestamp:desc\"\n",
    "es_query_forth = \\\n",
    "{\n",
    "    \"query\": {\n",
    "        \"filtered\": {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"bool\": {\n",
    "                    \"must\":[\n",
    "                        { 'term': { 'src': src_data[\"latency\"] } },\n",
    "                        { 'term': { 'dest': dest_data[\"latency\"] } },\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es_query_back = \\\n",
    "{\n",
    "    \"query\": {\n",
    "        \"filtered\": {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"bool\": {\n",
    "                    \"must\":[\n",
    "                        { 'term': { 'src': src_data[\"latency\"] } },\n",
    "                        { 'term': { 'dest': dest_data[\"latency\"] } },\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es_index = \"network_weather_2-*\"\n",
    "\n",
    "# Collect all needed data raw data\n",
    "event_type = 'latency'\n",
    "response_latency_forth = es.search(index=es_index, body=es_query_forth, size=response_size, sort=es_sorting, doc_type = event_type, request_timeout=10)\n",
    "response_latency_back = es.search(index=es_index, body=es_query_back, size=response_size, sort=es_sorting, doc_type = event_type, request_timeout=10)\n",
    "event_type = 'packet_loss_rate'\n",
    "response_packetloss = es.search(index=es_index, body=es_query_forth, size=response_size, sort=es_sorting, doc_type = event_type, request_timeout=10)\n",
    "\n",
    "# analyze data\n",
    "for dataPoint in response_latency_forth['hits']['hits']:\n",
    "    time = dateutil.parser.parse(dataPoint['_source']['timestamp'])\n",
    "    timestamp_epoch = calendar.timegm(time.timetuple())\n",
    "    data.set_value(timestamp_epoch, 'delay_mean_forth', dataPoint['_source']['delay_mean'])\n",
    "    \n",
    "for dataPoint in response_latency_back['hits']['hits']:\n",
    "    time = dateutil.parser.parse(dataPoint['_source']['timestamp'])\n",
    "    timestamp_epoch = calendar.timegm(time.timetuple())\n",
    "    data.set_value(timestamp_epoch, 'delay_mean_back', dataPoint['_source']['delay_mean'])\n",
    "    \n",
    "for dataPoint in response_packetloss['hits']['hits']:\n",
    "    time = dateutil.parser.parse(dataPoint['_source']['timestamp'])\n",
    "    timestamp_epoch = calendar.timegm(time.timetuple())\n",
    "    data.set_value(timestamp_epoch, 'packet_loss', dataPoint['_source']['packet_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delay_mean_forth</th>\n",
       "      <th>delay_mean_back</th>\n",
       "      <th>packet_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1473174900</th>\n",
       "      <td>76.425722</td>\n",
       "      <td>76.425722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473174600</th>\n",
       "      <td>76.668233</td>\n",
       "      <td>76.668233</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473174300</th>\n",
       "      <td>76.636967</td>\n",
       "      <td>76.636967</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473174000</th>\n",
       "      <td>76.596900</td>\n",
       "      <td>76.596900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473173700</th>\n",
       "      <td>76.537067</td>\n",
       "      <td>76.537067</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473173400</th>\n",
       "      <td>76.427667</td>\n",
       "      <td>76.427667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473173100</th>\n",
       "      <td>76.418000</td>\n",
       "      <td>76.418000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473172800</th>\n",
       "      <td>76.722333</td>\n",
       "      <td>76.722333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473172500</th>\n",
       "      <td>76.364400</td>\n",
       "      <td>76.364400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473172200</th>\n",
       "      <td>76.398200</td>\n",
       "      <td>76.398200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            delay_mean_forth  delay_mean_back  packet_loss\n",
       "1473174900         76.425722        76.425722          0.0\n",
       "1473174600         76.668233        76.668233          0.0\n",
       "1473174300         76.636967        76.636967          0.0\n",
       "1473174000         76.596900        76.596900          0.0\n",
       "1473173700         76.537067        76.537067          0.0\n",
       "1473173400         76.427667        76.427667          0.0\n",
       "1473173100         76.418000        76.418000          0.0\n",
       "1473172800         76.722333        76.722333          0.0\n",
       "1473172500         76.364400        76.364400          0.0\n",
       "1473172200         76.398200        76.398200          0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this method should only be used for \"small\" timeframes containig less than 10.000 results\n",
    "# otherwise it will crash\n",
    "# a good cache size could be 1000\n",
    "def collect_and_cleanRawData_RealTime(es, src_site, dest_site, timeframe, cache_size):\n",
    "    print(\"--- Starting data collection for {} to {}\".format(src_site, dest_site))\n",
    "    es_sorting = \"timestamp:desc\"\n",
    "    es_size = cache_size\n",
    "    es_query = \\\n",
    "    {\n",
    "        \"query\": {\n",
    "            \"filtered\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\":[\n",
    "                            { 'term': { 'srcSite': src_site } },\n",
    "                            { 'term': { 'destSite': dest_site } },\n",
    "                            { 'range': { 'timestamp': timeframe } }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es_index = \"network_weather_2-*\"\n",
    "    \n",
    "    # Collect latency raw data\n",
    "    print(\"--- Collectiong latency data\")\n",
    "    event_type = 'latency'\n",
    "    es_from = 0\n",
    "    response = es.search(index=es_index, body=es_query, size=10, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "    numDataPoints = response['hits']['total']\n",
    "    while (es_from <= numDataPoints):\n",
    "        response = es.search(index=es_index, body=es_query, size=es_size, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "        for dataPoint in response['hits']['hits']:\n",
    "            timestamp_epoch = dataPoint['_source']['timestamp']\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'delay_median', dataPoint['_source']['delay_median'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'delay_mean', dataPoint['_source']['delay_mean'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'delay_sd', dataPoint['_source']['delay_sd'])\n",
    "        es_from = es_from + es_size\n",
    "    print('Number of raw records of {} is {}'.format(event_type, numDataPoints))\n",
    "        \n",
    "    # Collect packet_loss_rate raw data\n",
    "    print(\"--- Collectiong packet_loss_rate data\")\n",
    "    event_type = 'packet_loss_rate'\n",
    "    es_from = 0\n",
    "    response = es.search(index=es_index, body=es_query, size=10, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "    numDataPoints = response['hits']['total']\n",
    "    while (es_from <= numDataPoints):\n",
    "        response = es.search(index=es_index, body=es_query, size=es_size, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "        for dataPoint in response['hits']['hits']:\n",
    "            timestamp_epoch = dataPoint['_source']['timestamp']\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'packet_loss', dataPoint['_source']['packet_loss'])\n",
    "        es_from = es_from + es_size\n",
    "    print('Number of raw records of {} is {}'.format(event_type, numDataPoints))\n",
    "    \n",
    "    # Collect throughput raw data\n",
    "    print(\"--- Collectiong throughput data\")\n",
    "    event_type = 'throughput'\n",
    "    es_from = 0\n",
    "    response = es.search(index=es_index, body=es_query, size=10, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "    numDataPoints = response['hits']['total']\n",
    "    while (es_from <= numDataPoints):\n",
    "        response = es.search(index=es_index, body=es_query, size=es_size, from_=es_from, sort=es_sorting, doc_type = event_type, request_timeout=600)\n",
    "        for dataPoint in response['hits']['hits']:\n",
    "            timestamp_epoch = dataPoint['_source']['timestamp']\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'iso_8601', dataPoint['_source']['timestamp'])\n",
    "            put_data(src_site, dest_site, timestamp_epoch, 'throughput', dataPoint['_source']['throughput'])\n",
    "        es_from = es_from + es_size\n",
    "    \n",
    "    print('Number of raw records of {} is {}'.format(event_type, numDataPoints))\n",
    "    # View statistics of raw_data_pool (already de-duplicated)\n",
    "    print(\"\")\n",
    "    print('De-duplication result:')\n",
    "    print(raw_data_pool[src_site][dest_site].count(axis='index'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ipykernel_py2)",
   "language": "python",
   "name": "ipykernel_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
