{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pprint\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from fractions import Fraction \n",
    "import scipy.odr.odrpack as odrpack\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "import calendar\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for the import of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# smir out data\n",
    "def smoothDataFrame(data, steps):\n",
    "    # make sure that there are no nans! This would brick the cumsum function!\n",
    "    # This may happen naturally when the packet_loss is at 1, so no packets get through\n",
    "    # for simplicity we will drop those lines    \n",
    "    data_smired = data.copy(deep=True)\n",
    "    if \"packet_loss\" in data_smired.axes[1]:\n",
    "        data_smired = data_smired.dropna(axis=0, subset=[\"delay_avg\"])\n",
    "    for col in data_smired.axes[1]:\n",
    "        # do NOT smir our \"throughput_perfSonar\"\n",
    "        if col == \"throughput_perfSonar\":\n",
    "            continue\n",
    "        vals = data_smired[col].values\n",
    "        valsSumed = np.cumsum(vals)\n",
    "        for i in range(steps, len(data_smired)):\n",
    "            summ = valsSumed[i] - valsSumed[i-steps]\n",
    "            vals[i] = summ/steps\n",
    "        vals[:steps] = float('nan')\n",
    "        data_smired[col] = vals\n",
    "    # make sure not to use the first values, which were not averaged, e.g. delete them\n",
    "    data_smired = data_smired.drop(data_smired.index[:steps])\n",
    "    return data_smired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import raw data and compute dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read our raw data\n",
    "data_router = pd.read_pickle(\"raw_data_routers/raw_router_CERN_to_RAL-PIC-TRIUMF-KIT-IN2P3-BNL-CNAF-JINR-T1_highres.pkl\")\n",
    "data_perfSonar = pd.read_pickle(\"raw_data_routers/raw_toolkit_CERN_to_RAL-PIC-TRIUMF-KIT-IN2P3-BNL-CNAF-JINR-T1_sorted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "# smooth data\n",
    "steps = 3\n",
    "data_router = smoothDataFrame(data_router, steps)\n",
    "steps = 15\n",
    "for key1 in data_perfSonar.keys():\n",
    "    for key2 in data_perfSonar[key1]:\n",
    "        for key3 in data_perfSonar[key1][key2]:\n",
    "            # smir the data\n",
    "            data_perfSonar[key1][key2][key3] = smoothDataFrame(data_perfSonar[key1][key2][key3], steps)\n",
    "            # add the gradient for delay_avg and packetloss\n",
    "            data_perfSonar[key1][key2][key3]['delay_avg_gradient'] = np.gradient(data_perfSonar[key1][key2][key3]['delay_avg'], edge_order=2)\n",
    "            data_perfSonar[key1][key2][key3]['packet_loss_gradient'] = np.gradient(data_perfSonar[key1][key2][key3]['packet_loss'], edge_order=2)\n",
    "       \n",
    "# make dates for the plots\n",
    "dates_router = []\n",
    "for stamp in data_router.index:\n",
    "    dates_router.append(dt.datetime.fromtimestamp(stamp))\n",
    "    \n",
    "dates_perfSonar = {}\n",
    "for src in data_perfSonar:\n",
    "    dates_perfSonar[src] = {}\n",
    "    for dest in data_perfSonar[src]:\n",
    "        dates_perfSonar[src][dest] = {}\n",
    "        for direction in data_perfSonar[src][dest]:\n",
    "            dates_perfSonar[src][dest][direction] = []\n",
    "            for stamp in data_perfSonar[src][dest][direction].index:\n",
    "                dates_perfSonar[src][dest][direction].append(dt.datetime.fromtimestamp(stamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "CERN <-> TRIUMF | JINR-T1 | CCIN2P3 | CNAF | PIC | RAL | BNL | KIT |  \n",
      "Index(['packet_loss', 'ttl_avg', 'ttl_std', 'ttl_median',\n",
      "       'time_error_estimates', 'delay_avg', 'delay_std', 'delay_median',\n",
      "       'throughput_perfSonar', 'delay_avg_gradient', 'packet_loss_gradient'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#test printing\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(data_perfSonar['CERN']['RAL']['in']['throughput_perfSonar'])):\n",
    "    if math.isnan(data_perfSonar['CERN']['RAL']['in']['throughput_perfSonar'].iloc[i]):\n",
    "        continue\n",
    "    counter += 1\n",
    "print(counter)\n",
    "\n",
    "for key1 in data_perfSonar.keys():\n",
    "    print(key1 + \" <-> \", end=\"\")\n",
    "    for key2 in data_perfSonar[key1].keys():\n",
    "        print(key2 + \" | \", end=\"\")\n",
    "    print(\" \")\n",
    "\n",
    "print(data_perfSonar['CERN']['KIT']['out'].axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Do NN learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIndexClosestToTimestamp_mod(timestamp, dataPoints, collumThatIsNotNAN):\n",
    "    '''\n",
    "    Returns a data point from dataPoints, that is closest to the given timestamp.\n",
    "    As well the dataPoint will not be nan in the collum \"collumThatIsNotNAN\"\n",
    "    Complexity of this command: O( log( len(dataPoints) ) )\n",
    "    '''\n",
    "    timestamps_toSearch = dataPoints.axes[0]\n",
    "    cols = list(dataPoints.axes[1])\n",
    "    data = dataPoints.values\n",
    "    # bisect only works because we know our list is sorted\n",
    "    pos = bisect_left(timestamps_toSearch, timestamp)\n",
    "    if pos == len(dataPoints):\n",
    "        pos = len(dataPoints)-1\n",
    "    #print(pos)\n",
    "    # find the closest datapoint, that is not a NAN and return it\n",
    "    currentDataPoint = dict(zip(cols, dataPoints.values[pos]))\n",
    "    if not math.isnan(currentDataPoint[collumThatIsNotNAN]):\n",
    "        return pos\n",
    "    else:\n",
    "        pos = findIndexClostestThatIsNotNAN(pos, dataPoints, cols.index(collumThatIsNotNAN))\n",
    "        return pos\n",
    "\n",
    "def makeNewDataSet_NN(data_from_routers, data_from_perfSonar, timeframe, num_of_last_measurements=10, cut_throughput=0.7, throughput_max=(20*1e9)):\n",
    "    # bisect doesn't work if the data is not sortet this way...\n",
    "    data_from_routers = data_from_routers.sort_index(ascending=True).copy(deep=True)\n",
    "    data_from_perfSonar = data_from_perfSonar.sort_index(ascending=True).copy(deep=True)\n",
    "    # cut routers data\n",
    "    pos1 = bisect_left(data_from_routers.index, timeframe[0])\n",
    "    if pos1 == len(data_from_routers):\n",
    "        pos1 = len(data_from_routers)-1\n",
    "    pos2 = bisect_left(data_from_routers.index, timeframe[1])\n",
    "    if pos2 == len(data_from_routers):\n",
    "        pos2 = len(data_from_routers)-1\n",
    "    data_from_routers = data_from_routers.drop(data_from_routers.index[0:pos1])\n",
    "    data_from_routers = data_from_routers.drop(data_from_routers.index[pos2:len(data_from_routers)-1])\n",
    "    data = []\n",
    "    delay_all_time_avg = data_from_perfSonar['delay_avg'].mean()\n",
    "    delay_all_time_std = data_from_perfSonar['delay_avg'].std()\n",
    "    delay_all_time_min = data_from_perfSonar['delay_avg'].min()\n",
    "    loss_all_time_avg = data_from_perfSonar['packet_loss'].mean()\n",
    "    loss_all_time_std = data_from_perfSonar['packet_loss'].std()\n",
    "    for i in range(len(data_from_routers)):\n",
    "        if math.isnan(data_from_routers.values[i]) or data_from_routers.values[i] == 0.0:\n",
    "            continue\n",
    "        currentTimestamp = data_from_routers.index[i]\n",
    "        index_perfSonar = findIndexClosestToTimestamp_mod(currentTimestamp, data_from_perfSonar, 'delay_avg')\n",
    "        zw = dict(data_from_perfSonar.iloc[index_perfSonar])\n",
    "        # include all time data\n",
    "        zw['delay_all_time_avg'] = delay_all_time_avg\n",
    "        zw['delay_all_time_std'] = delay_all_time_std\n",
    "        zw['delay_all_time_min'] = delay_all_time_min\n",
    "        zw['loss_all_time_avg'] = loss_all_time_avg\n",
    "        zw['loss_all_time_std'] = loss_all_time_std\n",
    "        # add last measurements\n",
    "        if index_perfSonar < num_of_last_measurements:\n",
    "            continue\n",
    "        sumdelay = 0\n",
    "        sumdelay_square = 0\n",
    "        sumloss = 0\n",
    "        sumloss_square = 0\n",
    "        for j in range(1, num_of_last_measurements+1):\n",
    "            dataPoint = data_from_perfSonar.iloc[index_perfSonar-j]\n",
    "            zw['delay_avg_past_'+str(j)] = dataPoint['delay_avg']\n",
    "            sumdelay += dataPoint['delay_avg']\n",
    "            sumdelay_square += dataPoint['delay_avg']*dataPoint['delay_avg']\n",
    "            zw['packet_loss_past_'+str(j)] = dataPoint['packet_loss']\n",
    "            sumloss += dataPoint['packet_loss']\n",
    "            sumloss_square += dataPoint['packet_loss']*dataPoint['packet_loss']\n",
    "        zw['delay_avg_past_avg'] = sumdelay/num_of_last_measurements\n",
    "        zw['packet_loss_past_avg'] = sumloss/num_of_last_measurements\n",
    "        zw['delay_avg_past_std'] = np.sqrt((sumdelay_square/num_of_last_measurements)-(sumdelay/num_of_last_measurements)**2)\n",
    "        zw['packet_loss_past_std'] = np.sqrt((sumloss_square/num_of_last_measurements)-(sumloss/num_of_last_measurements)**2)\n",
    "        if math.isnan(zw['packet_loss_past_std']):\n",
    "            zw['packet_loss_past_std'] = 0\n",
    "        if math.isnan(zw['delay_avg_past_std']):\n",
    "            zw['delay_avg_past_std'] = 0\n",
    "        # add timestamp and throughput\n",
    "        zw['timestamp'] = currentTimestamp\n",
    "        zw['throughput'] = data_from_routers.values[i]\n",
    "        if zw['throughput'] <= cut_throughput*throughput_max:\n",
    "            zw['throughput'] = 0\n",
    "        # pop deprecated data\n",
    "        zw.pop('ttl_median', None)\n",
    "        zw.pop('ttl_avg', None)\n",
    "        zw.pop('ttl_std', None)\n",
    "        zw.pop('time_error_estimates', None)\n",
    "        data.append(zw)\n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    return data\n",
    "\n",
    "def normalizeDataSet(dataSet, scaler_feature_range=(0, 1), scaler=None):\n",
    "    # make sure we don't change our original object (wo don't want that)\n",
    "    # use the sktlearn.preprocessing procedures, they prommis to be quite flexible\n",
    "    workDataSet = dataSet.copy(deep=True)\n",
    "    # append zeroes so that we are actually normalized to zero\n",
    "    #zeroesDataFrame = pd.DataFrame(np.zeros([1,len(workDataSet.axes[1])]), columns=workDataSet.axes[1])\n",
    "    #workDataSet = workDataSet.append(zeroesDataFrame, ignore_index=True)\n",
    "    # fill up our nans as the function dosen't run with nans \n",
    "    workDataSet = workDataSet.fillna(0)\n",
    "    collums = []\n",
    "    for collum in workDataSet.axes[1]:\n",
    "        # what to skip:\n",
    "        if collum == \"timestamp\" or collum == \"throughput\" or collum == \"throughput_perfSonar\":\n",
    "            continue\n",
    "        collums.append(collum)\n",
    "    # if you want to keep the scaler, then split this into a fit and a transform call\n",
    "    if scaler == None:\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=scaler_feature_range).fit(workDataSet[collums].as_matrix())\n",
    "    workDataSet[collums] = scaler.transform(workDataSet[collums].as_matrix())\n",
    "    # remove zeroes again\n",
    "    #workDataSet = workDataSet.drop(len(workDataSet)-1, axis=0)\n",
    "    return workDataSet, scaler\n",
    "\n",
    "def inverseNormalizeDataSet(dataSet, scaler):\n",
    "    # make sure we don't change our original object (wo don't want that)\n",
    "    # use the sktlearn.preprocessing procedures, they prommis to be quite flexible\n",
    "    workDataSet = dataSet.copy(deep=True)\n",
    "    collums = []\n",
    "    for collum in workDataSet.axes[1]:\n",
    "        # what to skip:\n",
    "        if collum == \"timestamp\" or collum == \"throughput\":\n",
    "            continue\n",
    "        collums.append(collum)\n",
    "    workDataSet[collums] = scaler.inverse_transform(workDataSet[collums].as_matrix())\n",
    "    # remove zeroes again\n",
    "    #workDataSet = workDataSet.drop(len(workDataSet)-1, axis=0)\n",
    "    return workDataSet\n",
    "\n",
    "def normalize_throughput(throughput_array, throughputMaxGBits):\n",
    "    zw = throughput_array\n",
    "    zw = throughput_array/(throughputMaxGBits*1e9)\n",
    "    zw = (zw*0.8)+0.1\n",
    "    return zw\n",
    "\n",
    "def inverses_normalize_throughput(throughput_array, throughputMaxGBits):\n",
    "    zw = throughput_array\n",
    "    zw = (zw-0.1)/0.8\n",
    "    zw = zw * (throughputMaxGBits*1e9)\n",
    "    return zw\n",
    "\n",
    "def make_all_data_sets_for_NN(timeframe_ext, num_of_last_measurements, cut_throughput, throughput_mapping):\n",
    "    # set up stuff\n",
    "    timeframe = [calendar.timegm(dt.datetime.strptime(t, '%d-%m-%Y %H:%M').timetuple()) for t in timeframe_ext]\n",
    "    # get all directions\n",
    "    data_set = pd.DataFrame()\n",
    "\n",
    "    for src in data_perfSonar.keys():\n",
    "        print(src)\n",
    "        for dest in data_perfSonar[src].keys():\n",
    "            print(dest)\n",
    "            # skip JINR-T1 it has super strange router data...\n",
    "            # as well skip RAL, they have a baaaad router\n",
    "            if dest == \"JINR-T1\" or dest == \"RAL\":\n",
    "                continue\n",
    "            #if dest == \"JINR-T1\":\n",
    "            #    continue\n",
    "            throughputMaxGBits = throughput_mapping[dest]\n",
    "            # make dataset for direction: in\n",
    "            direction = \"out\"\n",
    "            print(\"Creating data_set: \"+src+\" and \"+dest+\" dir: \"+direction)\n",
    "            throughputs = []\n",
    "            for router in data_router.axes[1]:\n",
    "                if dest in router and direction in router[len(router)-3:]:\n",
    "                    throughputs.append(data_router[router])\n",
    "            data_from_routers = sum(throughputs).copy(deep=True)\n",
    "            data_from_perfSonar = data_perfSonar[src][dest][direction].copy(deep=True)\n",
    "            data_set_tmp = makeNewDataSet_NN(data_from_routers, data_from_perfSonar,\n",
    "                             timeframe, num_of_last_measurements=num_of_last_measurements,\n",
    "                            cut_throughput=cut_throughput, throughput_max=(throughputMaxGBits*1e9))\n",
    "            # normalize throughput\n",
    "            data_set_tmp['throughput'] = normalize_throughput(data_set_tmp['throughput'], throughputMaxGBits)\n",
    "            # append data_set\n",
    "            data_set = data_set.append(data_set_tmp, ignore_index=True)\n",
    "            # make dataset for direction: out\n",
    "            # basically do the same\n",
    "            continue\n",
    "            direction = \"in\"\n",
    "            print(\"Creating data_set: \"+src+\" and \"+dest+\" dir: \"+direction)\n",
    "            throughputs = []\n",
    "            for router in data_router.axes[1]:\n",
    "                if dest in router and direction in router[len(router)-3:]:\n",
    "                    throughputs.append(data_router[router])\n",
    "            data_from_routers = sum(throughputs).copy(deep=True)\n",
    "            data_from_perfSonar = data_perfSonar[src][dest][direction].copy(deep=True)\n",
    "            data_set_tmp = makeNewDataSet_NN(data_from_routers, data_from_perfSonar,\n",
    "                             timeframe, num_of_last_measurements=num_of_last_measurements,\n",
    "                            cut_throughput=cut_throughput, throughput_max=(throughputMaxGBits*1e9))\n",
    "            data_set_tmp['throughput'] = normalize_throughput(data_set_tmp['throughput'], throughputMaxGBits)\n",
    "            data_set = data_set.append(data_set_tmp, ignore_index=True)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN\n",
      "TRIUMF\n",
      "Creating data_set: CERN and TRIUMF dir: out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivukotic/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:72: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JINR-T1\n",
      "CCIN2P3\n",
      "Creating data_set: CERN and CCIN2P3 dir: out\n",
      "CNAF\n",
      "Creating data_set: CERN and CNAF dir: out\n",
      "PIC\n",
      "Creating data_set: CERN and PIC dir: out\n",
      "RAL\n",
      "BNL\n",
      "Creating data_set: CERN and BNL dir: out\n",
      "KIT\n",
      "Creating data_set: CERN and KIT dir: out\n",
      "25633\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_Set_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-78cf18d94443>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata_set_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_all_data_sets_for_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeframe_readable_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_last_measurements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_throughput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthroughput_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdata_Set_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Ravin/hendrik_data_set.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_Set_all' is not defined"
     ]
    }
   ],
   "source": [
    "# make dataset\n",
    "throughput_mapping = {\"JINR-T1\": 20, \"RAL\": 20, \"TRIUMF\": 10, \"CNAF\": 40, \"KIT\": 20,\n",
    "                      \"CCIN2P3\": 20, \"PIC\": 10, \"BNL\": 50}\n",
    "num_of_last_measurements = 15\n",
    "cut_throughput = 0.7\n",
    "timeframe_readable_train = ['11-09-2016 16:00', '28-09-2016 16:00']\n",
    "\n",
    "data_set_all = make_all_data_sets_for_NN(timeframe_readable_train, num_of_last_measurements, cut_throughput, throughput_mapping)\n",
    "print(len(data_set_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Ravin/hendrik_data_set.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f394ce2bfc1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_set_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'~/Ravin/hendrik_data_set.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ivukotic/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(self, path, compression)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \"\"\"\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_clipboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexcel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ivukotic/anaconda3/lib/python3.5/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(obj, path, compression)\u001b[0m\n\u001b[0;32m     25\u001b[0m     f, fh = _get_handle(path, 'wb',\n\u001b[0;32m     26\u001b[0m                         \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minferred_compression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                         is_text=False)\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ivukotic/anaconda3/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Ravin/hendrik_data_set.pkl'"
     ]
    }
   ],
   "source": [
    "data_set_all.to_pickle('~/Ravin/hendrik_data_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/share/home/ivukotic/workspace/AnalyticsNotebooks/Hendrik\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT Index(['delay_all_time_avg', 'delay_all_time_min', 'delay_all_time_std',\n",
      "       'delay_avg', 'delay_avg_gradient', 'delay_avg_past_1',\n",
      "       'delay_avg_past_10', 'delay_avg_past_11', 'delay_avg_past_12',\n",
      "       'delay_avg_past_13', 'delay_avg_past_14', 'delay_avg_past_15',\n",
      "       'delay_avg_past_2', 'delay_avg_past_3', 'delay_avg_past_4',\n",
      "       'delay_avg_past_5', 'delay_avg_past_6', 'delay_avg_past_7',\n",
      "       'delay_avg_past_8', 'delay_avg_past_9', 'delay_avg_past_avg',\n",
      "       'delay_avg_past_std', 'delay_median', 'delay_std', 'loss_all_time_avg',\n",
      "       'loss_all_time_std', 'packet_loss', 'packet_loss_gradient',\n",
      "       'packet_loss_past_1', 'packet_loss_past_10', 'packet_loss_past_11',\n",
      "       'packet_loss_past_12', 'packet_loss_past_13', 'packet_loss_past_14',\n",
      "       'packet_loss_past_15', 'packet_loss_past_2', 'packet_loss_past_3',\n",
      "       'packet_loss_past_4', 'packet_loss_past_5', 'packet_loss_past_6',\n",
      "       'packet_loss_past_7', 'packet_loss_past_8', 'packet_loss_past_9',\n",
      "       'packet_loss_past_avg', 'packet_loss_past_std'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_set_normalized, scaler_for_dataSet_all = normalizeDataSet(data_set_all)\n",
    "data_set_normalized_NN_Input = data_set_normalized.drop(['throughput','timestamp','throughput_perfSonar'], axis=1)\n",
    "data_set_normalized_NN_Output = data_set_normalized['throughput']\n",
    "print(\"INPUT\", data_set_normalized_NN_Input.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivukotic/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"uniform\", input_dim=45, activation=\"relu\")`\n",
      "/home/ivukotic/anaconda3/lib/python3.5/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0200     \n",
      "Epoch 2/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0147     \n",
      "Epoch 3/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0133     \n",
      "Epoch 4/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0125     \n",
      "Epoch 5/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0117     \n",
      "Epoch 6/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0112     \n",
      "Epoch 7/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0109     \n",
      "Epoch 8/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0104     \n",
      "Epoch 9/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0099     \n",
      "Epoch 10/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0099     \n",
      "Epoch 11/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0098     \n",
      "Epoch 12/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0096     \n",
      "Epoch 13/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0095     \n",
      "Epoch 14/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0092     \n",
      "Epoch 15/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0092     \n",
      "Epoch 16/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0090     \n",
      "Epoch 17/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0089     \n",
      "Epoch 18/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0090     \n",
      "Epoch 19/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0090     \n",
      "Epoch 20/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0087     \n",
      "Epoch 21/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0088     \n",
      "Epoch 22/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0086     \n",
      "Epoch 23/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0086     \n",
      "Epoch 24/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0087     \n",
      "Epoch 25/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0084     \n",
      "Epoch 26/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0086     \n",
      "Epoch 27/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0085     \n",
      "Epoch 28/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0085     \n",
      "Epoch 29/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0084     \n",
      "Epoch 30/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0083     \n",
      "Epoch 31/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0083     \n",
      "Epoch 32/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0083     \n",
      "Epoch 33/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0081     \n",
      "Epoch 34/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0081     \n",
      "Epoch 35/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0081     \n",
      "Epoch 36/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0081     \n",
      "Epoch 37/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0080     \n",
      "Epoch 38/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0080     \n",
      "Epoch 39/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0080     \n",
      "Epoch 40/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0078     \n",
      "Epoch 41/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0078     \n",
      "Epoch 42/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0079     \n",
      "Epoch 43/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0079     \n",
      "Epoch 44/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0078     \n",
      "Epoch 45/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0077     \n",
      "Epoch 46/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0077     \n",
      "Epoch 47/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0078     \n",
      "Epoch 48/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0077     \n",
      "Epoch 49/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0077     \n",
      "Epoch 50/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 51/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 52/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 53/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 54/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 55/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0077     \n",
      "Epoch 56/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 57/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 58/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 59/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 60/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 61/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 62/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 63/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 64/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 65/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 66/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 67/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 68/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 69/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 70/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 71/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 72/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 73/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 74/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 75/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 76/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 77/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 78/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 79/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 80/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 81/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 82/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 83/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 84/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 85/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0075     \n",
      "Epoch 86/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 87/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 88/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 89/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 90/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 91/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 92/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 93/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 94/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 95/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 96/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 97/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 98/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 99/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 100/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 101/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 102/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 103/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 104/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 105/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 106/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 107/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 108/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 109/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 110/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 111/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 112/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 113/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 114/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 115/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 116/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 117/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 118/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 119/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 120/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 121/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 122/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 123/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 124/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 125/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 126/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 127/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 128/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 129/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 130/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 131/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 132/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 133/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 134/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 135/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 136/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 137/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 138/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 139/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 140/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 141/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 142/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0074     \n",
      "Epoch 143/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 144/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 145/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 146/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 147/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 148/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 149/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 150/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 151/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 152/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 153/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 154/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 155/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 156/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 157/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 158/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 159/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 160/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 161/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 162/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 163/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 164/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 165/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 166/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 167/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 168/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 169/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 170/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 171/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 172/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 173/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 174/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 175/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 176/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 177/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 178/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 179/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 180/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 181/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 182/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 183/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 184/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 185/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 186/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 187/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 188/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 189/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 190/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 191/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 192/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 193/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 194/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 195/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 196/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 197/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 198/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 199/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 200/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 201/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0073     \n",
      "Epoch 202/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 203/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 204/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 205/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 206/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 207/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 208/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 209/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 210/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 211/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 212/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 213/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 214/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 215/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 216/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0072     \n",
      "Epoch 217/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 218/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 219/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 220/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 221/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 222/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 223/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 224/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 225/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 226/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 227/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 228/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 229/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 230/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 231/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 232/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 233/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 234/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0071     \n",
      "Epoch 235/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 236/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 237/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 238/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 239/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 240/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 241/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 242/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 243/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 244/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 245/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 246/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 247/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 248/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 249/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 250/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 251/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 252/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 253/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 254/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 255/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 256/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 257/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 258/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 259/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 260/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 261/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 262/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 263/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 264/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 265/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 266/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 267/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 268/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 269/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 270/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 271/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 272/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 273/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 274/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 275/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 276/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 277/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 278/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 279/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 280/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 281/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 282/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0070     \n",
      "Epoch 283/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 284/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 285/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 286/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 287/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 288/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 289/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 290/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0066     \n",
      "Epoch 291/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 292/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 293/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 294/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 295/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 296/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 297/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 298/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 299/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 300/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 301/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0069     \n",
      "Epoch 302/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 303/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 304/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 305/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0068     \n",
      "Epoch 306/5000\n",
      "51256/51256 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 307/5000\n",
      "39424/51256 [======================>.......] - ETA: 0s - loss: 0.0067"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "# we don't want to see the output, as it badly slows the browser\n",
    "\n",
    "# normalize dataset\n",
    "\n",
    "# train NN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "activation_func = 'relu'\n",
    "layer_init = \"\"\n",
    "# standard:\n",
    "#b_size = 32\n",
    "b_size = 512\n",
    "validation_split = 0.0\n",
    "training_epochs = 5000\n",
    "dropout_ratio = 0.5\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=len(data_set_normalized_NN_Input.axes[1]), activation=activation_func, init='uniform'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(50, activation=activation_func))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(5, activation=activation_func))\n",
    "# last/output layer\n",
    "model.add(Dense(1, activation=activation_func))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "history = model.fit(data_set_normalized_NN_Input.as_matrix(), data_set_normalized_NN_Output.as_matrix(),\n",
    "                    nb_epoch=training_epochs, batch_size=b_size, shuffle=True, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save model\n",
    "model.save('global_NN_for_netTel.h5')  # creates a HDF5 file 'global_NN_model_1.h5'\n",
    "# save our scaler\n",
    "pickle.dump(scaler_for_dataSet_all, open( \"global_NN_for_netTel_scaler.pkl\", \"wb\" ),\n",
    "            protocol=2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate on other timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create valida\n",
    "throughput_mapping = {\"JINR-T1\": 20, \"RAL\": 20, \"TRIUMF\": 10, \"CNAF\": 40, \"KIT\": 20,\n",
    "                      \"CCIN2P3\": 20, \"PIC\": 10, \"BNL\": 50}\n",
    "num_of_last_measurements = 15\n",
    "cut_throughput = 0.7\n",
    "timeframe_readable = ['28-09-2016 16:00', '02-10-2016 16:00']\n",
    "\n",
    "data_set2 = make_all_data_sets_for_NN(timeframe_readable, num_of_last_measurements, cut_throughput, throughput_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set_normalized, useless_scaler = normalizeDataSet(data_set2, scaler=scaler_for_dataSet_all)\n",
    "data_set_normalized_NN_Input = data_set_normalized.drop(['throughput','timestamp','throughput_perfSonar'], axis=1)\n",
    "data_set_normalized_NN_Output = data_set_normalized['throughput']\n",
    "\n",
    "score = model.evaluate(data_set_normalized_NN_Input.as_matrix(), data_set_normalized_NN_Output.as_matrix(),\n",
    "                       batch_size=b_size)\n",
    "prediction = model.predict(data_set_normalized_NN_Input.as_matrix(), batch_size=b_size)\n",
    "\n",
    "# make error histogram\n",
    "bins = np.arange(-0.8, 0.8, 0.01)\n",
    "#bins = 300\n",
    "cut_offset = 0.1\n",
    "#cut_offset = 0.06\n",
    "savePath = ''\n",
    "savePath = 'plots/a_presentation_plots/'\n",
    "\n",
    "title = \"Error histogram with cut_off for all connections\"\n",
    "title += \"\\nTimeframe training: \"+str(timeframe_readable_train[0])+\" to \"+str(timeframe_readable_train[1]) + \" | Epochs: \"+ str(training_epochs) \n",
    "#title += \"\\n activation func: \" + activation_func\n",
    "title += \"\\nTimeframe validation: \"+str(timeframe_readable[0])+\" to \"+str(timeframe_readable[1])\n",
    "title += \" | \"+ str(model.loss) +\" for validation: \" + str(round(score, 4))\n",
    "\n",
    "rs_prediction = np.reshape(prediction, (len(prediction),))\n",
    "diff = rs_prediction - data_set_normalized_NN_Output.as_matrix()\n",
    "diff = diff\n",
    "# cut median, its falsely called minimum here\n",
    "minimum = np.median(rs_prediction)\n",
    "validPoints = []\n",
    "for i in range(len(diff)):\n",
    "    if rs_prediction[i] >= (minimum+cut_offset):\n",
    "        validPoints.append(diff[i])\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(12,7)\n",
    "fig.set_dpi(300)\n",
    "n, bins, patches = plt.hist(validPoints, bins, normed=False, histtype='step', facecolor='green',\n",
    "                            alpha=0.75, orientation='vertical')\n",
    "\n",
    "plt.xlabel(\"Error [1]\")\n",
    "plt.ylabel('Count [1]')\n",
    "plt.title(title, fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "if savePath != '' :\n",
    "    fname = title\n",
    "    plt.savefig(savePath+ fname +'.png', format='PNG', dpi=200, bbox_inches='tight', pad_inches=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate on one specific connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create data Set\n",
    "# configuration\n",
    "src = \"CERN\"\n",
    "dest = \"RAL\"\n",
    "direction = \"out\"\n",
    "num_of_last_measurements = 15\n",
    "throughputMaxGBits = 20\n",
    "throughputPercentage = 0.7\n",
    "#timeframe_readable = ['11-09-2016 16:00', '27-09-2016 16:00']\n",
    "timeframe_readable = ['27-09-2016 16:00', '02-10-2016 16:00']\n",
    "#scaler = None\n",
    "scaler = scaler_for_dataSet_all\n",
    "\n",
    "# setting stuff up\n",
    "timeframe = [calendar.timegm(dt.datetime.strptime(t, '%d-%m-%Y %H:%M').timetuple()) for t in timeframe_readable]\n",
    "\n",
    "# make dataSet\n",
    "print(\"Creating data_set: \"+src+\" and \"+dest+\" | dir: \"+direction)\n",
    "throughputs = []\n",
    "for router in data_router.axes[1]:\n",
    "    if dest in router and direction in router[len(router)-3:]:\n",
    "        throughputs.append(data_router[router])\n",
    "data_from_routers = sum(throughputs).copy(deep=True)\n",
    "data_from_perfSonar = data_perfSonar[src][dest][direction].copy(deep=True)\n",
    "\n",
    "data_set1 = makeNewDataSet_NN(data_from_routers, data_from_perfSonar,\n",
    "                             timeframe, num_of_last_measurements=num_of_last_measurements,\n",
    "                            cut_throughput=cut_throughput, throughput_max=(throughputMaxGBits*1e9))\n",
    "\n",
    "print(\"data_set size: \"+ str(len(data_set1)))\n",
    "print(\"vars per data point: \"+ str(len(data_set1.axes[1])))\n",
    "\n",
    "#data_set_normalized, scaler_for_dataSet = normalizeDataSet(data_set1, scaler_feature_range=(0.1, 0.9), scaler=scaler)\n",
    "data_set_normalized, scaler_for_dataSet = normalizeDataSet(data_set1, scaler=scaler)\n",
    "# normalize throughput manualy as we have intel about it ^^\n",
    "# bring it to a region between 0.05 and 0.95\n",
    "data_set_normalized['throughput'] = normalize_throughput(data_set_normalized['throughput'], throughputMaxGBits)\n",
    "\n",
    "data_set_normalized_NN_Input = data_set_normalized.drop(['throughput','timestamp','throughput_perfSonar'], axis=1)\n",
    "data_set_normalized_NN_Output = data_set_normalized['throughput']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(data_set_normalized_NN_Input.as_matrix(), data_set_normalized_NN_Output.as_matrix(),\n",
    "                       batch_size=b_size)\n",
    "prediction = model.predict(data_set_normalized_NN_Input.as_matrix(), batch_size=b_size)\n",
    "\n",
    "timeframe_readable_new = timeframe_readable\n",
    "#timeframe_readable_new = ['14-09-2016 06:00', '16-09-2016 00:00']\n",
    "savePath = ''\n",
    "#savePath = 'plots/router_multi_plots_2016-09-29/'\n",
    "title_suffix = \"\\n General NN | \"+ str(model.loss) +\" for this data set: \" + str(score) + \" | activation func: \" + activation_func\n",
    "\n",
    "title_NN_plot = makePlot_forNN(src, dest, direction, timeframe_readable_new,\n",
    "               data_set1, inverses_normalize_throughput(prediction, throughputMaxGBits), title_suffix=title_suffix, savePath=savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make error histogram\n",
    "bins = np.arange(-0.8, 0.8, 0.01)\n",
    "cut_offset = 0.06\n",
    "savePath = ''\n",
    "#savePath = 'plots/router_multi_plots_2016-09-29/'\n",
    "title = \"Error histogram with cut_off for:\\n\" + title_NN_plot\n",
    "\n",
    "rs_prediction = np.reshape(prediction, (len(prediction),))\n",
    "diff = rs_prediction - data_set_normalized_NN_Output.as_matrix()\n",
    "diff = diff\n",
    "# cut minimum\n",
    "minimum = np.median(rs_prediction)\n",
    "print(\"median is: \"+str(minimum))\n",
    "validPoints = []\n",
    "for i in range(len(diff)):\n",
    "    if rs_prediction[i] >= (minimum+cut_offset):\n",
    "        validPoints.append(diff[i])\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(12,7)\n",
    "fig.set_dpi(300)\n",
    "n, bins, patches = plt.hist(validPoints, bins, normed=False, histtype='step', facecolor='green',\n",
    "                            alpha=0.75, orientation='vertical')\n",
    "\n",
    "plt.xlabel(\"Error [1]\")\n",
    "plt.ylabel('Count [1]')\n",
    "plt.title(title, fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "if savePath != '' :\n",
    "    fname = title\n",
    "    plt.savefig(savePath+ fname +'.png', format='PNG', dpi=200, bbox_inches='tight', pad_inches=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scatter plot, yeah!\n",
    "num_bins1 = 150\n",
    "num_bins2 = 150\n",
    "normed = False\n",
    "ylim = (throughputMaxGBits*0.65, throughputMaxGBits)\n",
    "title = \"Scatter plot for:\\n\" + title_NN_plot\n",
    "fname = title\n",
    "\n",
    "rs_prediction = np.reshape(prediction, (len(prediction),))\n",
    "predictionGBits = inverses_normalize_throughput(rs_prediction, throughputMaxGBits)/1e9\n",
    "actualValsGBits = inverses_normalize_throughput(data_set_normalized_NN_Output.as_matrix(), throughputMaxGBits)/1e9\n",
    "pointsForXprojection = []\n",
    "for i in range(len(actualValsGBits)):\n",
    "    if actualValsGBits[i] >= ylim[0]:\n",
    "        pointsForXprojection.append(predictionGBits[i])\n",
    "\n",
    "# plot the data we got\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(15,15)\n",
    "fig.set_dpi(300)\n",
    "\n",
    "plt.subplot(221)\n",
    "# scaterplot itself\n",
    "plt.title(title,fontsize=14)\n",
    "plt.plot(predictionGBits, actualValsGBits, 'bx', label=\"\")\n",
    "plt.ylabel(\"Throughput all routers [Gbit/s]\")\n",
    "plt.xlabel(\"Prediction from NN [Gbit/s]\")\n",
    "\n",
    "#plt.legend(loc='best',fontsize=8)\n",
    "current_xlim = (plt.xlim())\n",
    "plt.ylim(ylim)\n",
    "\n",
    "# histogram y axis\n",
    "plt.subplot(222)\n",
    "bin1 = np.arange(ylim[0], ylim[1], abs(ylim[0]-ylim[1])/num_bins1)\n",
    "plt.title(\"y axis projection (count of data points)\",fontsize=10)\n",
    "n, bins, patches = plt.hist(actualValsGBits, bin1, normed=normed, histtype='step', facecolor='green',\n",
    "                                alpha=0.75, orientation='horizontal')\n",
    "\n",
    "# histogram x axis\n",
    "plt.subplot(223)\n",
    "plt.title(\"x axis projection (count of data points)\",fontsize=10)\n",
    "#bin2 = np.arange(xlim[0], xlim[1], abs(xlim[0]-xlim[1])/num_bins2)\n",
    "n, bins, patches = plt.hist(pointsForXprojection, num_bins2, normed=normed, histtype='step', facecolor='green',\n",
    "                                alpha=0.75, orientation='vertical')\n",
    "plt.xlim(current_xlim)\n",
    "#plt.ylim(0, 100)\n",
    "\n",
    "\n",
    "#plt.savefig('plots/router_multi_plots_2016-09-29/'+ fname +'.png', format='PNG', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makePlot_forNN(src, dest, direction, timeframe, data_set_local, prediction_bw, title_suffix='', savePath=''):\n",
    "    # set stuff up\n",
    "    direction_counter = \"in\"\n",
    "    if direction == \"in\":\n",
    "        direction_counter = \"out\"\n",
    "    dirMarker = \" <- \"\n",
    "    if direction == \"out\":\n",
    "        dirMarker = \" -> \"\n",
    "    title = \"NN plot for \"+src+\" \"+dirMarker+\" \"+dest +\"\\nTimeframe: \"+str(timeframe[0])+\" to \"+str(timeframe[1])+title_suffix\n",
    "    fname = title\n",
    "    # make dates\n",
    "    dates_data_set = []\n",
    "    for stamp in data_set_local['timestamp']:\n",
    "        dates_data_set.append(dt.datetime.fromtimestamp(stamp))\n",
    "    # plot the data we got\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(18,12)\n",
    "    fig.set_dpi(300)\n",
    "    plt.title(title,fontsize=14)\n",
    "\n",
    "    # make first axis\n",
    "    ax1.hlines(inverses_normalize_throughput((minimum+cut_offset), throughputMaxGBits)/1e9, [0], [999999999999999999], label=\"cut_off\")\n",
    "    ax1.plot(dates_data_set, data_set_local[\"delay_avg\"]/10,\n",
    "             'y', label=r'Delay /10')\n",
    "    # get throughput\n",
    "    ax1.plot(dates_data_set, data_set_local['throughput']/1e9,\n",
    "             'c', label=r'Throughput (actual from routers)')\n",
    "    ax1.plot(dates_data_set, prediction_bw/1e9,\n",
    "             'b', label=r' Throughput (NN prediction)')\n",
    "    \n",
    "    # make second axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dates_data_set, data_set_local[\"packet_loss\"],\n",
    "             'g', label=r'Packetloss')\n",
    "\n",
    "    # limits\n",
    "    #ax1.set_ylim([0, 20])\n",
    "    ax2.set_ylim([0, 0.05])\n",
    "    plt.xlim([dt.datetime.strptime(t, '%d-%m-%Y %H:%M') for t in timeframe])\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y %H:%M'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    # legends and lables\n",
    "    ax1.set_ylabel(\"Throughput [Gbit/s] and Latency [ms/10]\")\n",
    "    ax1.set_xlabel(\"Time [s since epoch]\")\n",
    "    ax1.legend(loc='upper left',fontsize=14)\n",
    "    ax2.set_ylabel(\"Packetloss\")\n",
    "    ax2.legend(loc='upper right',fontsize=14)\n",
    "    # saving?\n",
    "    if savePath != '' :\n",
    "        plt.savefig(savePath+ fname +'.png', format='PNG', dpi=200, bbox_inches='tight', pad_inches=0.7)\n",
    "    plt.show()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_set_normalized_NN_Input.axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
