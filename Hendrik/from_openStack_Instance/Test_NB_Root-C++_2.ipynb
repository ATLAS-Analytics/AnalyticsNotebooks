{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// @(#)root/tmva $Id$\n",
    "/**********************************************************************************\n",
    " * Project   : TMVA - a Root-integrated toolkit for multivariate data analysis    *\n",
    " * Package   : TMVA                                                               *\n",
    " * Root Macro: TMVARegression                                                     *\n",
    " *                                                                                *\n",
    " * This macro provides examples for the training and testing of the               *\n",
    " * TMVA classifiers.                                                              *\n",
    " *                                                                                *\n",
    " * As input data is used a toy-MC sample consisting of four Gaussian-distributed  *\n",
    " * and linearly correlated input variables.                                       *\n",
    " *                                                                                *\n",
    " * The methods to be used can be switched on and off by means of booleans, or     *\n",
    " * via the prompt command, for example:                                           *\n",
    " *                                                                                *\n",
    " *    root -l TMVARegression.C\\(\\\"LD,MLP\\\"\\)                                      *\n",
    " *                                                                                *\n",
    " * (note that the backslashes are mandatory)                                      *\n",
    " * If no method given, a default set is used.                                     *\n",
    " *                                                                                *\n",
    " * The output file \"TMVAReg.root\" can be analysed with the use of dedicated       *\n",
    " * macros (simply say: root -l <macro.C>), which can be conveniently              *\n",
    " * invoked through a GUI that will appear at the end of the run of this macro.    *\n",
    " **********************************************************************************/\n",
    "\n",
    "#include <cstdlib>\n",
    "#include <iostream> \n",
    "#include <map>\n",
    "#include <string>\n",
    "\n",
    "#include \"TChain.h\"\n",
    "#include \"TFile.h\"\n",
    "#include \"TTree.h\"\n",
    "#include \"TString.h\"\n",
    "#include \"TObjString.h\"\n",
    "#include \"TSystem.h\"\n",
    "#include \"TROOT.h\"\n",
    "\n",
    "#include \"TMVA/Tools.h\"\n",
    "#include \"TMVA/Factory.h\"\n",
    "#include \"TMVA/TMVARegGui.h\"\n",
    "\n",
    "\n",
    "using namespace TMVA;\n",
    "   \n",
    "void TMVARegression( TString myMethodList = \"\" ) \n",
    "{\n",
    "   // The explicit loading of the shared libTMVA is done in TMVAlogon.C, defined in .rootrc\n",
    "   // if you use your private .rootrc, or run from a different directory, please copy the \n",
    "   // corresponding lines from .rootrc\n",
    "\n",
    "   // methods to be processed can be given as an argument; use format:\n",
    "   //\n",
    "   // mylinux~> root -l TMVARegression.C\\(\\\"myMethod1,myMethod2,myMethod3\\\"\\)\n",
    "   //\n",
    "\n",
    "   //---------------------------------------------------------------\n",
    "   // This loads the library\n",
    "   TMVA::Tools::Instance();\n",
    "\n",
    "\n",
    "\n",
    "   // Default MVA methods to be trained + tested\n",
    "   std::map<std::string,int> Use;\n",
    "\n",
    "   // --- Mutidimensional likelihood and Nearest-Neighbour methods\n",
    "   Use[\"PDERS\"]           = 0;\n",
    "   Use[\"PDEFoam\"]         = 0; \n",
    "   Use[\"KNN\"]             = 0;\n",
    "   // \n",
    "   // --- Linear Discriminant Analysis\n",
    "   Use[\"LD\"]\t\t        = 0;\n",
    "   // \n",
    "   // --- Function Discriminant analysis\n",
    "   Use[\"FDA_GA\"]          = 0;\n",
    "   Use[\"FDA_MC\"]          = 0;\n",
    "   Use[\"FDA_MT\"]          = 0;\n",
    "   Use[\"FDA_GAMT\"]        = 0;\n",
    "   // \n",
    "   // --- Neural Network\n",
    "   Use[\"MLP\"]             = 1;\n",
    "//   Use[\"MLPBFGS\"]         = 1;\n",
    "//   Use[\"MLPBNN\"]          = 1;    \n",
    "   // \n",
    "   // --- Support Vector Machine \n",
    "   Use[\"SVM\"]             = 0;\n",
    "   // \n",
    "   // --- Boosted Decision Trees\n",
    "   Use[\"BDT\"]             = 0;\n",
    "   Use[\"BDTG\"]            = 0;\n",
    "   // ---------------------------------------------------------------\n",
    "\n",
    "   std::cout << std::endl;\n",
    "   std::cout << \"==> Start TMVARegression\" << std::endl;\n",
    "\n",
    "   // Select methods (don't look at this code - not of interest)\n",
    "   if (myMethodList != \"\") {\n",
    "      for (std::map<std::string,int>::iterator it = Use.begin(); it != Use.end(); it++) it->second = 0;\n",
    "\n",
    "      std::vector<TString> mlist = gTools().SplitString( myMethodList, ',' );\n",
    "      for (UInt_t i=0; i<mlist.size(); i++) {\n",
    "         std::string regMethod(mlist[i]);\n",
    "\n",
    "         if (Use.find(regMethod) == Use.end()) {\n",
    "            std::cout << \"Method \\\"\" << regMethod << \"\\\" not known in TMVA under this name. Choose among the following:\" << std::endl;\n",
    "            for (std::map<std::string,int>::iterator it = Use.begin(); it != Use.end(); it++) std::cout << it->first << \" \";\n",
    "            std::cout << std::endl;\n",
    "            return;\n",
    "         }\n",
    "         Use[regMethod] = 1;\n",
    "      }\n",
    "   }\n",
    "\n",
    "   // --------------------------------------------------------------------------------------------------\n",
    "\n",
    "   // --- Here the preparation phase begins\n",
    "\n",
    "   // Create a new root output file\n",
    "   TString outfileName( \"TMVAReg.root\" );\n",
    "   TFile* outputFile = TFile::Open( outfileName, \"RECREATE\" );\n",
    "\n",
    "   // Create the factory object. Later you can choose the methods\n",
    "   // whose performance you'd like to investigate. The factory will\n",
    "   // then run the performance analysis for you.\n",
    "   //\n",
    "   // The first argument is the base of the name of all the\n",
    "   // weightfiles in the directory weight/ \n",
    "   //\n",
    "   // The second argument is the output file for the training results\n",
    "   // All TMVA output can be suppressed by removing the \"!\" (not) in \n",
    "   // front of the \"Silent\" argument in the option string\n",
    "   TMVA::Factory *factory = new TMVA::Factory( \"TMVARegression\", outputFile, \n",
    "                                               \"!V:!Silent:Color:DrawProgressBar\" );\n",
    "\n",
    "   // If you wish to modify default settings \n",
    "   // (please check \"src/Config.h\" to see all available global options)\n",
    "   //    (TMVA::gConfig().GetVariablePlotting()).fTimesRMS = 8.0;\n",
    "   //    (TMVA::gConfig().GetIONames()).fWeightFileDir = \"myWeightDirectory\";\n",
    "\n",
    "   // Define the input variables that shall be used for the MVA training\n",
    "   // note that you may also use variable expressions, such as: \"3*var1/var2*abs(var3)\"\n",
    "   // [all types of expressions that can also be parsed by TTree::Draw( \"expression\" )]\n",
    "   factory->AddVariable( \"var1\", \"Variable 1\", \"units\", 'F' );\n",
    "   factory->AddVariable( \"var2\", \"Variable 2\", \"units\", 'F' );\n",
    "\n",
    "   // You can add so-called \"Spectator variables\", which are not used in the MVA training, \n",
    "   // but will appear in the final \"TestTree\" produced by TMVA. This TestTree will contain the \n",
    "   // input variables, the response values of all trained MVAs, and the spectator variables\n",
    "   factory->AddSpectator( \"spec1:=var1*2\",  \"Spectator 1\", \"units\", 'F' );\n",
    "   factory->AddSpectator( \"spec2:=var1*3\",  \"Spectator 2\", \"units\", 'F' );\n",
    "\n",
    "   // Add the variable carrying the regression target\n",
    "   factory->AddTarget( \"fvalue\" ); \n",
    "\n",
    "   // It is also possible to declare additional targets for multi-dimensional regression, ie:\n",
    "   // -- factory->AddTarget( \"fvalue2\" );\n",
    "   // BUT: this is currently ONLY implemented for MLP\n",
    "\n",
    "   // Read training and test data (see TMVAClassification for reading ASCII files)\n",
    "   // load the signal and background event samples from ROOT trees\n",
    "   TFile *input(0);\n",
    "   TString fname = \"./tmva_reg_example.root\";\n",
    "   if (!gSystem->AccessPathName( fname )) \n",
    "      input = TFile::Open( fname ); // check if file in local directory exists\n",
    "   else \n",
    "      input = TFile::Open( \"http://root.cern.ch/files/tmva_reg_example.root\" ); // if not: download from ROOT server\n",
    "   \n",
    "   if (!input) {\n",
    "      std::cout << \"ERROR: could not open data file\" << std::endl;\n",
    "      exit(1);\n",
    "   }\n",
    "   std::cout << \"--- TMVARegression           : Using input file: \" << input->GetName() << std::endl;\n",
    "\n",
    "   // --- Register the regression tree\n",
    "\n",
    "   TTree *regTree = (TTree*)input->Get(\"TreeR\");\n",
    "\n",
    "   // global event weights per tree (see below for setting event-wise weights)\n",
    "   Double_t regWeight  = 1.0;   \n",
    "\n",
    "   // You can add an arbitrary number of regression trees\n",
    "   factory->AddRegressionTree( regTree, regWeight );\n",
    "\n",
    "   // This would set individual event weights (the variables defined in the \n",
    "   // expression need to exist in the original TTree)\n",
    "   factory->SetWeightExpression( \"var1\", \"Regression\" );\n",
    "\n",
    "   // Apply additional cuts on the signal and background samples (can be different)\n",
    "   TCut mycut = \"\"; // for example: TCut mycut = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "\n",
    "   // tell the factory to use all remaining events in the trees after training for testing:\n",
    "   factory->PrepareTrainingAndTestTree( mycut, \n",
    "                                         \"nTrain_Regression=1000:nTest_Regression=0:SplitMode=Random:NormMode=NumEvents:!V\" );\n",
    "   // factory->PrepareTrainingAndTestTree( mycut, \n",
    "   //                                      \"nTrain_Regression=0:nTest_Regression=0:SplitMode=Random:NormMode=NumEvents:!V\" );\n",
    "\n",
    "   // If no numbers of events are given, half of the events in the tree are used \n",
    "   // for training, and the other half for testing:\n",
    "   //    factory->PrepareTrainingAndTestTree( mycut, \"SplitMode=random:!V\" );  \n",
    "\n",
    "   // ---- Book MVA methods\n",
    "   //\n",
    "   // please lookup the various method configuration options in the corresponding cxx files, eg:\n",
    "   // src/MethoCuts.cxx, etc, or here: http://tmva.sourceforge.net/optionRef.html\n",
    "   // it is possible to preset ranges in the option string in which the cut optimisation should be done:\n",
    "   // \"...:CutRangeMin[2]=-1:CutRangeMax[2]=1\"...\", where [2] is the third input variable\n",
    "\n",
    "   // PDE - RS method\n",
    "   if (Use[\"PDERS\"])\n",
    "      factory->BookMethod( TMVA::Types::kPDERS, \"PDERS\", \n",
    "                           \"!H:!V:NormTree=T:VolumeRangeMode=Adaptive:KernelEstimator=Gauss:GaussSigma=0.3:NEventsMin=40:NEventsMax=60:VarTransform=None\" );\n",
    "   // And the options strings for the MinMax and RMS methods, respectively:\n",
    "   //      \"!H:!V:VolumeRangeMode=MinMax:DeltaFrac=0.2:KernelEstimator=Gauss:GaussSigma=0.3\" );   \n",
    "   //      \"!H:!V:VolumeRangeMode=RMS:DeltaFrac=3:KernelEstimator=Gauss:GaussSigma=0.3\" );   \n",
    "\n",
    "   if (Use[\"PDEFoam\"])\n",
    "       factory->BookMethod( TMVA::Types::kPDEFoam, \"PDEFoam\", \n",
    "\t\t\t    \"!H:!V:MultiTargetRegression=F:TargetSelection=Mpv:TailCut=0.001:VolFrac=0.0666:nActiveCells=500:nSampl=2000:nBin=5:Compress=T:Kernel=None:Nmin=10:VarTransform=None\" );\n",
    "\n",
    "   // K-Nearest Neighbour classifier (KNN)\n",
    "   if (Use[\"KNN\"])\n",
    "      factory->BookMethod( TMVA::Types::kKNN, \"KNN\", \n",
    "                           \"nkNN=20:ScaleFrac=0.8:SigmaFact=1.0:Kernel=Gaus:UseKernel=F:UseWeight=T:!Trim\" );\n",
    "\n",
    "   // Linear discriminant\n",
    "   if (Use[\"LD\"])\n",
    "      factory->BookMethod( TMVA::Types::kLD, \"LD\", \n",
    "                           \"!H:!V:VarTransform=None\" );\n",
    "\n",
    "\t// Function discrimination analysis (FDA) -- test of various fitters - the recommended one is Minuit (or GA or SA)\n",
    "   if (Use[\"FDA_MC\"]) \n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_MC\",\n",
    "                          \"!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100):FitMethod=MC:SampleSize=100000:Sigma=0.1:VarTransform=D\" );\n",
    "   \n",
    "   if (Use[\"FDA_GA\"]) // can also use Simulated Annealing (SA) algorithm (see Cuts_SA options) .. the formula of this example is good for parabolas\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_GA\",\n",
    "                           \"!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100):FitMethod=GA:PopSize=100:Cycles=3:Steps=30:Trim=True:SaveBestGen=1:VarTransform=Norm\" );\n",
    "\n",
    "   if (Use[\"FDA_MT\"]) \n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_MT\",\n",
    "                           \"!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100);(-10,10):FitMethod=MINUIT:ErrorLevel=1:PrintLevel=-1:FitStrategy=2:UseImprove:UseMinos:SetBatch\" );\n",
    "\n",
    "   if (Use[\"FDA_GAMT\"]) \n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_GAMT\",\n",
    "                           \"!H:!V:Formula=(0)+(1)*x0+(2)*x1:ParRanges=(-100,100);(-100,100);(-100,100):FitMethod=GA:Converger=MINUIT:ErrorLevel=1:PrintLevel=-1:FitStrategy=0:!UseImprove:!UseMinos:SetBatch:Cycles=1:PopSize=5:Steps=5:Trim\" );\n",
    "\n",
    "   // Neural network (MLP)\n",
    "   if (Use[\"MLP\"])\n",
    "      factory->BookMethod( TMVA::Types::kMLP, \"MLP\", \"!H:!V:VarTransform=Norm:NeuronType=tanh:NCycles=20000:HiddenLayers=N+20:TestRate=6:TrainingMethod=BFGS:Sampling=0.3:SamplingEpoch=0.8:ConvergenceImprove=1e-6:ConvergenceTests=15:!UseRegulator\" );\n",
    "\n",
    "   // Support Vector Machine\n",
    "   if (Use[\"SVM\"])\n",
    "      factory->BookMethod( TMVA::Types::kSVM, \"SVM\", \"Gamma=0.25:Tol=0.001:VarTransform=Norm\" );\n",
    "\n",
    "   // Boosted Decision Trees\n",
    "   if (Use[\"BDT\"])\n",
    "     factory->BookMethod( TMVA::Types::kBDT, \"BDT\",\n",
    "                           \"!H:!V:NTrees=100:MinNodeSize=1.0%:BoostType=AdaBoostR2:SeparationType=RegressionVariance:nCuts=20:PruneMethod=CostComplexity:PruneStrength=30\" );\n",
    "\n",
    "   if (Use[\"BDTG\"])\n",
    "     factory->BookMethod( TMVA::Types::kBDT, \"BDTG\",\n",
    "                           \"!H:!V:NTrees=2000::BoostType=Grad:Shrinkage=0.1:UseBaggedBoost:BaggedSampleFraction=0.5:nCuts=20:MaxDepth=3:MaxDepth=4\" );\n",
    "   // --------------------------------------------------------------------------------------------------\n",
    "\n",
    "   // ---- Now you can tell the factory to train, test, and evaluate the MVAs\n",
    "\n",
    "   // Train MVAs using the set of training events\n",
    "   factory->TrainAllMethods();\n",
    "\n",
    "   // ---- Evaluate all MVAs using the set of test events\n",
    "   factory->TestAllMethods();\n",
    "\n",
    "   // ----- Evaluate and compare performance of all configured MVAs\n",
    "   factory->EvaluateAllMethods();    \n",
    "\n",
    "   // --------------------------------------------------------------\n",
    "   \n",
    "   // Save the output\n",
    "   outputFile->Close();\n",
    "\n",
    "   std::cout << \"==> Wrote root file: \" << outputFile->GetName() << std::endl;\n",
    "   std::cout << \"==> TMVARegression is done!\" << std::endl;      \n",
    "\n",
    "   delete factory;\n",
    "\n",
    "   // Launch the GUI for the root macros\n",
    "   if (!gROOT->IsBatch()) TMVA::TMVARegGui( outfileName );\n",
    "}\n",
    "\n",
    "int main( int argc, char** argv )\n",
    "{\n",
    "   // Select methods (don't look at this code - not of interest)\n",
    "   TString methodList; \n",
    "   for (int i=1; i<argc; i++) {\n",
    "      TString regMethod(argv[i]);\n",
    "      if(regMethod==\"-b\" || regMethod==\"--batch\") continue;\n",
    "      if (!methodList.IsNull()) methodList += TString(\",\"); \n",
    "      methodList += regMethod;\n",
    "   }\n",
    "   TMVARegression(methodList);\n",
    "   return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT Prompt",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
