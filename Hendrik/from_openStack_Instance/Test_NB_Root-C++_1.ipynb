{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/**********************************************************************************\n",
    " * Project   : TMVA - a ROOT-integrated toolkit for multivariate data analysis    *\n",
    " * Package   : TMVA                                                               *\n",
    " * Root Macro: TMVAClassification                                                 *\n",
    " *                                                                                *\n",
    " * This macro provides examples for the training and testing of the               *\n",
    " * TMVA classifiers.                                                              *\n",
    " *                                                                                *\n",
    " * As input data is used a toy-MC sample consisting of four Gaussian-distributed  *\n",
    " * and linearly correlated input variables.                                       *\n",
    " *                                                                                *\n",
    " * The methods to be used can be switched on and off by means of booleans, or     *\n",
    " * via the prompt command, for example:                                           *\n",
    " *                                                                                *\n",
    " *    root -l ./TMVAClassification.C\\(\\\"Fisher,Likelihood\\\"\\)                     *\n",
    " *                                                                                *\n",
    " * (note that the backslashes are mandatory)                                      *\n",
    " * If no method given, a default set of classifiers is used.                      *\n",
    " *                                                                                *\n",
    " * The output file \"TMVA.root\" can be analysed with the use of dedicated          *\n",
    " * macros (simply say: root -l <macro.C>), which can be conveniently              *\n",
    " * invoked through a GUI that will appear at the end of the run of this macro.    *\n",
    " * Launch the GUI via the command:                                                *\n",
    " *                                                                                *\n",
    " *    root -l ./TMVAGui.C                                                         *\n",
    " *                                                                                *\n",
    " * You can also compile and run the example with the following commands           *\n",
    " *                                                                                *\n",
    " *    make                                                                        *\n",
    " *    ./TMVAClassification <Methods>                                              *\n",
    " *                                                                                *\n",
    " * where: <Methods> = \"method1 method2\"                                           *\n",
    " *        are the TMVA classifier names                                           *\n",
    " *                                                                                *\n",
    " * example:                                                                       *\n",
    " *    ./TMVAClassification Fisher LikelihoodPCA BDT                               *\n",
    " *                                                                                *\n",
    " * If no method given, a default set is of classifiers is used                    *\n",
    " **********************************************************************************/\n",
    "\n",
    "#include <cstdlib>\n",
    "#include <iostream>\n",
    "#include <map>\n",
    "#include <string>\n",
    "\n",
    "#include \"TChain.h\"\n",
    "#include \"TFile.h\"\n",
    "#include \"TTree.h\"\n",
    "#include \"TString.h\"\n",
    "#include \"TObjString.h\"\n",
    "#include \"TSystem.h\"\n",
    "#include \"TROOT.h\"\n",
    "\n",
    "#include \"TMVA/Factory.h\"\n",
    "#include \"TMVA/Tools.h\"\n",
    "#include \"TMVA/TMVAGui.h\"\n",
    "\n",
    "int TMVAClassification( TString myMethodList = \"\" )\n",
    "{\n",
    "   // The explicit loading of the shared libTMVA is done in TMVAlogon.C, defined in .rootrc\n",
    "   // if you use your private .rootrc, or run from a different directory, please copy the\n",
    "   // corresponding lines from .rootrc\n",
    "\n",
    "   // methods to be processed can be given as an argument; use format:\n",
    "   //\n",
    "   // mylinux~> root -l TMVAClassification.C\\(\\\"myMethod1,myMethod2,myMethod3\\\"\\)\n",
    "   //\n",
    "   // if you like to use a method via the plugin mechanism, we recommend using\n",
    "   //\n",
    "   // mylinux~> root -l TMVAClassification.C\\(\\\"P_myMethod\\\"\\)\n",
    "   // (an example is given for using the BDT as plugin (see below),\n",
    "   // but of course the real application is when you write your own\n",
    "   // method based)\n",
    "\n",
    "   //---------------------------------------------------------------\n",
    "   // This loads the library\n",
    "   TMVA::Tools::Instance();\n",
    "\n",
    "   // Default MVA methods to be trained + tested\n",
    "   std::map<std::string,int> Use;\n",
    "\n",
    "   // --- Cut optimisation\n",
    "   Use[\"Cuts\"]            = 0;\n",
    "   Use[\"CutsD\"]           = 0;\n",
    "   Use[\"CutsPCA\"]         = 0;\n",
    "   Use[\"CutsGA\"]          = 0;\n",
    "   Use[\"CutsSA\"]          = 0;\n",
    "   // \n",
    "   // --- 1-dimensional likelihood (\"naive Bayes estimator\")\n",
    "   Use[\"Likelihood\"]      = 0;\n",
    "   Use[\"LikelihoodD\"]     = 0; // the \"D\" extension indicates decorrelated input variables (see option strings)\n",
    "   Use[\"LikelihoodPCA\"]   = 0; // the \"PCA\" extension indicates PCA-transformed input variables (see option strings)\n",
    "   Use[\"LikelihoodKDE\"]   = 0;\n",
    "   Use[\"LikelihoodMIX\"]   = 0;\n",
    "   //\n",
    "   // --- Mutidimensional likelihood and Nearest-Neighbour methods\n",
    "   Use[\"PDERS\"]           = 0;\n",
    "   Use[\"PDERSD\"]          = 0;\n",
    "   Use[\"PDERSPCA\"]        = 0;\n",
    "   Use[\"PDEFoam\"]         = 0;\n",
    "   Use[\"PDEFoamBoost\"]    = 0; // uses generalised MVA method boosting\n",
    "   Use[\"KNN\"]             = 0; // k-nearest neighbour method\n",
    "   //\n",
    "   // --- Linear Discriminant Analysis\n",
    "   Use[\"LD\"]              = 0; // Linear Discriminant identical to Fisher\n",
    "   Use[\"Fisher\"]          = 0;\n",
    "   Use[\"FisherG\"]         = 0;\n",
    "   Use[\"BoostedFisher\"]   = 0; // uses generalised MVA method boosting\n",
    "   Use[\"HMatrix\"]         = 0;\n",
    "   //\n",
    "   // --- Function Discriminant analysis\n",
    "   Use[\"FDA_GA\"]          = 0; // minimisation of user-defined function using Genetics Algorithm\n",
    "   Use[\"FDA_SA\"]          = 0;\n",
    "   Use[\"FDA_MC\"]          = 0;\n",
    "   Use[\"FDA_MT\"]          = 0;\n",
    "   Use[\"FDA_GAMT\"]        = 0;\n",
    "   Use[\"FDA_MCMT\"]        = 0;\n",
    "   //\n",
    "   // --- Neural Networks (all are feed-forward Multilayer Perceptrons)\n",
    "   Use[\"MLP\"]             = 1; // Recommended ANN\n",
    "   Use[\"MLPBFGS\"]         = 1; // Recommended ANN with optional training method\n",
    "   Use[\"MLPBNN\"]          = 1; // Recommended ANN with BFGS training method and bayesian regulator\n",
    "   Use[\"CFMlpANN\"]        = 0; // Depreciated ANN from ALEPH\n",
    "   Use[\"TMlpANN\"]         = 0; // ROOT's own ANN\n",
    "   //\n",
    "   // --- Support Vector Machine \n",
    "   Use[\"SVM\"]             = 0;\n",
    "   // \n",
    "   // --- Boosted Decision Trees\n",
    "   Use[\"BDT\"]             = 0; // uses Adaptive Boost\n",
    "   Use[\"BDTG\"]            = 0; // uses Gradient Boost\n",
    "   Use[\"BDTB\"]            = 0; // uses Bagging\n",
    "   Use[\"BDTD\"]            = 0; // decorrelation + Adaptive Boost\n",
    "   Use[\"BDTF\"]            = 0; // allow usage of fisher discriminant for node splitting \n",
    "   // \n",
    "   // --- Friedman's RuleFit method, ie, an optimised series of cuts (\"rules\")\n",
    "   Use[\"RuleFit\"]         = 0;\n",
    "   // ---------------------------------------------------------------\n",
    "\n",
    "   std::cout << std::endl;\n",
    "   std::cout << \"==> Start TMVAClassification\" << std::endl;\n",
    "\n",
    "   // Select methods (don't look at this code - not of interest)\n",
    "   if (myMethodList != \"\") {\n",
    "      for (std::map<std::string,int>::iterator it = Use.begin(); it != Use.end(); it++) it->second = 0;\n",
    "\n",
    "      std::vector<TString> mlist = TMVA::gTools().SplitString( myMethodList, ',' );\n",
    "      for (UInt_t i=0; i<mlist.size(); i++) {\n",
    "         std::string regMethod(mlist[i]);\n",
    "\n",
    "         if (Use.find(regMethod) == Use.end()) {\n",
    "            std::cout << \"Method \\\"\" << regMethod << \"\\\" not known in TMVA under this name. Choose among the following:\" << std::endl;\n",
    "            for (std::map<std::string,int>::iterator it = Use.begin(); it != Use.end(); it++) std::cout << it->first << \" \";\n",
    "            std::cout << std::endl;\n",
    "            return 1;\n",
    "         }\n",
    "         Use[regMethod] = 1;\n",
    "      }\n",
    "   }\n",
    "\n",
    "   // --------------------------------------------------------------------------------------------------\n",
    "\n",
    "   // --- Here the preparation phase begins\n",
    "\n",
    "   // Create a ROOT output file where TMVA will store ntuples, histograms, etc.\n",
    "   TString outfileName( \"TMVA.root\" );\n",
    "   TFile* outputFile = TFile::Open( outfileName, \"RECREATE\" );\n",
    "\n",
    "   // Create the factory object. Later you can choose the methods\n",
    "   // whose performance you'd like to investigate. The factory is \n",
    "   // the only TMVA object you have to interact with\n",
    "   //\n",
    "   // The first argument is the base of the name of all the\n",
    "   // weightfiles in the directory weight/\n",
    "   //\n",
    "   // The second argument is the output file for the training results\n",
    "   // All TMVA output can be suppressed by removing the \"!\" (not) in\n",
    "   // front of the \"Silent\" argument in the option string\n",
    "   TMVA::Factory *factory = new TMVA::Factory( \"TMVAClassification\", outputFile,\n",
    "                                               \"!V:!Silent:Color:DrawProgressBar:Transformations=I;D;P;G,D:AnalysisType=Classification\" );\n",
    "\n",
    "   // If you wish to modify default settings\n",
    "   // (please check \"src/Config.h\" to see all available global options)\n",
    "   //    (TMVA::gConfig().GetVariablePlotting()).fTimesRMS = 8.0;\n",
    "   //    (TMVA::gConfig().GetIONames()).fWeightFileDir = \"myWeightDirectory\";\n",
    "\n",
    "   // Define the input variables that shall be used for the MVA training\n",
    "   // note that you may also use variable expressions, such as: \"3*var1/var2*abs(var3)\"\n",
    "   // [all types of expressions that can also be parsed by TTree::Draw( \"expression\" )]\n",
    "   factory->AddVariable( \"myvar1 := var1+var2\", 'F' );\n",
    "   factory->AddVariable( \"myvar2 := var1-var2\", \"Expression 2\", \"\", 'F' );\n",
    "   factory->AddVariable( \"var3\",                \"Variable 3\", \"units\", 'F' );\n",
    "   factory->AddVariable( \"var4\",                \"Variable 4\", \"units\", 'F' );\n",
    "\n",
    "   // You can add so-called \"Spectator variables\", which are not used in the MVA training,\n",
    "   // but will appear in the final \"TestTree\" produced by TMVA. This TestTree will contain the\n",
    "   // input variables, the response values of all trained MVAs, and the spectator variables\n",
    "   factory->AddSpectator( \"spec1 := var1*2\",  \"Spectator 1\", \"units\", 'F' );\n",
    "   factory->AddSpectator( \"spec2 := var1*3\",  \"Spectator 2\", \"units\", 'F' );\n",
    "\n",
    "   // Read training and test data\n",
    "   // (it is also possible to use ASCII format as input -> see TMVA Users Guide)\n",
    "   TString fname = \"./tmva_class_example.root\";\n",
    "   \n",
    "   if (gSystem->AccessPathName( fname ))  // file does not exist in local directory\n",
    "      gSystem->Exec(\"curl -O http://root.cern.ch/files/tmva_class_example.root\");\n",
    "   \n",
    "   TFile *input = TFile::Open( fname );\n",
    "   \n",
    "   std::cout << \"--- TMVAClassification       : Using input file: \" << input->GetName() << std::endl;\n",
    "   \n",
    "   // --- Register the training and test trees\n",
    "\n",
    "   TTree *signal     = (TTree*)input->Get(\"TreeS\");\n",
    "   TTree *background = (TTree*)input->Get(\"TreeB\");\n",
    "   \n",
    "   // global event weights per tree (see below for setting event-wise weights)\n",
    "   Double_t signalWeight     = 1.0;\n",
    "   Double_t backgroundWeight = 1.0;\n",
    "   \n",
    "   // You can add an arbitrary number of signal or background trees\n",
    "   factory->AddSignalTree    ( signal,     signalWeight     );\n",
    "   factory->AddBackgroundTree( background, backgroundWeight );\n",
    "   \n",
    "   // To give different trees for training and testing, do as follows:\n",
    "   //    factory->AddSignalTree( signalTrainingTree, signalTrainWeight, \"Training\" );\n",
    "   //    factory->AddSignalTree( signalTestTree,     signalTestWeight,  \"Test\" );\n",
    "   \n",
    "   // Use the following code instead of the above two or four lines to add signal and background\n",
    "   // training and test events \"by hand\"\n",
    "   // NOTE that in this case one should not give expressions (such as \"var1+var2\") in the input\n",
    "   //      variable definition, but simply compute the expression before adding the event\n",
    "   //\n",
    "   //     // --- begin ----------------------------------------------------------\n",
    "   //     std::vector<Double_t> vars( 4 ); // vector has size of number of input variables\n",
    "   //     Float_t  treevars[4], weight;\n",
    "   //     \n",
    "   //     // Signal\n",
    "   //     for (UInt_t ivar=0; ivar<4; ivar++) signal->SetBranchAddress( Form( \"var%i\", ivar+1 ), &(treevars[ivar]) );\n",
    "   //     for (UInt_t i=0; i<signal->GetEntries(); i++) {\n",
    "   //        signal->GetEntry(i);\n",
    "   //        for (UInt_t ivar=0; ivar<4; ivar++) vars[ivar] = treevars[ivar];\n",
    "   //        // add training and test events; here: first half is training, second is testing\n",
    "   //        // note that the weight can also be event-wise\n",
    "   //        if (i < signal->GetEntries()/2.0) factory->AddSignalTrainingEvent( vars, signalWeight );\n",
    "   //        else                              factory->AddSignalTestEvent    ( vars, signalWeight );\n",
    "   //     }\n",
    "   //   \n",
    "   //     // Background (has event weights)\n",
    "   //     background->SetBranchAddress( \"weight\", &weight );\n",
    "   //     for (UInt_t ivar=0; ivar<4; ivar++) background->SetBranchAddress( Form( \"var%i\", ivar+1 ), &(treevars[ivar]) );\n",
    "   //     for (UInt_t i=0; i<background->GetEntries(); i++) {\n",
    "   //        background->GetEntry(i);\n",
    "   //        for (UInt_t ivar=0; ivar<4; ivar++) vars[ivar] = treevars[ivar];\n",
    "   //        // add training and test events; here: first half is training, second is testing\n",
    "   //        // note that the weight can also be event-wise\n",
    "   //        if (i < background->GetEntries()/2) factory->AddBackgroundTrainingEvent( vars, backgroundWeight*weight );\n",
    "   //        else                                factory->AddBackgroundTestEvent    ( vars, backgroundWeight*weight );\n",
    "   //     }\n",
    "         // --- end ------------------------------------------------------------\n",
    "   //\n",
    "   // --- end of tree registration \n",
    "\n",
    "   // Set individual event weights (the variables must exist in the original TTree)\n",
    "   //    for signal    : factory->SetSignalWeightExpression    (\"weight1*weight2\");\n",
    "   //    for background: factory->SetBackgroundWeightExpression(\"weight1*weight2\");\n",
    "   factory->SetBackgroundWeightExpression( \"weight\" );\n",
    "\n",
    "   // Apply additional cuts on the signal and background samples (can be different)\n",
    "   TCut mycuts = \"\"; // for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "   TCut mycutb = \"\"; // for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "   // Tell the factory how to use the training and testing events\n",
    "   //\n",
    "   // If no numbers of events are given, half of the events in the tree are used \n",
    "   // for training, and the other half for testing:\n",
    "   //    factory->PrepareTrainingAndTestTree( mycut, \"SplitMode=random:!V\" );\n",
    "   // To also specify the number of testing events, use:\n",
    "   //    factory->PrepareTrainingAndTestTree( mycut,\n",
    "   //                                         \"NSigTrain=3000:NBkgTrain=3000:NSigTest=3000:NBkgTest=3000:SplitMode=Random:!V\" );\n",
    "   factory->PrepareTrainingAndTestTree( mycuts, mycutb,\n",
    "                                        \"nTrain_Signal=1000:nTrain_Background=1000:SplitMode=Random:NormMode=NumEvents:!V\" );\n",
    "\n",
    "   // ---- Book MVA methods\n",
    "   //\n",
    "   // Please lookup the various method configuration options in the corresponding cxx files, eg:\n",
    "   // src/MethoCuts.cxx, etc, or here: http://tmva.sourceforge.net/optionRef.html\n",
    "   // it is possible to preset ranges in the option string in which the cut optimisation should be done:\n",
    "   // \"...:CutRangeMin[2]=-1:CutRangeMax[2]=1\"...\", where [2] is the third input variable\n",
    "\n",
    "   // Cut optimisation\n",
    "   if (Use[\"Cuts\"])\n",
    "      factory->BookMethod( TMVA::Types::kCuts, \"Cuts\",\n",
    "                           \"!H:!V:FitMethod=MC:EffSel:SampleSize=200000:VarProp=FSmart\" );\n",
    "\n",
    "   if (Use[\"CutsD\"])\n",
    "      factory->BookMethod( TMVA::Types::kCuts, \"CutsD\",\n",
    "                           \"!H:!V:FitMethod=MC:EffSel:SampleSize=200000:VarProp=FSmart:VarTransform=Decorrelate\" );\n",
    "\n",
    "   if (Use[\"CutsPCA\"])\n",
    "      factory->BookMethod( TMVA::Types::kCuts, \"CutsPCA\",\n",
    "                           \"!H:!V:FitMethod=MC:EffSel:SampleSize=200000:VarProp=FSmart:VarTransform=PCA\" );\n",
    "\n",
    "   if (Use[\"CutsGA\"])\n",
    "      factory->BookMethod( TMVA::Types::kCuts, \"CutsGA\",\n",
    "                           \"H:!V:FitMethod=GA:CutRangeMin[0]=-10:CutRangeMax[0]=10:VarProp[1]=FMax:EffSel:Steps=30:Cycles=3:PopSize=400:SC_steps=10:SC_rate=5:SC_factor=0.95\" );\n",
    "\n",
    "   if (Use[\"CutsSA\"])\n",
    "      factory->BookMethod( TMVA::Types::kCuts, \"CutsSA\",\n",
    "                           \"!H:!V:FitMethod=SA:EffSel:MaxCalls=150000:KernelTemp=IncAdaptive:InitialTemp=1e+6:MinTemp=1e-6:Eps=1e-10:UseDefaultScale\" );\n",
    "\n",
    "   // Likelihood (\"naive Bayes estimator\")\n",
    "   if (Use[\"Likelihood\"])\n",
    "      factory->BookMethod( TMVA::Types::kLikelihood, \"Likelihood\",\n",
    "                           \"H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmoothBkg[1]=10:NSmooth=1:NAvEvtPerBin=50\" );\n",
    "\n",
    "   // Decorrelated likelihood\n",
    "   if (Use[\"LikelihoodD\"])\n",
    "      factory->BookMethod( TMVA::Types::kLikelihood, \"LikelihoodD\",\n",
    "                           \"!H:!V:TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmooth=5:NAvEvtPerBin=50:VarTransform=Decorrelate\" );\n",
    "\n",
    "   // PCA-transformed likelihood\n",
    "   if (Use[\"LikelihoodPCA\"])\n",
    "      factory->BookMethod( TMVA::Types::kLikelihood, \"LikelihoodPCA\",\n",
    "                           \"!H:!V:!TransformOutput:PDFInterpol=Spline2:NSmoothSig[0]=20:NSmoothBkg[0]=20:NSmooth=5:NAvEvtPerBin=50:VarTransform=PCA\" ); \n",
    "\n",
    "   // Use a kernel density estimator to approximate the PDFs\n",
    "   if (Use[\"LikelihoodKDE\"])\n",
    "      factory->BookMethod( TMVA::Types::kLikelihood, \"LikelihoodKDE\",\n",
    "                           \"!H:!V:!TransformOutput:PDFInterpol=KDE:KDEtype=Gauss:KDEiter=Adaptive:KDEFineFactor=0.3:KDEborder=None:NAvEvtPerBin=50\" ); \n",
    "\n",
    "   // Use a variable-dependent mix of splines and kernel density estimator\n",
    "   if (Use[\"LikelihoodMIX\"])\n",
    "      factory->BookMethod( TMVA::Types::kLikelihood, \"LikelihoodMIX\",\n",
    "                           \"!H:!V:!TransformOutput:PDFInterpolSig[0]=KDE:PDFInterpolBkg[0]=KDE:PDFInterpolSig[1]=KDE:PDFInterpolBkg[1]=KDE:PDFInterpolSig[2]=Spline2:PDFInterpolBkg[2]=Spline2:PDFInterpolSig[3]=Spline2:PDFInterpolBkg[3]=Spline2:KDEtype=Gauss:KDEiter=Nonadaptive:KDEborder=None:NAvEvtPerBin=50\" ); \n",
    "\n",
    "   // Test the multi-dimensional probability density estimator\n",
    "   // here are the options strings for the MinMax and RMS methods, respectively:\n",
    "   //      \"!H:!V:VolumeRangeMode=MinMax:DeltaFrac=0.2:KernelEstimator=Gauss:GaussSigma=0.3\" );\n",
    "   //      \"!H:!V:VolumeRangeMode=RMS:DeltaFrac=3:KernelEstimator=Gauss:GaussSigma=0.3\" );\n",
    "   if (Use[\"PDERS\"])\n",
    "      factory->BookMethod( TMVA::Types::kPDERS, \"PDERS\",\n",
    "                           \"!H:!V:NormTree=T:VolumeRangeMode=Adaptive:KernelEstimator=Gauss:GaussSigma=0.3:NEventsMin=400:NEventsMax=600\" );\n",
    "\n",
    "   if (Use[\"PDERSD\"])\n",
    "      factory->BookMethod( TMVA::Types::kPDERS, \"PDERSD\",\n",
    "                           \"!H:!V:VolumeRangeMode=Adaptive:KernelEstimator=Gauss:GaussSigma=0.3:NEventsMin=400:NEventsMax=600:VarTransform=Decorrelate\" );\n",
    "\n",
    "   if (Use[\"PDERSPCA\"])\n",
    "      factory->BookMethod( TMVA::Types::kPDERS, \"PDERSPCA\",\n",
    "                           \"!H:!V:VolumeRangeMode=Adaptive:KernelEstimator=Gauss:GaussSigma=0.3:NEventsMin=400:NEventsMax=600:VarTransform=PCA\" );\n",
    "\n",
    "   // Multi-dimensional likelihood estimator using self-adapting phase-space binning\n",
    "   if (Use[\"PDEFoam\"])\n",
    "      factory->BookMethod( TMVA::Types::kPDEFoam, \"PDEFoam\",\n",
    "                           \"!H:!V:SigBgSeparate=F:TailCut=0.001:VolFrac=0.0666:nActiveCells=500:nSampl=2000:nBin=5:Nmin=100:Kernel=None:Compress=T\" );\n",
    "\n",
    "   if (Use[\"PDEFoamBoost\"])\n",
    "      factory->BookMethod( TMVA::Types::kPDEFoam, \"PDEFoamBoost\",\n",
    "                           \"!H:!V:Boost_Num=30:Boost_Transform=linear:SigBgSeparate=F:MaxDepth=4:UseYesNoCell=T:DTLogic=MisClassificationError:FillFoamWithOrigWeights=F:TailCut=0:nActiveCells=500:nBin=20:Nmin=400:Kernel=None:Compress=T\" );\n",
    "\n",
    "   // K-Nearest Neighbour classifier (KNN)\n",
    "   if (Use[\"KNN\"])\n",
    "      factory->BookMethod( TMVA::Types::kKNN, \"KNN\",\n",
    "                           \"H:nkNN=20:ScaleFrac=0.8:SigmaFact=1.0:Kernel=Gaus:UseKernel=F:UseWeight=T:!Trim\" );\n",
    "\n",
    "   // H-Matrix (chi2-squared) method\n",
    "   if (Use[\"HMatrix\"])\n",
    "      factory->BookMethod( TMVA::Types::kHMatrix, \"HMatrix\", \"!H:!V:VarTransform=None\" );\n",
    "\n",
    "   // Linear discriminant (same as Fisher discriminant)\n",
    "   if (Use[\"LD\"])\n",
    "      factory->BookMethod( TMVA::Types::kLD, \"LD\", \"H:!V:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10\" );\n",
    "\n",
    "   // Fisher discriminant (same as LD)\n",
    "   if (Use[\"Fisher\"])\n",
    "      factory->BookMethod( TMVA::Types::kFisher, \"Fisher\", \"H:!V:Fisher:VarTransform=None:CreateMVAPdfs:PDFInterpolMVAPdf=Spline2:NbinsMVAPdf=50:NsmoothMVAPdf=10\" );\n",
    "\n",
    "   // Fisher with Gauss-transformed input variables\n",
    "   if (Use[\"FisherG\"])\n",
    "      factory->BookMethod( TMVA::Types::kFisher, \"FisherG\", \"H:!V:VarTransform=Gauss\" );\n",
    "\n",
    "   // Composite classifier: ensemble (tree) of boosted Fisher classifiers\n",
    "   if (Use[\"BoostedFisher\"])\n",
    "      factory->BookMethod( TMVA::Types::kFisher, \"BoostedFisher\", \n",
    "                           \"H:!V:Boost_Num=20:Boost_Transform=log:Boost_Type=AdaBoost:Boost_AdaBoostBeta=0.2:!Boost_DetailedMonitoring\" );\n",
    "\n",
    "   // Function discrimination analysis (FDA) -- test of various fitters - the recommended one is Minuit (or GA or SA)\n",
    "   if (Use[\"FDA_MC\"])\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_MC\",\n",
    "                           \"H:!V:Formula=(0)+(1)*x0+(2)*x1+(3)*x2+(4)*x3:ParRanges=(-1,1);(-10,10);(-10,10);(-10,10);(-10,10):FitMethod=MC:SampleSize=100000:Sigma=0.1\" );\n",
    "\n",
    "   if (Use[\"FDA_GA\"]) // can also use Simulated Annealing (SA) algorithm (see Cuts_SA options])\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_GA\",\n",
    "                           \"H:!V:Formula=(0)+(1)*x0+(2)*x1+(3)*x2+(4)*x3:ParRanges=(-1,1);(-10,10);(-10,10);(-10,10);(-10,10):FitMethod=GA:PopSize=300:Cycles=3:Steps=20:Trim=True:SaveBestGen=1\" );\n",
    "\n",
    "   if (Use[\"FDA_SA\"]) // can also use Simulated Annealing (SA) algorithm (see Cuts_SA options])\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_SA\",\n",
    "                           \"H:!V:Formula=(0)+(1)*x0+(2)*x1+(3)*x2+(4)*x3:ParRanges=(-1,1);(-10,10);(-10,10);(-10,10);(-10,10):FitMethod=SA:MaxCalls=15000:KernelTemp=IncAdaptive:InitialTemp=1e+6:MinTemp=1e-6:Eps=1e-10:UseDefaultScale\" );\n",
    "\n",
    "   if (Use[\"FDA_MT\"])\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_MT\",\n",
    "                           \"H:!V:Formula=(0)+(1)*x0+(2)*x1+(3)*x2+(4)*x3:ParRanges=(-1,1);(-10,10);(-10,10);(-10,10);(-10,10):FitMethod=MINUIT:ErrorLevel=1:PrintLevel=-1:FitStrategy=2:UseImprove:UseMinos:SetBatch\" );\n",
    "\n",
    "   if (Use[\"FDA_GAMT\"])\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_GAMT\",\n",
    "                           \"H:!V:Formula=(0)+(1)*x0+(2)*x1+(3)*x2+(4)*x3:ParRanges=(-1,1);(-10,10);(-10,10);(-10,10);(-10,10):FitMethod=GA:Converger=MINUIT:ErrorLevel=1:PrintLevel=-1:FitStrategy=0:!UseImprove:!UseMinos:SetBatch:Cycles=1:PopSize=5:Steps=5:Trim\" );\n",
    "\n",
    "   if (Use[\"FDA_MCMT\"])\n",
    "      factory->BookMethod( TMVA::Types::kFDA, \"FDA_MCMT\",\n",
    "                           \"H:!V:Formula=(0)+(1)*x0+(2)*x1+(3)*x2+(4)*x3:ParRanges=(-1,1);(-10,10);(-10,10);(-10,10);(-10,10):FitMethod=MC:Converger=MINUIT:ErrorLevel=1:PrintLevel=-1:FitStrategy=0:!UseImprove:!UseMinos:SetBatch:SampleSize=20\" );\n",
    "\n",
    "   // TMVA ANN: MLP (recommended ANN) -- all ANNs in TMVA are Multilayer Perceptrons\n",
    "   if (Use[\"MLP\"])\n",
    "      factory->BookMethod( TMVA::Types::kMLP, \"MLP\", \"H:!V:NeuronType=tanh:VarTransform=N:NCycles=600:HiddenLayers=N+5:TestRate=5:!UseRegulator\" );\n",
    "\n",
    "   if (Use[\"MLPBFGS\"])\n",
    "      factory->BookMethod( TMVA::Types::kMLP, \"MLPBFGS\", \"H:!V:NeuronType=tanh:VarTransform=N:NCycles=600:HiddenLayers=N+5:TestRate=5:TrainingMethod=BFGS:!UseRegulator\" );\n",
    "\n",
    "   if (Use[\"MLPBNN\"])\n",
    "      factory->BookMethod( TMVA::Types::kMLP, \"MLPBNN\", \"H:!V:NeuronType=tanh:VarTransform=N:NCycles=600:HiddenLayers=N+5:TestRate=5:TrainingMethod=BFGS:UseRegulator\" ); // BFGS training with bayesian regulators\n",
    "\n",
    "   // CF(Clermont-Ferrand)ANN\n",
    "   if (Use[\"CFMlpANN\"])\n",
    "      factory->BookMethod( TMVA::Types::kCFMlpANN, \"CFMlpANN\", \"!H:!V:NCycles=2000:HiddenLayers=N+1,N\"  ); // n_cycles:#nodes:#nodes:...  \n",
    "\n",
    "   // Tmlp(Root)ANN\n",
    "   if (Use[\"TMlpANN\"])\n",
    "      factory->BookMethod( TMVA::Types::kTMlpANN, \"TMlpANN\", \"!H:!V:NCycles=200:HiddenLayers=N+1,N:LearningMethod=BFGS:ValidationFraction=0.3\"  ); // n_cycles:#nodes:#nodes:...\n",
    "\n",
    "   // Support Vector Machine\n",
    "   if (Use[\"SVM\"])\n",
    "      factory->BookMethod( TMVA::Types::kSVM, \"SVM\", \"Gamma=0.25:Tol=0.001:VarTransform=Norm\" );\n",
    "\n",
    "   // Boosted Decision Trees\n",
    "   if (Use[\"BDTG\"]) // Gradient Boost\n",
    "      factory->BookMethod( TMVA::Types::kBDT, \"BDTG\",\n",
    "                           \"!H:!V:NTrees=1000:MinNodeSize=2.5%:BoostType=Grad:Shrinkage=0.10:UseBaggedBoost:BaggedSampleFraction=0.5:nCuts=20:MaxDepth=2\" );\n",
    "\n",
    "   if (Use[\"BDT\"])  // Adaptive Boost\n",
    "      factory->BookMethod( TMVA::Types::kBDT, \"BDT\",\n",
    "                           \"!H:!V:NTrees=850:MinNodeSize=2.5%:MaxDepth=3:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20\" );\n",
    "\n",
    "   if (Use[\"BDTB\"]) // Bagging\n",
    "      factory->BookMethod( TMVA::Types::kBDT, \"BDTB\",\n",
    "                           \"!H:!V:NTrees=400:BoostType=Bagging:SeparationType=GiniIndex:nCuts=20\" );\n",
    "\n",
    "   if (Use[\"BDTD\"]) // Decorrelation + Adaptive Boost\n",
    "      factory->BookMethod( TMVA::Types::kBDT, \"BDTD\",\n",
    "                           \"!H:!V:NTrees=400:MinNodeSize=5%:MaxDepth=3:BoostType=AdaBoost:SeparationType=GiniIndex:nCuts=20:VarTransform=Decorrelate\" );\n",
    "\n",
    "   if (Use[\"BDTF\"])  // Allow Using Fisher discriminant in node splitting for (strong) linearly correlated variables\n",
    "      factory->BookMethod( TMVA::Types::kBDT, \"BDTMitFisher\",\n",
    "                           \"!H:!V:NTrees=50:MinNodeSize=2.5%:UseFisherCuts:MaxDepth=3:BoostType=AdaBoost:AdaBoostBeta=0.5:SeparationType=GiniIndex:nCuts=20\" );\n",
    "\n",
    "   // RuleFit -- TMVA implementation of Friedman's method\n",
    "   if (Use[\"RuleFit\"])\n",
    "      factory->BookMethod( TMVA::Types::kRuleFit, \"RuleFit\",\n",
    "                           \"H:!V:RuleFitModule=RFTMVA:Model=ModRuleLinear:MinImp=0.001:RuleMinDist=0.001:NTrees=20:fEventsMin=0.01:fEventsMax=0.5:GDTau=-1.0:GDTauPrec=0.01:GDStep=0.01:GDNSteps=10000:GDErrScale=1.02\" );\n",
    "\n",
    "   // For an example of the category classifier usage, see: TMVAClassificationCategory\n",
    "\n",
    "   // --------------------------------------------------------------------------------------------------\n",
    "\n",
    "   // ---- Now you can optimize the setting (configuration) of the MVAs using the set of training events\n",
    "\n",
    "   // ---- STILL EXPERIMENTAL and only implemented for BDT's ! \n",
    "   // factory->OptimizeAllMethods(\"SigEffAt001\",\"Scan\");\n",
    "   // factory->OptimizeAllMethods(\"ROCIntegral\",\"FitGA\");\n",
    "\n",
    "   // --------------------------------------------------------------------------------------------------\n",
    "\n",
    "   // ---- Now you can tell the factory to train, test, and evaluate the MVAs\n",
    "\n",
    "   // Train MVAs using the set of training events\n",
    "   factory->TrainAllMethods();\n",
    "\n",
    "   // ---- Evaluate all MVAs using the set of test events\n",
    "   factory->TestAllMethods();\n",
    "\n",
    "   // ----- Evaluate and compare performance of all configured MVAs\n",
    "   factory->EvaluateAllMethods();\n",
    "\n",
    "   // --------------------------------------------------------------\n",
    "\n",
    "   // Save the output\n",
    "   outputFile->Close();\n",
    "\n",
    "   std::cout << \"==> Wrote root file: \" << outputFile->GetName() << std::endl;\n",
    "   std::cout << \"==> TMVAClassification is done!\" << std::endl;\n",
    "\n",
    "   delete factory;\n",
    "\n",
    "   // Launch the GUI for the root macros\n",
    "   if (!gROOT->IsBatch()) TMVA::TMVAGui( outfileName );\n",
    "\n",
    "   return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TMVAClassification();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "std::cout << \"==> TMVAClassification is done!\" << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT Prompt",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
