{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines set up inline plotting, and apply a standard size\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **{'size': 22})\n",
    "# Standard includes\n",
    "import datetime\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define query for the tasks\n",
    "taskQuery = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\"term\":{\"reqid\": \"18663\"}}\n",
    "                #{\"term\":{\"transhome\": \"AthDerivation-21.2.23.0\"}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the queries\n",
    "es = Elasticsearch(['atlas-kibana-dev.mwt2.org'],timeout=120)\n",
    "\n",
    "# Tasks\n",
    "taskIndex = \"tasks*\"\n",
    "tasks = scan(es, query=taskQuery, index=taskIndex, scroll='5m', timeout=\"5m\", size=1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the trains and the quantities for collection\n",
    "quantities = [\n",
    "    'cpuconsumptiontime',\n",
    "    'nevents',\n",
    "    'starttime',\n",
    "    'endtime',\n",
    "    'timeExe',\n",
    "    'timeSetup',\n",
    "    'timeGetJob',\n",
    "    'timeStageIn',\n",
    "    'timeStageOut',\n",
    "    'actualcorecount',\n",
    "    'wall_time',\n",
    "    'inputfilebytes',\n",
    "    'outputfilebytes',\n",
    "    'Max_PSS_per_core'\n",
    "]\n",
    "\n",
    "# used for mc16e validation\n",
    "trainsAndIDs = [\n",
    "    (['DAOD_STDM5','DAOD_EGAM4','DAOD_EGAM2','DAOD_EXOT12','DAOD_EXOT9'],[],{}),\n",
    "    (['DAOD_SUSY12','DAOD_STDM3','DAOD_EXOT15','DAOD_JETM3','DAOD_EXOT19','DAOD_HIGG4D6','DAOD_HIGG6D1','DAOD_HIGG1D1'],[],{}),\n",
    "    (['DAOD_EXOT22','DAOD_SUSY4','DAOD_JETM11','DAOD_EXOT21','DAOD_STDM7','DAOD_SUSY8','DAOD_SUSY10'],[],{}),\n",
    "    (['DAOD_HIGG2D1'],[],{}),\n",
    "    (['DAOD_JETM9','DAOD_STDM4','DAOD_FTAG4'],[],{}),\n",
    "    (['DAOD_MUON2','DAOD_HIGG4D4','DAOD_JETM7','DAOD_BPHY7','DAOD_EXOT17','DAOD_BPHY5','DAOD_EGAM7','DAOD_HIGG1D2'],[],{}),\n",
    "    (['DAOD_STDM2','DAOD_SUSY18','DAOD_EXOT3','DAOD_EGAM1','DAOD_EGAM5','DAOD_EXOT2','DAOD_SUSY3','DAOD_EXOT5','DAOD_HIGG6D2'],[],{}),\n",
    "    (['DAOD_JETM12','DAOD_EGAM3','DAOD_JETM10'],[],{}),\n",
    "    (['DAOD_TOPQ1'],[],{}),\n",
    "    (['DAOD_TCAL1','DAOD_EXOT10','DAOD_HIGG2D5'],[],{}),\n",
    "    (['DAOD_SUSY1'],[],{}),\n",
    "    (['DAOD_SUSY9'],[],{}),\n",
    "    (['DAOD_EXOT13','DAOD_SUSY5','DAOD_SUSY7','DAOD_EXOT8','DAOD_EXOT4','DAOD_HIGG4D2'],[],{}),\n",
    "    (['DAOD_STDM5','DAOD_EGAM4','DAOD_EGAM2','DAOD_EXOT12','DAOD_SUSY9','DAOD_EXOT9'],[],{}),\n",
    "    (['DAOD_TAUP1','DAOD_HIGG4D5','DAOD_TOPQ5','DAOD_JETM4','DAOD_HIGG4D3','DAOD_SUSY16','DAOD_EXOT7'],[],{}),\n",
    "    (['DAOD_HIGG8D1','DAOD_JETM6','DAOD_MUON1','DAOD_SUSY6','DAOD_JETM1','DAOD_MUON0','DAOD_TAUP3'],[],{}),\n",
    "    (['DAOD_EGAM9','DAOD_EXOT20','DAOD_SUSY11','DAOD_EXOT6','DAOD_SUSY2','DAOD_HIGG4D1','DAOD_BPHY1','DAOD_BPHY4'],[],{})\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching tasks =  51\n",
      "Time to extract information =  0.0421378742903471\n"
     ]
    }
   ],
   "source": [
    "# Get the IDs matching each train\n",
    "start_time = timeit.default_timer()\n",
    "matchingTasks = 0\n",
    "for res in tasks:\n",
    "    if 'output_formats' in res['_source'].keys():\n",
    "        for item in trainsAndIDs:\n",
    "            train = item[0]\n",
    "            if set(res['_source']['output_formats']) == set(train):\n",
    "                item[1].append(res['_id'])\n",
    "                matchingTasks += 1\n",
    "print(\"Total matching tasks = \",matchingTasks)\n",
    "print(\"Time to extract information = \",timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up query for the jobs relevant to the trains\n",
    "start_time = timeit.default_timer()\n",
    "taskCounter = 0\n",
    "jobIndex = \"jobs_archive_2018*\"\n",
    "\n",
    "to_read = quantities\n",
    "to_read.append('jeditaskid')\n",
    "to_read.append('transformation')\n",
    "\n",
    "for item in trainsAndIDs: # Loop over trains\n",
    "    for theId in item[1]: # Loop over tasks for that train\n",
    "        taskCounter += 1\n",
    "        jobQuery = {\n",
    "            \"_source\": to_read,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"term\":{\"jeditaskid\": theId}},\n",
    "                        {\"term\":{\"jobstatus\": \"finished\"}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # query the jobs\n",
    "        jobs = scan(es, query=jobQuery, index=jobIndex, scroll='5m', timeout=\"5m\", size=1000)\n",
    "        for res in jobs: # Loop over jobs from that task\n",
    "            for quantity in quantities: # get the relevant quantities\n",
    "                if quantity in res['_source'].keys(): \n",
    "                    if quantity not in item[2].keys(): # store the quantities in a long list\n",
    "                        item[2][quantity] = [res['_source'][quantity]]\n",
    "                    else:\n",
    "                        item[2][quantity].append(res['_source'][quantity])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the per-train information\n",
    "pickle.dump(trainsAndIDs,open(\"mc_request18663.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
