{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Automating support for ATLAS Distributed Computing Operations\n",
    "\n",
    "#Three sentence description: This project delivers a system to automate workflows usually done by\n",
    "#human shifters. The first milestone is to provide a recommendation\n",
    "#system with an integrated feedback loop. The second milestone is to\n",
    "#automate recommendations and decision based on machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines set up inline plotting, and apply a standard size\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "matplotlib.rc('font', **{'size': 15})\n",
    "from elasticsearch.helpers import scan\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487743200000 1487656800000 1487138400000 1485064800000\n",
      "86400000 604800000\n"
     ]
    }
   ],
   "source": [
    "##converting to epoch time\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "today = time.strftime('%d/%m/%Y')\n",
    "yesterday = (datetime.date.today() - datetime.timedelta(1)).strftime(\"%d/%m/%Y\")\n",
    "last_week = (datetime.date.today() - datetime.timedelta(7)).strftime(\"%d/%m/%Y\")\n",
    "last_month = (datetime.date.today() - datetime.timedelta(31)).strftime(\"%d/%m/%Y\")\n",
    "today_epoch = int(time.mktime(datetime.datetime.strptime(today, \"%d/%m/%Y\").timetuple()))*1000\n",
    "yesterday_epoch = int(time.mktime(datetime.datetime.strptime(yesterday, \"%d/%m/%Y\").timetuple()))*1000\n",
    "last_week_epoch = int(time.mktime(datetime.datetime.strptime(last_week, \"%d/%m/%Y\").timetuple()))*1000\n",
    "last_month_epoch = int(time.mktime(datetime.datetime.strptime(last_month, \"%d/%m/%Y\").timetuple()))*1000\n",
    "print(today_epoch,yesterday_epoch,last_week_epoch,last_month_epoch)\n",
    "print(today_epoch-yesterday_epoch,today_epoch-last_week_epoch)\n",
    "##add three zeros because the time format specified in the script is milliseconds (epoch_millis).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the query \n",
    "# All transfer submits in the period\n",
    "\n",
    "\n",
    "# Define the query\n",
    "new_query = {\n",
    "  \"size\": 0,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "    \"must\": [\n",
    "    {\n",
    "      \"range\": {\n",
    "        \"@timestamp\": {\n",
    "        \"gte\": last_month_epoch,\n",
    "        \"lte\": today_epoch,\n",
    "        \"format\": \"epoch_millis\"\n",
    "         }\n",
    "        }\n",
    "       },\n",
    "      {\"term\": {\"event_type\": \"transfer-done\"}},\n",
    "      {\"term\": {\"payload.scope\": \"data16_13TeV\"}}\n",
    "    ]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the query \n",
    "# All transfer failures in period\n",
    "\n",
    "\n",
    "# Define the query\n",
    "my_query = {\n",
    "  \"size\": 0,\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "    \"must\": [\n",
    "    {\n",
    "      \"range\": {\n",
    "        \"@timestamp\": {\n",
    "        \"gte\": last_month_epoch,\n",
    "        \"lte\": today_epoch,\n",
    "        \"format\": \"epoch_millis\"\n",
    "         }\n",
    "        }\n",
    "       },\n",
    "      {\"term\": {\"event_type\": \"transfer-failed\"}},\n",
    "      {\"term\": {\"payload.scope\": \"data16_13TeV\"}}\n",
    "    ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(['atlas-kibana.mwt2.org:9200'],timeout=60) \n",
    "my_index = \"rucio-events-2017*\"\n",
    "scroll = scan(es, query=my_query, index=my_index, scroll='5m', timeout=\"5m\", size=100)\n",
    "new_scroll = scan(es, query=new_query, index=my_index, scroll='5m', timeout=\"5m\", size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for res in new_scroll:\n",
    "#    print(res['_source']['event_type'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'_source': {'@timestamp': '2017-02-09T00:36:16.290Z', 'created_at': '2017-02-09 00:36:15', 'payload': {'started_at': '2017-02-08 20:15:28', 'transferred_at': '2017-02-09 00:33:12', 'transfer-link': 'https://fts3.cern.ch:8449/fts3/ftsmon/#/job/04db112d-2f61-5550-b0ee-c64327fcdc7b', 'created_at': None, 'submitted_at': '2017-02-08 20:11:37', 'protocol': 'srm', 'transfer-id': '04db112d-2f61-5550-b0ee-c64327fcdc7b', 'src-type': 'TAPE', 'previous-request-id': '2a371460bbba492f800f977d94605bf2', 'checksum-adler': '0a8e1498', 'src-rse': 'IN2P3-CC_DATATAPE', 'scope': 'data16_13TeV', 'name': 'data16_13TeV.00301932.physics_Main.daq.RAW._lb0651._SFO-4._0002.data', 'file-size': 2621880440, 'guid': None, 'bytes': 2621880440, 'activity': 'User Subscriptions', 'tool-id': 'rucio-conveyor', 'dst-rse': 'IN2P3-CC_DATADISK', 'src-url': 'srm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/pnfs/in2p3.fr/data/atlas/atlasdatatape/data16_13TeV/RAW/other/data16_13TeV.00301932.physics_Main.daq.RAW/data16_13TeV.00301932.physics_Main.daq.RAW._lb0651._SFO-4._0002.data', 'reason': 'error on the bring online request: [SE][StatusOfBringOnlineRequest][SRM_FAILURE] Request lifetime expired. ', 'request-id': '83a1998fb9b844a9804116fe071c65bc', 'checksum-md5': None, 'dst-type': 'DISK', 'duration': 15464, 'dst-url': 'srm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/pnfs/in2p3.fr/data/atlas/atlasdatadisk/rucio/data16_13TeV/0c/b8/data16_13TeV.00301932.physics_Main.daq.RAW._lb0651._SFO-4._0002.data', 'transfer-endpoint': 'https://fts3.cern.ch:8446'}, 'event_type': 'transfer-failed', '@version': '1', 'type': 'rucio-event'}, 'sort': [28], '_index': 'rucio-events-2017.02.09', '_type': 'rucio-event', '_id': 'AVogTJ8lsxHoI1r8K_p9', '_score': None}\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "format  NTUP_TRIGRATE.10666835._032220.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._032630.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._032579.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._032921.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._026447.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._032635.pool.root.1  not in format_list\n",
      "120000\n",
      "format  NTUP_TRIGRATE.10666835._032641.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._022627.pool.root.1  not in format_list\n",
      "130000\n",
      "format  NTUP_TRIGRATE.10666835._032630.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._034109.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._035355.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._035139.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._034451.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._037870.pool.root.1  not in format_list\n",
      "140000\n",
      "format  NTUP_TRIGRATE.10666835._026534.pool.root.1  not in format_list\n",
      "150000\n",
      "format  NTUP_TRIGRATE.10666835._032054.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._026505.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGRATE.10666835._026505.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._032074.pool.root.1  not in format_list\n",
      "160000\n",
      "format  NTUP_TRIGCOST.10666835._022767.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._022740.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._035183.pool.root.1  not in format_list\n",
      "170000\n",
      "180000\n",
      "format  NTUP_TRIGRATE.10666835._034531.pool.root.1  not in format_list\n",
      "format  NTUP_TRIGCOST.10666835._034372.pool.root.1  not in format_list\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Put this on ice for now\n",
    "\"\"\"\n",
    "error_list = ['SOURCE CHECKSUM MISMATCH User defined checksum and source checksum do not match',\n",
    "              'TRANSFER SOURCE CHECKSUM MISMATCH User defined checksum and source checksum do not match',\n",
    "              'TRANSFER CHECKSUM MISMATCH Source and destination checksums do not match',\n",
    "              'the server responded with an error 421',\n",
    "              'the server responded with an error 451',\n",
    "              'the server responded with an error 500',\n",
    "              'the server responded with an error 530',\n",
    "              'TRANSFER SOURCE CHECKSUM srm-ifce err: Communication error on send',\n",
    "              'Transfer process died with:',\n",
    "              'TRANSFER  globus_xio: Unable to connect to',\n",
    "              'TRANSFER DESTINATION MAKE_PARENT srm-ifce err: Permission denied',\n",
    "              'DESTINATION SRM_PUTDONE call to srm_ifce error:',\n",
    "              'DESTINATION OVERWRITE srm-ifce err: Communication error on send',\n",
    "              'TRANSFER DESTINATION OVERWRITE srm-ifce err: Communication error on send',\n",
    "              'TRANSFER SOURCE SRM_GET_TURL error on the turl request ',\n",
    "              'TRANSFER DESTINATION SRM_PUTDONE call to srm_ifce error:',\n",
    "              'Operation timed out, operation timeout',\n",
    "              'User specified source file size is', #X but stat returned Y\n",
    "             ]\n",
    "\"\"\"\n",
    "\n",
    "exceptions = ('[gfalt_copy_file][perform_copy][srm_plugin_filecopy][srm_resolve_turls][srm_resolve_put_turl] DESTINATION OVERWRITE [srm_plugin_prepare_dest_put][srm_plugin_delete_existing_copy][gfal_srm_unlinkG][gfal_srm_rm_srmv2_internal] error reported from srm_ifce, [SE][srmRm][SRM_AUTHORIZATION_FAILURE] No approachable VFS found for user!',\n",
    "              'error on the bring online request',\n",
    "              'bring-online timeout has been exceeded',\n",
    "              'Transfer process died with:')\n",
    "\n",
    "count=0\n",
    "size_failures = []\n",
    "endpointPairs = {}\n",
    "problem_sources = {}\n",
    "problem_destinations = {}\n",
    "duration_failures = []\n",
    "reasons = {}\n",
    "failure_types = {}\n",
    "data_types = {}\n",
    "for res in scroll:\n",
    "    if not count%10000:  print(count)\n",
    "    if count<1: print(res)\n",
    "    count += 1\n",
    "    if res['_source']['event_type']=='transfer-failed':\n",
    "        name = res['_source']['payload']['name']\n",
    "        size_failures.append(res['_source']['payload']['file-size'])\n",
    "        duration_failures.append(res['_source']['payload']['duration'])\n",
    "        source = res['_source']['payload']['src-rse']\n",
    "        destination = res['_source']['payload']['dst-rse']\n",
    "        reason = res['_source']['payload']['reason']\n",
    "        src_type = res['_source']['payload']['src-type']\n",
    "        pair = (source,destination)\n",
    "        if pair not in endpointPairs.keys():\n",
    "            endpointPairs[pair] = 1\n",
    "        if pair in endpointPairs.keys():\n",
    "            endpointPairs[pair] = endpointPairs[pair]+1\n",
    "            \n",
    "        if src_type not in failure_types.keys():\n",
    "            failure_types[src_type] = 1\n",
    "        if src_type in failure_types.keys():\n",
    "            failure_types[src_type] = failure_types[src_type]+1   \n",
    "            \n",
    "        if source not in problem_sources.keys():\n",
    "            problem_sources[source] = 1\n",
    "        if source in problem_sources.keys():\n",
    "            problem_sources[source] = problem_sources[source]+1\n",
    "        \n",
    "        if destination not in problem_destinations.keys():\n",
    "            problem_destinations[destination] = 1\n",
    "        if destination in problem_destinations.keys():\n",
    "            problem_destinations[destination] = problem_destinations[destination]+1\n",
    "        \n",
    "        \n",
    "        #categorising the error messages\n",
    "        for word in exceptions:\n",
    "            if not word in reasons.keys() and word in reason:\n",
    "                reasons[word] = 1\n",
    "            if word in reasons.keys() and word in reason:\n",
    "                reasons[word] += 1\n",
    "\n",
    "        for word in reason.split(' '):\n",
    "            if not word.isupper() and not any(part in reason for part in exceptions):\n",
    "                if \" \".join(item for item in reason.split(' ')[:reason.split(' ').index(word)]) not in reasons.keys():\n",
    "                    reasons[\" \".join(item for item in reason.split(' ')[:reason.split(' ').index(word)])] = 1\n",
    "                if \" \".join(item for item in reason.split(' ')[:reason.split(' ').index(word)]) in reasons.keys():\n",
    "                    reasons[\" \".join(item for item in reason.split(' ')[:reason.split(' ').index(word)])] += 1\n",
    "                break\n",
    "         \n",
    "        #data format\n",
    "        \n",
    "        format_list = ['HIST','log','AOD','DAOD','RAW','DRAW','ESD','DESDM']\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for word in name.split('.'):\n",
    "            if name.split('.').index(word) == 0:\n",
    "                for element in format_list:\n",
    "                    if (word.split('_')[0] == element):\n",
    "                        if element not in data_types.keys():\n",
    "                            data_types[element] = 1\n",
    "                        if element in data_types.keys():\n",
    "                            data_types[element] += 1\n",
    "                        counter += 1\n",
    "            if name.split('.').index(word) == 4:\n",
    "                for element in format_list:\n",
    "                    if (word.split('_')[0] == element):\n",
    "                        if element not in data_types.keys():\n",
    "                            data_types[element] = 1\n",
    "                        if element in data_types.keys():\n",
    "                            data_types[element] += 1\n",
    "                        counter += 1\n",
    "        \n",
    "        if counter == 0:\n",
    "            print('format ',name,' not in format_list' )\n",
    "        \n",
    "       \n",
    "                    \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "           # if reason not in reasons.keys():\n",
    "           #     reasons[reason] = 1\n",
    "           # if reason in reasons.keys():\n",
    "           #     reasons[reason] = reasons[reason]+1\n",
    "           \n",
    "   \n",
    "    #check if string is in a list of strings ex: looking for \"or\" in [\"red\",\"orange\"] will output True\n",
    "     #result = any(word_to_check in word for word in worldlist)\n",
    "    #conversely:\n",
    "    #any(substring in string for substring in substring_list)\n",
    "#It will return True if any of the substrings in substring_list is contained in string.\n",
    "        \n",
    "    #string manipulation for the reasons:\n",
    "    #word.split(' ')  # Split on whitespace\n",
    "#word.startswith(\"H\") word.endswith(\"H\")\n",
    "#uppercase string.upper() lowercase string.lower()\n",
    "#word.isdigit()         #test if string contains digits\n",
    "          \n",
    "count = 0\n",
    "pairs = {}\n",
    "sources = {}\n",
    "destinations = {}\n",
    "size_all = []\n",
    "duration_all = []\n",
    "all_types = {}\n",
    "for res in new_scroll:\n",
    "    if not count%10000: print(count)\n",
    "    if count<1: print(res)\n",
    "    count += 1\n",
    "    if res['_source']['event_type']=='transfer-done':\n",
    "        size_all.append(res['_source']['payload']['file-size'])\n",
    "        duration_all.append(res['_source']['payload']['duration'])\n",
    "        new_source = res['_source']['payload']['src-rse']\n",
    "        new_destination = res['_source']['payload']['dst-rse']\n",
    "        src_type = res['_source']['payload']['src-type']\n",
    "        new_pair = (new_source,new_destination)\n",
    "        if new_pair not in pairs.keys():\n",
    "            pairs[new_pair] = 1\n",
    "        if new_pair in pairs.keys():\n",
    "            pairs[new_pair] = pairs[new_pair]+1\n",
    "\n",
    "        if new_source not in sources.keys():\n",
    "            sources[new_source] = 1\n",
    "        if new_source in sources.keys():\n",
    "            sources[new_source] = sources[new_source]+1\n",
    "        \n",
    "        if new_destination not in destinations.keys():\n",
    "            destinations[new_destination] = 1\n",
    "        if new_destination in destinations.keys():\n",
    "            destinations[new_destination] = destinations[new_destination]+1\n",
    "            \n",
    "        if src_type not in all_types.keys():\n",
    "            all_types[src_type] = 1\n",
    "        if src_type in all_types.keys():\n",
    "            all_types[src_type] = all_types[src_type]+1 \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "bin_width = int(max(size_failures)/bins)\n",
    "plt.hist(size_failures,bins=np.arange(0, max(size_failures) + bin_width, bin_width))\n",
    "plt.xlabel('File Size')\n",
    "plt.ylabel('Failures')\n",
    "plt.title('Failures as a Function of File Size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_width = int(max(duration_failures)/bins)\n",
    "plt.hist(duration_failures,bins=np.arange(0, max(duration_failures) + bin_width, bin_width))\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Failures')\n",
    "plt.title('Failures as a Function of Event Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "bin_width = int(max(size_all)/bins)\n",
    "plt.hist(size_all,bins=np.arange(0, max(size_failures) + bin_width, bin_width))\n",
    "plt.xlabel('File Size')\n",
    "plt.ylabel('Transfers')\n",
    "plt.title('Transfers as a Function of File Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_width = int(max(duration_all)/bins)\n",
    "plt.hist(duration_all,bins=np.arange(0, max(duration_all) + bin_width, bin_width))\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Transfers')\n",
    "plt.title('Transfers as a Function of Duration')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalising transfer duration plot\n",
    "\n",
    "hist_duration_failures = np.histogram(duration_failures,bins, range = (0,max(duration_failures)))\n",
    "hist_duration_all      = np.histogram(duration_all,bins,range=(0,max(duration_all)))\n",
    "\n",
    "duration_values = []\n",
    "for i in range(len(hist_duration_failures[0])):\n",
    "    duration_values.append(hist_duration_failures[0][i]/(hist_duration_failures[0][i] + hist_duration_all[0][i]))\n",
    "    \n",
    "duration_norm = (duration_values,np.arange(0,max(duration_failures),max(duration_failures)/bins))\n",
    "\n",
    "plt.bar(duration_norm[1],duration_norm[0],width=max(duration_failures)/bins)\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Failure Rate')\n",
    "plt.title('Transfer Duration Failure Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalising file size plot\n",
    "# syntax numpy.histogram(a, bins=10, range=None, normed=False, weights=None, density=None)\n",
    "\n",
    "failures = np.histogram(size_failures, bins, range = (0 , max(size_failures)) ) \n",
    "all_transfers = np.histogram(size_all, bins, range = (0 , max(size_failures)) )\n",
    "\n",
    "values = []\n",
    "for i in range(len(failures[0])):\n",
    "    values.append(failures[0][i]/(failures[0][i]+all_transfers[0][i]))\n",
    "\n",
    "failures_norm = (values,np.arange(0, max(size_failures), max(size_failures)/(bins)))\n",
    "\n",
    "plt.bar(failures_norm[1],failures_norm[0],width=max(size_failures)/bins)\n",
    "plt.xlabel('File Size')\n",
    "plt.ylabel('Failure Rate')\n",
    "plt.title('File Size Failure Rate')\n",
    "\n",
    "#for i in range(len(values)):\n",
    "#    print(values[i],'\\t\\t',failures[0][i],all_transfers[0][i],)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_endpointPairs = sorted(endpointPairs.items(), key=operator.itemgetter(1))\n",
    "sorted_problem_sources = sorted(problem_sources.items(),key=operator.itemgetter(1))\n",
    "sorted_problem_destinations = sorted(problem_destinations.items(),key=operator.itemgetter(1))\n",
    "\n",
    "#for item in sorted_endpointPairs:\n",
    "    #if item[1] > 100:\n",
    "    #    print(item[0],'\\t\\t',item[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "#for site in sorted_problem_sources:\n",
    "    #sorted_problem_sources[site[0]] = site[1]/count\n",
    "#    print(site[0],site[1])\n",
    "        \n",
    "#for site in sorted_problem_destinations:\n",
    "#    print(site[0],site[1])\n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## failure rates for the site pairs\n",
    "ratios = {}\n",
    "source_ratios = []\n",
    "destination_ratios = []\n",
    "\n",
    "\n",
    "## Number of failed transfers needed\n",
    "threshold = 100\n",
    "ratio_threshold = 0.3\n",
    "\n",
    "for item in sorted_endpointPairs:\n",
    "    if item[0] in pairs and item[1] >= threshold and item[1]/(pairs[item[0]]+item[1]) >= ratio_threshold:\n",
    "        #print(item[0],item[1]/pairs[item[0]])\n",
    "        ratios[item[0]] = item[1]/(pairs[item[0]]+item[1])\n",
    "        \n",
    "sorted_ratios = sorted(ratios.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print('Failure Rates for Site Pairs:\\n')\n",
    "for item in sorted_ratios:\n",
    "    print(item[0],'\\t\\t',item[1])\n",
    "print('\\n')\n",
    "    \n",
    "for item in sorted_problem_sources:\n",
    "    if item[0] in sources and item[1] >= threshold:\n",
    "        source_ratios.append([item[0],item[1]/(sources[item[0]]+item[1])])\n",
    "\n",
    "sorted_problem_source_ratios = sorted(source_ratios,key=operator.itemgetter(1))\n",
    "\n",
    "print('Source Failure Rates:\\n')\n",
    "for item in sorted_problem_source_ratios:\n",
    "    print(item[0],'\\t\\t',item[1])\n",
    "print('\\n')           \n",
    "    \n",
    "for item in sorted_problem_destinations:\n",
    "    if item[0] in destinations and item[1] >= threshold:\n",
    "        destination_ratios.append([item[0],item[1]/(destinations[item[0]]+item[1])])\n",
    "\n",
    "sorted_problem_destination_ratios = sorted(destination_ratios,key=operator.itemgetter(1))\n",
    "        \n",
    "print('Destination Failure Rates:\\n')\n",
    "for item in sorted_problem_destination_ratios:\n",
    "    print(item[0],'\\t\\t',item[1])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2d histogram. duration vs file size for failures\n",
    "#list of values: \n",
    "#size_failures\n",
    "#duration_failures\n",
    "\n",
    "#x, y, bins=10, range=None, normed=False, weights=None, cmin=None, cmax=None, hold=None, data=None, **kwargs)\n",
    "\n",
    "plt.hist2d(size_failures, duration_failures,bins = 25 , range=[[0, max(size_failures)],[0, max(duration_failures)]], cmin=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('size')\n",
    "plt.ylabel('duration')\n",
    "plt.title('duration vs file size for failures')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2d histogram. duration vs file size for transfers\n",
    "#duration_all\n",
    "#size_all\n",
    "\n",
    "plt.hist2d(size_all, duration_all,bins = 25 , range=[[0, max(size_all)],[0, max(duration_all)]], cmin=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('size')\n",
    "plt.ylabel('duration')\n",
    "plt.title('duration vs file size for transfers')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##heatmap of failure rates\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "rows = []\n",
    "columns = []\n",
    "\n",
    "##used for debugging (matrices are hard)\n",
    "test_pairs = {}\n",
    "test_pairs[('a','b')] = 1\n",
    "test_pairs[('b','c')] = 2\n",
    "test_pairs[('a','c')] = 1\n",
    "test_pairs[('c','d')] = 3\n",
    "sorted_pairs = sorted(test_pairs.items(), key=operator.itemgetter(1))\n",
    "\n",
    "for pairs in sorted_ratios:\n",
    "    if not pairs[0][0] in columns:\n",
    "        columns.append(pairs[0][0])\n",
    "    if not pairs[0][1] in rows:\n",
    "        rows.append(pairs[0][1])\n",
    "\n",
    "\n",
    "values = np.zeros((len(rows),len(columns)))\n",
    "\n",
    "for pairs in sorted_ratios:\n",
    "    row_index = rows.index(pairs[0][1])\n",
    "    column_index = columns.index(pairs[0][0])\n",
    "    value = pairs[1]\n",
    "    values.itemset((row_index,column_index),value) \n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolor(values, cmap='Reds', edgecolor='black')\n",
    "fig.colorbar(im)\n",
    "\n",
    "ax.xaxis.set(ticks=np.arange(0.5, len(columns)), ticklabels=columns)\n",
    "ax.yaxis.set(ticks=np.arange(0.5, len(rows)), ticklabels=rows)\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.xlabel('Sending Site')\n",
    "plt.ylabel('Receiving Site')\n",
    "plt.title('Failure Rates')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##kept for posterity\n",
    "#p = pd.DataFrame(values, rows, columns, dtype=None, copy=False)\n",
    "\n",
    "#todo: compare top and bottom failure rate pairs for noticeable differences. file size etc?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reasons for errors\n",
    "\n",
    "#sort dict of error reasons\n",
    "sorted_reasons = sorted(reasons.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print(len(sorted_reasons))\n",
    "for i in sorted_reasons:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(failure_types)\n",
    "print(all_types)\n",
    "for key in failure_types:\n",
    "    print(key,' failure rate: ',failure_types[key]/(all_types[key]+failure_types[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file formats\n",
    "\n",
    "sorted_types = sorted(data_types.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print(len(sorted_types))\n",
    "for i in sorted_types:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
