{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sched\n",
    "from datetime import datetime, tzinfo\n",
    "from elasticsearch import Elasticsearch\n",
    "#matplotlib.rc('font', **{'size': 15})\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "import json\n",
    "import redis\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rs = redis.Redis('aianalytics13.cern.ch')\n",
    "#tmp = rs.get('metrics#UNI-DORTMUND:BNL-ATLAS')\n",
    "#out = json.loads(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2918\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "url = 'http://dashb-atlas-ddm.cern.ch/dashboard/request.py/matrix.json?activity=Data+Brokering&activity=Data+Consolidation&activity=Data+Export+Test&activity=Debug&activity=Deletion&activity=Express&activity=Functional+Test&activity=Group+Subscriptions&activity=Production&activity=Production+Input&activity=Production+Output&activity=Recovery&activity=Staging&activity=T0+Export&activity=T0+Tape&activity=User+Subscriptions&activity=default&activity=on&activity=on&activity=rucio-integration&activity=test&activity=test%3AT0_T1+export&activity=test%3AT1_T2+export&activity=testactivity10&activity=testactivity20&activity=testactivity70&activity_default_exclude=Upload%2FDownload+%28Job%29&activity_default_exclude=Upload%2FDownload+%28User%29&activity_default_exclude=Analysis+Download&activity_default_exclude=Analysis+Upload&activity_default_exclude=Production+Download&activity_default_exclude=Production+Upload&activity_default_exclude=CLI+Download&activity_default_exclude=CLI+Upload&src_grouping=site&src_grouping=token&dst_grouping=site&dst_grouping=token&interval=60'\n",
    "res = requests.get(url).json()['transfers']['rows']\n",
    "print (len(res))\n",
    "#print(res)\n",
    "\n",
    "\n",
    "#Mario: The ddm2 dashboard is filled with exactly these values, which come from a fancy pipeline out of DDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local time Chicago:  2017-05-04 04:42:32\n"
     ]
    }
   ],
   "source": [
    "#define scheduler\n",
    "s = sched.scheduler(time.time, time.sleep)\n",
    "\n",
    "#define variables\n",
    "interval = 6000*10 #update every [interval] seconds\n",
    "\n",
    "verbose = True\n",
    "\n",
    "pattern = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "\n",
    "\n",
    "#debug\n",
    "if verbose: print('Local time Chicago: ',datetime.now().strftime(pattern))\n",
    "\n",
    "#number of successful transfers in interval\n",
    "transfers = 0\n",
    "#number of failed transfers\n",
    "failures = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_last():\n",
    "    print()\n",
    "    #reset counters\n",
    "    transfers = 0\n",
    "    failures = 0\n",
    "    now_datetime = datetime.now().strftime(pattern)\n",
    "    if verbose: print('The clock is: ',now_datetime)\n",
    "    if verbose: print('Epoch time: ',int(time.time())*1000)\n",
    "\n",
    "    now = int(time.time()) * 1000\n",
    "    \n",
    "    past = now - interval*1000 #convert interval to epoch milliseconds\n",
    "   \n",
    "    \n",
    "    \n",
    "    query_done = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "        \"must\": [\n",
    "        {\n",
    "            \"range\": {\n",
    "            \"@timestamp\": {\n",
    "                \"gte\": past,\n",
    "                \"lte\": now,\n",
    "                \"format\": \"epoch_millis\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "            {\"term\": {\"event_type\": \"transfer-done\"}},\n",
    "            {\"term\": {\"payload.scope\": \"data16_13TeV\"}}\n",
    "        ]\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    query_failed = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "        \"must\": [\n",
    "        {\n",
    "            \"range\": {\n",
    "            \"@timestamp\": {\n",
    "                \"gte\": past,\n",
    "                \"lte\": now,\n",
    "                \"format\": \"epoch_millis\"\n",
    "                 }\n",
    "            }\n",
    "        },\n",
    "            {\"term\": {\"event_type\": \"transfer-failed\"}},\n",
    "            {\"term\": {\"payload.scope\": \"data16_13TeV\"}}\n",
    "        ]\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    # make scrolls\n",
    "    es = Elasticsearch(['atlas-kibana.mwt2.org:9200'],timeout=60) #chicago elasticsearch\n",
    "    #es = Elasticsearch(['es-atlas.cern.ch:9203'],timeout=60) #cern elasticsearch\n",
    "    \n",
    "    my_index = \"atlas_rucio-events-2017*\"\n",
    "    scroll_done = scan(es, query=query_done, index=my_index, scroll='5m', timeout=\"5m\", size=100)\n",
    "    scroll_errors = scan(es, query=query_failed, index=my_index, scroll='5m', timeout=\"5m\", size=100)\n",
    "    \n",
    "    print(scroll_done)\n",
    "    \n",
    "    for entry in scroll_done:\n",
    "        transfers += 1\n",
    "    \n",
    "    for entry in scroll_errors:\n",
    "        failures += 1\n",
    "        \n",
    "        \n",
    "    if verbose: print('Number of successful transfers last ',interval,' seconds: ',transfers)\n",
    "    if verbose: print('Number of failed transfers last ',interval,' seconds: ',failures)\n",
    "\n",
    "    #schedule rerun of job in [interval] seconds\n",
    "    print()\n",
    "    s.enter(interval, 1, process_last, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The clock is:  2017-05-04 04:42:32\n",
      "Epoch time:  1493890952000\n",
      "<generator object scan at 0x7f2f0c4ebc50>\n",
      "Number of successful transfers last  60000  seconds:  0\n",
      "Number of failed transfers last  60000  seconds:  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_last()\n",
    "#s.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
