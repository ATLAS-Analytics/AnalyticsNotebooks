{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to aggregate Job data percentiles and dump into python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "import argparse\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch import VERSION\n",
    "from collections import OrderedDict\n",
    "from sys import stdout\n",
    "\n",
    "\n",
    "def scan_es():\n",
    "  prim_field = 'processingtype'\n",
    "  time_fields = [\n",
    "    ('timeStageIn', None, \"_exists_:timeStageIn AND NOT timeStageIn:<0\"),\n",
    "    ('timeStageOut', None, \"_exists_:timeStageOut AND NOT timeStageOut:<0\"),\n",
    "    ('timeSetup', None, \"_exists_:timeSetup AND NOT timeSetup:<0\"),\n",
    "    ('timeExe', None, \"_exists_:timeExe AND NOT timeExe:<0\"),\n",
    "    ('timeGetJob', None, \"_exists_:timeGetJob AND NOT timeGetJob:<0\"),\n",
    "    ('wall_time', None, \"_exists_:wall_time AND NOT wall_time:<0\"),\n",
    "    ('walltime_times_cores', \"(doc['actualcorecount'].value) ? doc['wall_time'].value*doc['actualcorecount'].value : doc['wall_time'].value\", \"_exists_:wall_time AND NOT wall_time:<0\"),\n",
    "    ('dbTime', None, \"_exists_:dbTime AND NOT dbTime:<0\"),\n",
    "    ('cputimeperevent', \"(doc['nevents'].value) ? doc['cpuconsumptiontime'].value/doc['nevents'].value : 0\", \n",
    "     \"_exists_:cpuconsumptiontime\"),\n",
    "      \n",
    "  ]\n",
    "  mem_fields = [\n",
    "    ('max_pss_per_core', \"(doc['actualcorecount'].value) ? doc['maxpss'].value*1024/doc['actualcorecount'].value : doc['maxpss'].value*1024\", \"_exists_:maxpss AND NOT maxpss:<0\"),\n",
    "    # ('avgswap', \"doc['avgswap'].value*1024\", \"_exists_:avgswap AND NOT avgswap:<0\"),\n",
    "  ]\n",
    "  eff_fields = [\n",
    "    ('cpu_eff', None, \"_exists_:cpu_eff AND NOT cpu_eff:<0\"),\n",
    "    ('cpu_eff_per_core', \"(doc['actualcorecount'].value) ? doc['cpu_eff'].value/doc['actualcorecount'].value : doc['cpu_eff'].value\", \"_exists_:cpu_eff AND NOT cpu_eff:<0\"),\n",
    "    ('cpueff_per_core_over_timeExe', \"(doc['timeExe'].value && doc['actualcorecount'].value) ? doc['cpuconsumptiontime'].value/doc['actualcorecount'].value/doc['timeExe'].value:0\", \n",
    "     \"_exists_:timeExe AND NOT timeExe:<0\"),      \n",
    "#    ('cpueff_per_core_over_timeExe', None, \"_exists_:cpueff_per_core_over_timeExe AND NOT cpueff_per_core_over_timeExe:<0\"),\n",
    "#    ('cpueff_per_core_over_timeExe'\n",
    "  ]\n",
    "  data_fields = [\n",
    "    ('dbData', None, \"_exists_:dbData AND NOT dbData:<0\"),\n",
    "    ('inputfilebytes', None, \"_exists_:inputfilebytes AND NOT inputfilebytes:<0\"),\n",
    "    ('outputfilebytes', None, \"_exists_:outputfilebytes AND NOT outputfilebytes:<0\"),\n",
    "    ('IObytesReadRate', None, \"_exists_:IObytesReadRate AND NOT IObytesReadRate:<0\"),\n",
    "    ('IObytesWriteRate', None, \"_exists_:IObytesWriteRate AND NOT IObytesWriteRate:<0\"),\n",
    "    ('IO_Intensity',\"((doc['inputfilebytes'].value && doc['corecount'].value && doc['timeExe'].value ) ? (doc['inputfilebytes'].value+doc['outputfilebytes'].value)/((doc['timeStageIn'].value+doc['timeStageOut'].value+doc['timeExe'].value)*doc['corecount'].value) :   0) \",\n",
    "     \"_exists_:inputfilebytes AND NOT inputfilebytes:<0\"),\n",
    "\n",
    "  ]\n",
    "  sec_fields = time_fields + mem_fields + eff_fields + data_fields\n",
    "  indices = 'jobs_archive_*'\n",
    "\n",
    "  dates = [\n",
    "#GD    ('2017',)+get_date_range_year('2017'),\n",
    "    ('2017-11_2018-01',)+get_date_range_month('2017-11')[0:1]+get_date_range_month('2018-01')[1:2],\n",
    "    ('2017-11',)+get_date_range_month('2017-11'),\n",
    "    ('2017-12',)+get_date_range_month('2017-12'),\n",
    "    ('2018-01',)+get_date_range_month('2018-01'),\n",
    "  ]\n",
    "#GD  for date in ['2017-{:02.0f}'.format(l) for l in range(1, 13)]:\n",
    "#GD    dates.append((date,)+get_date_range_month(date))\n",
    "\n",
    "  es = Elasticsearch([{'host': 'atlas-kibana.mwt2.org', 'port': 9200, 'timeout': 300}])\n",
    "\n",
    "  out_dict = {}\n",
    "\n",
    "  for date_name, start_date, end_date in dates:\n",
    "    stdout.write('\\rRunning ES query for {}'.format(date_name))\n",
    "    stdout.flush()\n",
    "    out_dict[date_name] = {}\n",
    "    for field, script, filter_string in sec_fields:\n",
    "      query = get_query(prim_field, field, script, filter_string, start_time = get_utc_timestamp(start_date), end_time = get_utc_timestamp(end_date))\n",
    "      res = es.search(index = indices, body = query, size = 1)\n",
    "      out_dict[date_name][field] = res\n",
    "    with open('es_scan.json', 'w') as out_file:\n",
    "      json.dump(out_dict, out_file)\n",
    "\n",
    "  stdout.write('\\n')\n",
    "\n",
    "def get_query(primary_field, field, script, filter_string, start_time, end_time, percents = [50, 75, 95, 99]):\n",
    "  query = {\n",
    "    \"query\": {\n",
    "      \"bool\": {\n",
    "        \"must\": [\n",
    "          {\n",
    "            \"match_phrase\": {\n",
    "              \"prodsourcelabel\": {\n",
    "                \"query\": \"managed\"\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"match_phrase\": {\n",
    "              \"jobstatus\": {\n",
    "                \"query\": \"finished\"\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"range\": {\n",
    "              \"modificationtime\": {\n",
    "                \"gte\": start_time,\n",
    "                \"lte\": end_time,\n",
    "                \"format\": \"epoch_second\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"must_not\": []\n",
    "      }\n",
    "    },\n",
    "    \"size\": 0,\n",
    "    \"_source\": {\n",
    "      \"excludes\": []\n",
    "    },\n",
    "    \"aggs\": {\n",
    "      primary_field: {\n",
    "        \"terms\": {\n",
    "          \"field\": primary_field,\n",
    "          \"size\": 20,\n",
    "          \"order\": {\n",
    "            \"_term\": \"desc\"\n",
    "          }\n",
    "        },\n",
    "        'aggs': {}\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  ## Add filters\n",
    "  if filter_string is not None:\n",
    "    query['query']['bool']['must'].append(get_filter_entry(filter_string))\n",
    "  else:\n",
    "    query['query']['bool']['must'].append({ \"match_all\": {} })\n",
    "  ## Add sub-aggregations\n",
    "  # for field, script in secondary_fields:\n",
    "  #   query['aggs'][primary_field]['aggs'].update(get_agg_entry(field = field, script = script, percents = percents))\n",
    "  ## Add sub-aggregation\n",
    "  query['aggs'][primary_field]['aggs'].update(get_agg_entry(field = field, script = script, percents = percents))\n",
    "\n",
    "  return query\n",
    "\n",
    "def get_filter_entry(filter_string):\n",
    "  return {\n",
    "    \"query_string\": {\n",
    "      \"query\": filter_string,\n",
    "      \"analyze_wildcard\": True,\n",
    "      \"lowercase_expanded_terms\": False\n",
    "    }\n",
    "  }\n",
    "\n",
    "def get_agg_entry(field = None, script = None, percents = [50, 75, 95, 99]):\n",
    "  agg_dict = {\n",
    "    field: {\n",
    "      \"percentiles\": {\n",
    "        \"percents\": percents,\n",
    "        \"keyed\": False\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if script is not None:\n",
    "    agg_dict[field][\"percentiles\"][\"script\"] = {\n",
    "      \"inline\": script,\n",
    "      \"lang\": \"expression\"\n",
    "    }\n",
    "  else:\n",
    "    agg_dict[field][\"percentiles\"]['field'] = field\n",
    "\n",
    "  return agg_dict\n",
    "\n",
    "def get_utc_timestamp(date_string, format_string = '%Y-%m-%dT%H:%M:%S'):\n",
    "  ## Make datetime object from time_string\n",
    "  date = datetime.datetime.strptime(date_string, format_string)\n",
    "  time_tuple = date.timetuple()\n",
    "  ## Get unix time (this assumes that time_tuple was created with a UTC time)\n",
    "  time_unix = calendar.timegm(time_tuple)\n",
    "  \n",
    "  return time_unix\n",
    "\n",
    "def get_duration_string(time_seconds):\n",
    "  minutes, seconds = divmod(time_seconds, 60)\n",
    "  hours, minutes = divmod(minutes, 60)\n",
    "  \n",
    "  return '{:.0f}:{:02.0f}:{:02.0f}'.format(hours, minutes, seconds)\n",
    "\n",
    "def get_date_range_year(date_name):\n",
    "  year_start_string = '{}-01-01T00:00:00'.format(date_name)\n",
    "  year_end_string = '{}-12-31T23:59:59'.format(date_name)\n",
    "  return year_start_string, year_end_string\n",
    "\n",
    "def get_date_range_month(date_name, date_format = '%Y-%m-%dT%H:%M:%S'):\n",
    "  start_string = '{}-01T00:00:00'.format(date_name)\n",
    "  start_year, start_month = [int(l) for l in date_name.split('-', 1)]\n",
    "  end_month = (start_month % 12) + 1\n",
    "  end_year = start_year+1 if (end_month == 1) else start_year\n",
    "  end_string = '{}-{:02.0f}-01T00:00:00'.format(end_year, end_month)\n",
    "  start_date = datetime.datetime.strptime(start_string, date_format)\n",
    "  ## Subtract one second\n",
    "  end_date = datetime.datetime.strptime(end_string, date_format) - datetime.timedelta(seconds = 1)\n",
    "  end_string = datetime.datetime.strftime(end_date, date_format)\n",
    "  return start_string, end_string\n",
    "\n",
    "def get_date_range(in_string, date_format):\n",
    "  date = datetime.datetime.strptime(in_string, date_format)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ES query for 2018-01_2018-01\n"
     ]
    }
   ],
   "source": [
    "scan_es()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tuple('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=(1,2)\n",
    "a[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
