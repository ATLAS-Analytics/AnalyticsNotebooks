{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "import datetime\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from time import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define query for the tasks\n",
    "taskQuery = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\"term\":{\"transhome\": \"AthDerivation-21.2.23.0\"}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute the queries\n",
    "es = Elasticsearch(['atlas-kibana-dev.mwt2.org'],timeout=120)\n",
    "\n",
    "# Tasks\n",
    "taskIndex = \"tasks*\"\n",
    "tasks = scan(es, query=taskQuery, index=taskIndex, scroll='5m', timeout=\"5m\", size=1000)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the trains and the quantities for collection\n",
    "quantities = [\n",
    "    'cpuconsumptiontime',\n",
    "    'nevents',\n",
    "    'starttime',\n",
    "    'endtime',\n",
    "    'timeExe',\n",
    "    'timeSetup',\n",
    "    'timeGetJob',\n",
    "    'timeStageIn',\n",
    "    'timeStageOut',\n",
    "    'actualcorecount',\n",
    "    'wall_time',\n",
    "    'inputfilebytes',\n",
    "    'outputfilebytes',\n",
    "    'Max_PSS_per_core'\n",
    "]\n",
    "\n",
    "# Data structure as follows:\n",
    "# List of tuples, one per train, with following contents:\n",
    "# ( [List of carriages in the train],[list of jobs for that train],{map of quantity name->list of values for the jobs})\n",
    "trainsAndIDs = [\n",
    "    (['DAOD_HIGG2D5','DAOD_TCAL1','DAOD_JETM12'],[],{}), \n",
    "    (['DAOD_EGAM4','DAOD_STDM5','DAOD_EXOT12','DAOD_EXOT10'],[],{}), \n",
    "    (['DAOD_FTAG3'],[],{}), \n",
    "    (['DAOD_EGAM3','DAOD_HIGG1D2','DAOD_EXOT0','DAOD_EXOT17','DAOD_HIGG4D4'],[],{}), \n",
    "    (['DAOD_JETM7','DAOD_EXOT6','DAOD_SUSY9','DAOD_EGAM2','DAOD_SUSY12','DAOD_SUSY2'],[],{}), \n",
    "    (['DAOD_EXOT9','DAOD_EXOT15','DAOD_JETM3','DAOD_HIGG4D1','DAOD_TOPQ2','DAOD_MUON2','DAOD_SUSY16'],[],{}), \n",
    "    (['DAOD_EXOT7','DAOD_SUSY11','DAOD_SUSY9','DAOD_STDM3','DAOD_TOPQ5','DAOD_EXOT19'],[],{}), \n",
    "    (['DAOD_HIGG1D1','DAOD_EGAM9','DAOD_BPHY5','DAOD_BPHY4','DAOD_JETM4','DAOD_SUSY18'],[],{}), \n",
    "    (['DAOD_EGAM7','DAOD_HIGG6D1','DAOD_STDM2','DAOD_SUSY3','DAOD_EGAM1'],[],{}), \n",
    "    (['DAOD_EXOT22','DAOD_SUSY1','DAOD_JETM1','DAOD_EXOT3','DAOD_SUSY4','DAOD_BPHY1'],[],{}), \n",
    "    (['DAOD_JETM11','DAOD_HIGG8D1','DAOD_EXOT5','DAOD_SUSY5'],[],{}), \n",
    "    (['DAOD_SUSY10','DAOD_STDM7','DAOD_JETM6','DAOD_JETM9','DAOD_TAUP1','DAOD_SUSY7','DAOD_HIGG4D5'],[],{}), \n",
    "    (['DAOD_EXOT4','DAOD_HIGG4D6','DAOD_SUSY8','DAOD_HIGG4D2','DAOD_MUON1'],[],{}), \n",
    "    (['DAOD_EXOT8','DAOD_HIGG4D3','DAOD_EXOT2'],[],{}), \n",
    "    (['DAOD_HIGG2D1','DAOD_SUSY6','DAOD_EXOT13'],[],{}), \n",
    "    (['DAOD_FTAG1','DAOD_FTAG2','DAOD_FTAG4'],[],{}), \n",
    "    (['DAOD_TOPQ1','DAOD_EGAM5','DAOD_STDM4'],[],{}), \n",
    "    (['DAOD_HIGG6D2','DAOD_EXOT21','DAOD_MUON0','DAOD_TAUP3','DAOD_TOPQ4'],[],{})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching tasks =  34\n",
      "Time to extract information =  0.15155932889319956\n"
     ]
    }
   ],
   "source": [
    "# Get the IDs matching each train\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "matchingTasks = 0\n",
    "for res in tasks:\n",
    "    if 'output_formats' in res['_source'].keys():\n",
    "        for item in trainsAndIDs:\n",
    "            train = item[0]\n",
    "            if set(res['_source']['output_formats']) == set(train):\n",
    "                item[1].append(res['_id'])\n",
    "                matchingTasks += 1\n",
    "print(\"Total matching tasks = \",matchingTasks)\n",
    "print(\"Time to extract information = \",timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up query for the jobs relevant to the trains\n",
    "start_time = timeit.default_timer()\n",
    "taskCounter = 0\n",
    "jobIndex = \"jobs_archive_2018*,jobs_archive_2017*\"\n",
    "\n",
    "to_read = quantities\n",
    "to_read.append('jeditaskid')\n",
    "to_read.append('transformation')\n",
    "\n",
    "for item in trainsAndIDs: # Loop over trains\n",
    "    #taskCounterPerTrain = 0\n",
    "    for theId in item[1]: # Loop over tasks for that train\n",
    "        taskCounter += 1\n",
    "        #taskCounterPerTrain += 1\n",
    "        #if taskCounterPerTrain > 10: break\n",
    "        if taskCounter % 100 == 0: \n",
    "            et = timeit.default_timer() - start_time\n",
    "            print(\"Processing task \",taskCounter,\" Elapsed Time:\", et, '\\t projected finish in:',et/taskCounter*matchingTasks - et )\n",
    "        jobQuery = {\n",
    "            \"_source\": to_read,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"term\":{\"jeditaskid\": theId}},\n",
    "                        {\"term\":{\"jobstatus\": \"finished\"}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # query the jobs\n",
    "        jobs = scan(es, query=jobQuery, index=jobIndex, scroll='5m', timeout=\"5m\", size=1000)\n",
    "        for res in jobs: # Loop over jobs from that task\n",
    "            for quantity in quantities: # get the relevant quantities\n",
    "                if quantity in res['_source'].keys(): \n",
    "                    if quantity not in item[2].keys(): # store the quantities in a long list\n",
    "                        item[2][quantity] = [res['_source'][quantity]]\n",
    "                    else:\n",
    "                        item[2][quantity].append(res['_source'][quantity])\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the per-train information\n",
    "pickle.dump(trainsAndIDs,open(\"data_21_2_23_0.p\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
