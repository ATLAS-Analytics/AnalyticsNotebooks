{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import model_from_json\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=110, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/lib/python3.4/site-packages/ipykernel/__main__.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, kernel_initializer=\"uniform\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 690 samples, validate on 18108 samples\n",
      "Epoch 1/150\n",
      "690/690 [==============================] - 1s - loss: 7.1796 - acc: 0.5072 - val_loss: 6.5599 - val_acc: 0.6280\n",
      "Epoch 2/150\n",
      "690/690 [==============================] - 0s - loss: 6.2331 - acc: 0.5029 - val_loss: 5.6325 - val_acc: 0.5000\n",
      "Epoch 3/150\n",
      "690/690 [==============================] - 0s - loss: 5.3083 - acc: 0.5275 - val_loss: 4.7743 - val_acc: 0.5000\n",
      "Epoch 4/150\n",
      "690/690 [==============================] - 0s - loss: 4.4760 - acc: 0.5333 - val_loss: 4.0091 - val_acc: 0.5000\n",
      "Epoch 5/150\n",
      "690/690 [==============================] - 0s - loss: 3.7489 - acc: 0.5275 - val_loss: 3.3501 - val_acc: 0.5097\n",
      "Epoch 6/150\n",
      "690/690 [==============================] - 0s - loss: 3.1323 - acc: 0.5362 - val_loss: 2.8003 - val_acc: 0.5172\n",
      "Epoch 7/150\n",
      "690/690 [==============================] - 0s - loss: 2.6197 - acc: 0.5435 - val_loss: 2.3520 - val_acc: 0.5576\n",
      "Epoch 8/150\n",
      "690/690 [==============================] - 0s - loss: 2.2090 - acc: 0.5638 - val_loss: 1.9930 - val_acc: 0.5949\n",
      "Epoch 9/150\n",
      "690/690 [==============================] - 0s - loss: 1.8751 - acc: 0.6087 - val_loss: 1.7081 - val_acc: 0.6830\n",
      "Epoch 10/150\n",
      "690/690 [==============================] - 0s - loss: 1.6089 - acc: 0.5971 - val_loss: 1.4800 - val_acc: 0.6012\n",
      "Epoch 11/150\n",
      "690/690 [==============================] - 0s - loss: 1.3965 - acc: 0.6058 - val_loss: 1.2982 - val_acc: 0.5541\n",
      "Epoch 12/150\n",
      "690/690 [==============================] - 0s - loss: 1.2132 - acc: 0.6609 - val_loss: 1.1622 - val_acc: 0.5222\n",
      "Epoch 13/150\n",
      "690/690 [==============================] - 0s - loss: 1.0379 - acc: 0.7783 - val_loss: 1.2771 - val_acc: 0.5084\n",
      "Epoch 14/150\n",
      "690/690 [==============================] - 0s - loss: 0.9263 - acc: 0.8420 - val_loss: 1.3804 - val_acc: 0.5180\n",
      "Epoch 15/150\n",
      "690/690 [==============================] - 0s - loss: 0.8420 - acc: 0.8609 - val_loss: 1.4078 - val_acc: 0.5363\n",
      "Epoch 16/150\n",
      "690/690 [==============================] - 0s - loss: 0.7381 - acc: 0.8739 - val_loss: 1.4060 - val_acc: 0.5643\n",
      "Epoch 17/150\n",
      "690/690 [==============================] - 0s - loss: 0.6921 - acc: 0.9072 - val_loss: 1.1353 - val_acc: 0.6397\n",
      "Epoch 18/150\n",
      "690/690 [==============================] - 0s - loss: 0.6238 - acc: 0.9290 - val_loss: 1.0976 - val_acc: 0.6877\n",
      "Epoch 19/150\n",
      "690/690 [==============================] - 0s - loss: 0.5702 - acc: 0.9130 - val_loss: 1.0722 - val_acc: 0.7101\n",
      "Epoch 20/150\n",
      "690/690 [==============================] - 0s - loss: 0.5465 - acc: 0.9232 - val_loss: 0.9044 - val_acc: 0.7607\n",
      "Epoch 21/150\n",
      "690/690 [==============================] - 0s - loss: 0.4931 - acc: 0.9116 - val_loss: 0.8609 - val_acc: 0.7760\n",
      "Epoch 22/150\n",
      "690/690 [==============================] - 0s - loss: 0.4884 - acc: 0.9029 - val_loss: 0.7148 - val_acc: 0.8217\n",
      "Epoch 23/150\n",
      "690/690 [==============================] - 0s - loss: 0.4493 - acc: 0.9188 - val_loss: 0.7097 - val_acc: 0.8230\n",
      "Epoch 24/150\n",
      "690/690 [==============================] - 0s - loss: 0.4275 - acc: 0.9101 - val_loss: 0.7677 - val_acc: 0.7903\n",
      "Epoch 25/150\n",
      "690/690 [==============================] - 0s - loss: 0.4230 - acc: 0.9290 - val_loss: 0.7264 - val_acc: 0.7966\n",
      "Epoch 26/150\n",
      "690/690 [==============================] - 0s - loss: 0.3921 - acc: 0.9377 - val_loss: 0.6965 - val_acc: 0.8151\n",
      "Epoch 27/150\n",
      "690/690 [==============================] - 0s - loss: 0.3949 - acc: 0.9174 - val_loss: 0.5699 - val_acc: 0.8619\n",
      "Epoch 28/150\n",
      "690/690 [==============================] - 0s - loss: 0.3912 - acc: 0.9319 - val_loss: 0.6365 - val_acc: 0.8539\n",
      "Epoch 29/150\n",
      "690/690 [==============================] - 0s - loss: 0.4227 - acc: 0.9217 - val_loss: 0.5925 - val_acc: 0.8497\n",
      "Epoch 30/150\n",
      "690/690 [==============================] - 0s - loss: 0.4412 - acc: 0.9000 - val_loss: 0.8062 - val_acc: 0.7486\n",
      "Epoch 31/150\n",
      "690/690 [==============================] - 0s - loss: 0.4809 - acc: 0.8812 - val_loss: 0.4222 - val_acc: 0.8914\n",
      "Epoch 32/150\n",
      "690/690 [==============================] - 0s - loss: 0.4131 - acc: 0.9203 - val_loss: 0.6147 - val_acc: 0.8308\n",
      "Epoch 33/150\n",
      "690/690 [==============================] - 0s - loss: 0.3780 - acc: 0.9449 - val_loss: 0.5914 - val_acc: 0.8589\n",
      "Epoch 34/150\n",
      "690/690 [==============================] - 0s - loss: 0.3817 - acc: 0.9333 - val_loss: 0.5415 - val_acc: 0.8697\n",
      "Epoch 35/150\n",
      "690/690 [==============================] - 0s - loss: 0.4404 - acc: 0.9014 - val_loss: 0.5677 - val_acc: 0.8380\n",
      "Epoch 36/150\n",
      "690/690 [==============================] - 0s - loss: 0.3874 - acc: 0.9290 - val_loss: 0.4164 - val_acc: 0.8941\n",
      "Epoch 37/150\n",
      "690/690 [==============================] - 0s - loss: 0.3738 - acc: 0.9188 - val_loss: 0.4658 - val_acc: 0.8907\n",
      "Epoch 38/150\n",
      "690/690 [==============================] - 0s - loss: 0.3749 - acc: 0.9304 - val_loss: 0.6997 - val_acc: 0.8422\n",
      "Epoch 39/150\n",
      "690/690 [==============================] - 0s - loss: 0.3483 - acc: 0.9449 - val_loss: 0.5176 - val_acc: 0.8675\n",
      "Epoch 40/150\n",
      "690/690 [==============================] - 0s - loss: 0.3626 - acc: 0.9319 - val_loss: 0.5232 - val_acc: 0.8664\n",
      "Epoch 41/150\n",
      "690/690 [==============================] - 0s - loss: 0.3442 - acc: 0.9493 - val_loss: 0.4565 - val_acc: 0.8893\n",
      "Epoch 42/150\n",
      "690/690 [==============================] - 0s - loss: 0.3466 - acc: 0.9246 - val_loss: 0.4698 - val_acc: 0.8907\n",
      "Epoch 43/150\n",
      "690/690 [==============================] - 0s - loss: 0.3380 - acc: 0.9464 - val_loss: 0.5955 - val_acc: 0.8628\n",
      "Epoch 44/150\n",
      "690/690 [==============================] - 0s - loss: 0.3685 - acc: 0.9290 - val_loss: 0.4602 - val_acc: 0.8907\n",
      "Epoch 45/150\n",
      "690/690 [==============================] - 0s - loss: 0.3395 - acc: 0.9449 - val_loss: 0.4936 - val_acc: 0.8782\n",
      "Epoch 46/150\n",
      "690/690 [==============================] - 0s - loss: 0.3760 - acc: 0.9362 - val_loss: 0.5588 - val_acc: 0.8541\n",
      "Epoch 47/150\n",
      "690/690 [==============================] - 0s - loss: 0.3417 - acc: 0.9449 - val_loss: 0.4664 - val_acc: 0.8767\n",
      "Epoch 48/150\n",
      "690/690 [==============================] - 0s - loss: 0.3114 - acc: 0.9551 - val_loss: 0.5739 - val_acc: 0.8630\n",
      "Epoch 49/150\n",
      "690/690 [==============================] - 0s - loss: 0.3383 - acc: 0.9420 - val_loss: 0.6498 - val_acc: 0.8539\n",
      "Epoch 50/150\n",
      "690/690 [==============================] - 0s - loss: 0.3521 - acc: 0.9377 - val_loss: 0.4213 - val_acc: 0.9002\n",
      "Epoch 51/150\n",
      "690/690 [==============================] - 0s - loss: 0.3525 - acc: 0.9391 - val_loss: 0.3585 - val_acc: 0.9199\n",
      "Epoch 52/150\n",
      "690/690 [==============================] - 0s - loss: 0.3998 - acc: 0.9145 - val_loss: 0.5843 - val_acc: 0.8513\n",
      "Epoch 53/150\n",
      "690/690 [==============================] - 0s - loss: 0.3465 - acc: 0.9478 - val_loss: 0.4496 - val_acc: 0.8941\n",
      "Epoch 54/150\n",
      "690/690 [==============================] - 0s - loss: 0.3415 - acc: 0.9493 - val_loss: 0.5545 - val_acc: 0.8746\n",
      "Epoch 55/150\n",
      "690/690 [==============================] - 0s - loss: 0.3158 - acc: 0.9493 - val_loss: 0.5541 - val_acc: 0.8777\n",
      "Epoch 56/150\n",
      "690/690 [==============================] - 0s - loss: 0.3108 - acc: 0.9580 - val_loss: 0.4751 - val_acc: 0.8944\n",
      "Epoch 57/150\n",
      "690/690 [==============================] - 0s - loss: 0.3370 - acc: 0.9362 - val_loss: 0.5646 - val_acc: 0.8803\n",
      "Epoch 58/150\n",
      "690/690 [==============================] - 0s - loss: 0.3259 - acc: 0.9449 - val_loss: 0.4397 - val_acc: 0.9051\n",
      "Epoch 59/150\n",
      "690/690 [==============================] - 0s - loss: 0.3086 - acc: 0.9565 - val_loss: 0.5777 - val_acc: 0.8759\n",
      "Epoch 60/150\n",
      "690/690 [==============================] - 0s - loss: 0.3415 - acc: 0.9319 - val_loss: 0.4304 - val_acc: 0.9076\n",
      "Epoch 61/150\n",
      "690/690 [==============================] - 0s - loss: 0.3249 - acc: 0.9348 - val_loss: 0.4202 - val_acc: 0.9108\n",
      "Epoch 62/150\n",
      "690/690 [==============================] - 0s - loss: 0.3297 - acc: 0.9377 - val_loss: 0.3712 - val_acc: 0.9235\n",
      "Epoch 63/150\n",
      "690/690 [==============================] - 0s - loss: 0.3227 - acc: 0.9362 - val_loss: 0.5125 - val_acc: 0.8971\n",
      "Epoch 64/150\n",
      "690/690 [==============================] - 0s - loss: 0.3065 - acc: 0.9594 - val_loss: 0.4961 - val_acc: 0.8977\n",
      "Epoch 65/150\n",
      "690/690 [==============================] - 0s - loss: 0.2829 - acc: 0.9652 - val_loss: 0.4498 - val_acc: 0.9121\n",
      "Epoch 66/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/690 [==============================] - 0s - loss: 0.2777 - acc: 0.9638 - val_loss: 0.5608 - val_acc: 0.8923\n",
      "Epoch 67/150\n",
      "690/690 [==============================] - 0s - loss: 0.2949 - acc: 0.9623 - val_loss: 0.4666 - val_acc: 0.9143\n",
      "Epoch 68/150\n",
      "690/690 [==============================] - 0s - loss: 0.3060 - acc: 0.9522 - val_loss: 0.5286 - val_acc: 0.8977\n",
      "Epoch 69/150\n",
      "690/690 [==============================] - 0s - loss: 0.3308 - acc: 0.9406 - val_loss: 0.3900 - val_acc: 0.9211\n",
      "Epoch 70/150\n",
      "690/690 [==============================] - 0s - loss: 0.3136 - acc: 0.9478 - val_loss: 0.4136 - val_acc: 0.9180\n",
      "Epoch 71/150\n",
      "690/690 [==============================] - 0s - loss: 0.3242 - acc: 0.9464 - val_loss: 0.4953 - val_acc: 0.9000\n",
      "Epoch 72/150\n",
      "690/690 [==============================] - 0s - loss: 0.3182 - acc: 0.9478 - val_loss: 0.3820 - val_acc: 0.9213\n",
      "Epoch 73/150\n",
      "690/690 [==============================] - 0s - loss: 0.3150 - acc: 0.9478 - val_loss: 0.4147 - val_acc: 0.9146\n",
      "Epoch 74/150\n",
      "690/690 [==============================] - 0s - loss: 0.2976 - acc: 0.9493 - val_loss: 0.4567 - val_acc: 0.9071\n",
      "Epoch 75/150\n",
      "690/690 [==============================] - 0s - loss: 0.3224 - acc: 0.9478 - val_loss: 0.4658 - val_acc: 0.9109\n",
      "Epoch 76/150\n",
      "690/690 [==============================] - 0s - loss: 0.3082 - acc: 0.9536 - val_loss: 0.4144 - val_acc: 0.9190\n",
      "Epoch 77/150\n",
      "690/690 [==============================] - 0s - loss: 0.3219 - acc: 0.9420 - val_loss: 0.5017 - val_acc: 0.8968\n",
      "Epoch 78/150\n",
      "690/690 [==============================] - 0s - loss: 0.3385 - acc: 0.9290 - val_loss: 0.3456 - val_acc: 0.9326\n",
      "Epoch 79/150\n",
      "690/690 [==============================] - 0s - loss: 0.2843 - acc: 0.9652 - val_loss: 0.4285 - val_acc: 0.9179\n",
      "Epoch 80/150\n",
      "690/690 [==============================] - 0s - loss: 0.3008 - acc: 0.9580 - val_loss: 0.3953 - val_acc: 0.9232\n",
      "Epoch 81/150\n",
      "690/690 [==============================] - 0s - loss: 0.2875 - acc: 0.9623 - val_loss: 0.3832 - val_acc: 0.9279\n",
      "Epoch 82/150\n",
      "690/690 [==============================] - 0s - loss: 0.3089 - acc: 0.9594 - val_loss: 0.3904 - val_acc: 0.9283\n",
      "Epoch 83/150\n",
      "690/690 [==============================] - 0s - loss: 0.2964 - acc: 0.9551 - val_loss: 0.3993 - val_acc: 0.9241\n",
      "Epoch 84/150\n",
      "690/690 [==============================] - 0s - loss: 0.3015 - acc: 0.9464 - val_loss: 0.4028 - val_acc: 0.9224\n",
      "Epoch 85/150\n",
      "690/690 [==============================] - 0s - loss: 0.2929 - acc: 0.9522 - val_loss: 0.4777 - val_acc: 0.9058\n",
      "Epoch 86/150\n",
      "690/690 [==============================] - 0s - loss: 0.3077 - acc: 0.9435 - val_loss: 0.4661 - val_acc: 0.9011\n",
      "Epoch 87/150\n",
      "690/690 [==============================] - 0s - loss: 0.3374 - acc: 0.9377 - val_loss: 0.3412 - val_acc: 0.9297\n",
      "Epoch 88/150\n",
      "690/690 [==============================] - 0s - loss: 0.3176 - acc: 0.9449 - val_loss: 0.4721 - val_acc: 0.9028\n",
      "Epoch 89/150\n",
      "690/690 [==============================] - 0s - loss: 0.3038 - acc: 0.9565 - val_loss: 0.4386 - val_acc: 0.9139\n",
      "Epoch 90/150\n",
      "690/690 [==============================] - 0s - loss: 0.2958 - acc: 0.9507 - val_loss: 0.4535 - val_acc: 0.9113\n",
      "Epoch 91/150\n",
      "690/690 [==============================] - 0s - loss: 0.3025 - acc: 0.9551 - val_loss: 0.5217 - val_acc: 0.8990\n",
      "Epoch 92/150\n",
      "690/690 [==============================] - 0s - loss: 0.2990 - acc: 0.9464 - val_loss: 0.3974 - val_acc: 0.9211\n",
      "Epoch 93/150\n",
      "690/690 [==============================] - 0s - loss: 0.3013 - acc: 0.9594 - val_loss: 0.4678 - val_acc: 0.9120\n",
      "Epoch 94/150\n",
      "690/690 [==============================] - 0s - loss: 0.3107 - acc: 0.9435 - val_loss: 0.3887 - val_acc: 0.9259\n",
      "Epoch 95/150\n",
      "690/690 [==============================] - 0s - loss: 0.3151 - acc: 0.9565 - val_loss: 0.4630 - val_acc: 0.9118\n",
      "Epoch 96/150\n",
      "690/690 [==============================] - 0s - loss: 0.3079 - acc: 0.9493 - val_loss: 0.4379 - val_acc: 0.9161\n",
      "Epoch 97/150\n",
      "690/690 [==============================] - 0s - loss: 0.3080 - acc: 0.9406 - val_loss: 0.3893 - val_acc: 0.9242\n",
      "Epoch 98/150\n",
      "690/690 [==============================] - 0s - loss: 0.2855 - acc: 0.9507 - val_loss: 0.5162 - val_acc: 0.9008\n",
      "Epoch 99/150\n",
      "690/690 [==============================] - 0s - loss: 0.2776 - acc: 0.9594 - val_loss: 0.4414 - val_acc: 0.9166\n",
      "Epoch 100/150\n",
      "690/690 [==============================] - 0s - loss: 0.2799 - acc: 0.9580 - val_loss: 0.4689 - val_acc: 0.9109\n",
      "Epoch 101/150\n",
      "690/690 [==============================] - 0s - loss: 0.3012 - acc: 0.9565 - val_loss: 0.4300 - val_acc: 0.9093\n",
      "Epoch 102/150\n",
      "690/690 [==============================] - 0s - loss: 0.2787 - acc: 0.9609 - val_loss: 0.4192 - val_acc: 0.9121\n",
      "Epoch 103/150\n",
      "690/690 [==============================] - 0s - loss: 0.2843 - acc: 0.9594 - val_loss: 0.3641 - val_acc: 0.9322\n",
      "Epoch 104/150\n",
      "690/690 [==============================] - 0s - loss: 0.3215 - acc: 0.9493 - val_loss: 0.5207 - val_acc: 0.9053\n",
      "Epoch 105/150\n",
      "690/690 [==============================] - 0s - loss: 0.3516 - acc: 0.9290 - val_loss: 0.3308 - val_acc: 0.9386\n",
      "Epoch 106/150\n",
      "690/690 [==============================] - 0s - loss: 0.3169 - acc: 0.9551 - val_loss: 0.5106 - val_acc: 0.9032\n",
      "Epoch 107/150\n",
      "690/690 [==============================] - 0s - loss: 0.2960 - acc: 0.9507 - val_loss: 0.4172 - val_acc: 0.9167\n",
      "Epoch 108/150\n",
      "690/690 [==============================] - 0s - loss: 0.3470 - acc: 0.9348 - val_loss: 0.3624 - val_acc: 0.9241\n",
      "Epoch 109/150\n",
      "690/690 [==============================] - 0s - loss: 0.2925 - acc: 0.9551 - val_loss: 0.3474 - val_acc: 0.9296\n",
      "Epoch 110/150\n",
      "690/690 [==============================] - 0s - loss: 0.2930 - acc: 0.9536 - val_loss: 0.4214 - val_acc: 0.9219\n",
      "Epoch 111/150\n",
      "690/690 [==============================] - 0s - loss: 0.3163 - acc: 0.9377 - val_loss: 0.4567 - val_acc: 0.9134\n",
      "Epoch 112/150\n",
      "690/690 [==============================] - 0s - loss: 0.3537 - acc: 0.9261 - val_loss: 0.4521 - val_acc: 0.9035\n",
      "Epoch 113/150\n",
      "690/690 [==============================] - 0s - loss: 0.3109 - acc: 0.9435 - val_loss: 0.4556 - val_acc: 0.8984\n",
      "Epoch 114/150\n",
      "690/690 [==============================] - 0s - loss: 0.2815 - acc: 0.9594 - val_loss: 0.4285 - val_acc: 0.9078\n",
      "Epoch 115/150\n",
      "690/690 [==============================] - 0s - loss: 0.3068 - acc: 0.9580 - val_loss: 0.4712 - val_acc: 0.9000\n",
      "Epoch 116/150\n",
      "690/690 [==============================] - 0s - loss: 0.3223 - acc: 0.9420 - val_loss: 0.4412 - val_acc: 0.9053\n",
      "Epoch 117/150\n",
      "690/690 [==============================] - 0s - loss: 0.2940 - acc: 0.9580 - val_loss: 0.4939 - val_acc: 0.8942\n",
      "Epoch 118/150\n",
      "690/690 [==============================] - 0s - loss: 0.3002 - acc: 0.9435 - val_loss: 0.4840 - val_acc: 0.8937\n",
      "Epoch 119/150\n",
      "690/690 [==============================] - 0s - loss: 0.3129 - acc: 0.9435 - val_loss: 0.4873 - val_acc: 0.8927\n",
      "Epoch 120/150\n",
      "690/690 [==============================] - 0s - loss: 0.3359 - acc: 0.9377 - val_loss: 0.4222 - val_acc: 0.9075\n",
      "Epoch 121/150\n",
      "690/690 [==============================] - 0s - loss: 0.2818 - acc: 0.9667 - val_loss: 0.4631 - val_acc: 0.9023\n",
      "Epoch 122/150\n",
      "690/690 [==============================] - 0s - loss: 0.3017 - acc: 0.9565 - val_loss: 0.4489 - val_acc: 0.9084\n",
      "Epoch 123/150\n",
      "690/690 [==============================] - 0s - loss: 0.2836 - acc: 0.9609 - val_loss: 0.5088 - val_acc: 0.8988\n",
      "Epoch 124/150\n",
      "690/690 [==============================] - 0s - loss: 0.3053 - acc: 0.9551 - val_loss: 0.4503 - val_acc: 0.9028\n",
      "Epoch 125/150\n",
      "690/690 [==============================] - 0s - loss: 0.2896 - acc: 0.9609 - val_loss: 0.5594 - val_acc: 0.8823\n",
      "Epoch 126/150\n",
      "690/690 [==============================] - 0s - loss: 0.2936 - acc: 0.9638 - val_loss: 0.3969 - val_acc: 0.9104\n",
      "Epoch 127/150\n",
      "690/690 [==============================] - 0s - loss: 0.3084 - acc: 0.9464 - val_loss: 0.5357 - val_acc: 0.8863\n",
      "Epoch 128/150\n",
      "690/690 [==============================] - 0s - loss: 0.3183 - acc: 0.9507 - val_loss: 0.4422 - val_acc: 0.9060\n",
      "Epoch 129/150\n",
      "690/690 [==============================] - 0s - loss: 0.3107 - acc: 0.9449 - val_loss: 0.4710 - val_acc: 0.9002\n",
      "Epoch 130/150\n",
      "690/690 [==============================] - 0s - loss: 0.2695 - acc: 0.9638 - val_loss: 0.4134 - val_acc: 0.9128\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690/690 [==============================] - 0s - loss: 0.3042 - acc: 0.9536 - val_loss: 0.4430 - val_acc: 0.9139\n",
      "Epoch 132/150\n",
      "690/690 [==============================] - 0s - loss: 0.2934 - acc: 0.9565 - val_loss: 0.5966 - val_acc: 0.8910\n",
      "Epoch 133/150\n",
      "690/690 [==============================] - 0s - loss: 0.3250 - acc: 0.9478 - val_loss: 0.4970 - val_acc: 0.8982\n",
      "Epoch 134/150\n",
      "690/690 [==============================] - 0s - loss: 0.2984 - acc: 0.9464 - val_loss: 0.4080 - val_acc: 0.9113\n",
      "Epoch 135/150\n",
      "690/690 [==============================] - 0s - loss: 0.2866 - acc: 0.9536 - val_loss: 0.3800 - val_acc: 0.9195\n",
      "Epoch 136/150\n",
      "690/690 [==============================] - 0s - loss: 0.2945 - acc: 0.9536 - val_loss: 0.5559 - val_acc: 0.8903\n",
      "Epoch 137/150\n",
      "690/690 [==============================] - 0s - loss: 0.3843 - acc: 0.9232 - val_loss: 0.4523 - val_acc: 0.9025\n",
      "Epoch 138/150\n",
      "690/690 [==============================] - 0s - loss: 0.2775 - acc: 0.9652 - val_loss: 0.6138 - val_acc: 0.8677\n",
      "Epoch 139/150\n",
      "690/690 [==============================] - 0s - loss: 0.2612 - acc: 0.9638 - val_loss: 0.4182 - val_acc: 0.9105\n",
      "Epoch 140/150\n",
      "690/690 [==============================] - 0s - loss: 0.3091 - acc: 0.9522 - val_loss: 0.7689 - val_acc: 0.8585\n",
      "Epoch 141/150\n",
      "690/690 [==============================] - 0s - loss: 0.3125 - acc: 0.9464 - val_loss: 0.4260 - val_acc: 0.9105\n",
      "Epoch 142/150\n",
      "690/690 [==============================] - 0s - loss: 0.2800 - acc: 0.9609 - val_loss: 0.5271 - val_acc: 0.8971\n",
      "Epoch 143/150\n",
      "690/690 [==============================] - 0s - loss: 0.2908 - acc: 0.9551 - val_loss: 0.5747 - val_acc: 0.8846\n",
      "Epoch 144/150\n",
      "690/690 [==============================] - 0s - loss: 0.2783 - acc: 0.9565 - val_loss: 0.4246 - val_acc: 0.9084\n",
      "Epoch 145/150\n",
      "690/690 [==============================] - 0s - loss: 0.2854 - acc: 0.9464 - val_loss: 0.5176 - val_acc: 0.8899\n",
      "Epoch 146/150\n",
      "690/690 [==============================] - 0s - loss: 0.2708 - acc: 0.9638 - val_loss: 0.5302 - val_acc: 0.8903\n",
      "Epoch 147/150\n",
      "690/690 [==============================] - 0s - loss: 0.2553 - acc: 0.9696 - val_loss: 0.6030 - val_acc: 0.8876\n",
      "Epoch 148/150\n",
      "690/690 [==============================] - 0s - loss: 0.2462 - acc: 0.9652 - val_loss: 0.4655 - val_acc: 0.9121\n",
      "Epoch 149/150\n",
      "690/690 [==============================] - 0s - loss: 0.2692 - acc: 0.9623 - val_loss: 0.5169 - val_acc: 0.9004\n",
      "Epoch 150/150\n",
      "690/690 [==============================] - 0s - loss: 0.2579 - acc: 0.9667 - val_loss: 0.4437 - val_acc: 0.9090\n",
      "18108/18108 [==============================] - 1s     \n",
      "\n",
      "acc: 90.90%\n",
      "17280/18108 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16f8a26908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt(\"train.txt\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:110]\n",
    "Y = dataset[:,110]\n",
    "\n",
    "dataset_test = np.loadtxt(\"test.txt\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X_test = dataset_test[:,0:110]\n",
    "Y_test = dataset_test[:,110]\n",
    "\n",
    "l2val=0.018\n",
    "model=Sequential();\n",
    "model.add(Dense(256, input_dim=110, init='uniform', activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(256, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(256, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(256, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(256, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(256, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(256, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(128, activation='relu',init='uniform',kernel_regularizer=regularizers.l2(l2val)))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.001, decay=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y,validation_data=(X_test,Y_test), epochs=150, batch_size=128)\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "#np.savetxt('testout.txt',probs,delimiter=',')\n",
    "preds = probs[:,0]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('ROC py1')\n",
    "plt.plot(tpr, 1-fpr, 'b')\n",
    "#plt.plot(tpr, fpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "#plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "#plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('signal efficiency')\n",
    "plt.ylabel('background rejection')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model_test.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_test.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.4 GPU ML",
   "language": "python",
   "name": "sys_kernel_py3.4_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
