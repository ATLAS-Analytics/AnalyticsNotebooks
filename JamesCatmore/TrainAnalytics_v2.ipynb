{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines set up inline plotting, and apply a standard size\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **{'size': 22})\n",
    "# Standard includes\n",
    "import datetime\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define query for the tasks\n",
    "taskQuery = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                #{\"term\":{\"transhome\": \"AthDerivation-21.2.8.0\"}}\n",
    "                #{\"term\":{\"transhome\": \"AthDerivation-21.2.9.0\"}}\n",
    "                {\"term\":{\"transhome\": \"AthDerivation-21.2.10.0\"}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute the queries\n",
    "es = Elasticsearch(['atlas-kibana-dev.mwt2.org'],timeout=120)\n",
    "\n",
    "# Tasks\n",
    "taskIndex = \"tasks*\"\n",
    "tasks = scan(es, query=taskQuery, index=taskIndex, scroll='5m', timeout=\"5m\", size=1000)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List the available information\n",
    "#count = 0\n",
    "#for res in tasks:\n",
    "    #if count>0:\n",
    "    #    break\n",
    "    #count += 1\n",
    "    #print (\"Available keys = \",res.keys())\n",
    "#    print(\"_index = \",res['_index']) #str\n",
    "#    print(\"_source = \",res['_source'].keys()) #dict\n",
    "#    print(\"_id = \",res['_id']) #str\n",
    "#    print(\"sort = \",res['sort']) #list\n",
    "#    print(\"_type = \",res['_type']) #str\n",
    "#    print(\"_score = \",res['_score']) #NoneType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the trains and the quantities for collection\n",
    "quantities = [\n",
    "    'cpuconsumptiontime',\n",
    "    'nevents',\n",
    "    'starttime',\n",
    "    'endtime',\n",
    "    'timeExe',\n",
    "    'timeSetup',\n",
    "    'timeGetJob',\n",
    "    'timeStageIn',\n",
    "    'timeStageOut',\n",
    "    'actualcorecount',\n",
    "    'wall_time',\n",
    "    'inputfilebytes',\n",
    "    'outputfilebytes',\n",
    "    'Max_PSS_per_core'\n",
    "]\n",
    "\n",
    "# Data structure as follows:\n",
    "# List of tuples, one per train, with following contents:\n",
    "# ( [List of carriages in the train],[list of jobs for that train],{map of quantity name->list of values for the jobs})\n",
    "trainsAndIDs = [\n",
    "    (['DAOD_SUSY8'],[],{}),\n",
    "    (['DAOD_EXOT4'],[],{}),\n",
    "    (['DAOD_EXOT15'],[],{}),\n",
    "    (['DAOD_STDM9'],[],{}),\n",
    "    (['DAOD_EXOT7','DAOD_EXOT15'],[],{}),\n",
    "    (['DAOD_HIGG1D1'],[],{})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching tasks =  3069\n",
      "Time to extract information =  1.9331379234790802\n"
     ]
    }
   ],
   "source": [
    "# Get the IDs matching each train\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "matchingTasks = 0\n",
    "for res in tasks:\n",
    "    if 'output_formats' in res['_source'].keys():\n",
    "        for item in trainsAndIDs:\n",
    "            train = item[0]\n",
    "            if set(res['_source']['output_formats']) == set(train):\n",
    "                item[1].append(res['_id'])\n",
    "                matchingTasks += 1\n",
    "print(\"Total matching tasks = \",matchingTasks)\n",
    "print(\"Time to extract information = \",timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing task  100  Elapsed Time: 25.2772010974586 \t projected finish in: 750.4801005835459\n",
      "Processing task  200  Elapsed Time: 51.55242278240621 \t projected finish in: 739.5195048136171\n",
      "Processing task  300  Elapsed Time: 77.56485719792545 \t projected finish in: 715.923631936852\n",
      "Processing task  400  Elapsed Time: 103.52857425808907 \t projected finish in: 690.7944117370993\n",
      "Processing task  500  Elapsed Time: 129.08087248168886 \t projected finish in: 663.2175228109174\n",
      "Processing task  600  Elapsed Time: 154.42167072184384 \t projected finish in: 635.4451750203874\n",
      "Processing task  700  Elapsed Time: 180.05758000724018 \t projected finish in: 609.3662957673599\n",
      "Processing task  800  Elapsed Time: 206.28372655808926 \t projected finish in: 585.0722194503805\n",
      "Processing task  900  Elapsed Time: 232.0355133805424 \t projected finish in: 559.2055872471072\n",
      "Processing task  1000  Elapsed Time: 257.0816338695586 \t projected finish in: 531.9019004761167\n",
      "Processing task  1100  Elapsed Time: 282.54095640964806 \t projected finish in: 505.7483119732699\n",
      "Processing task  1200  Elapsed Time: 307.88777871616185 \t projected finish in: 479.53521535042216\n",
      "Processing task  1300  Elapsed Time: 333.4148878958076 \t projected finish in: 453.7007205289875\n",
      "Processing task  1400  Elapsed Time: 360.0000137668103 \t projected finish in: 429.17144498343316\n",
      "Processing task  1500  Elapsed Time: 385.3497058264911 \t projected finish in: 403.0757922945097\n",
      "Processing task  1600  Elapsed Time: 410.62206930294633 \t projected finish in: 377.0023873787677\n",
      "Processing task  1700  Elapsed Time: 436.03655182011425 \t projected finish in: 351.137670259845\n",
      "Processing task  1800  Elapsed Time: 462.68057735450566 \t projected finish in: 326.1898070349265\n",
      "Processing task  1900  Elapsed Time: 489.1891961004585 \t projected finish in: 300.9800896007557\n",
      "Processing task  2000  Elapsed Time: 513.3230911847204 \t projected finish in: 274.3711922382331\n",
      "Processing task  2100  Elapsed Time: 537.227838460356 \t projected finish in: 247.89227403242137\n",
      "Processing task  2200  Elapsed Time: 561.4769375231117 \t projected finish in: 221.7833903216291\n",
      "Processing task  2300  Elapsed Time: 585.407889155671 \t projected finish in: 195.72985511335264\n",
      "Processing task  2400  Elapsed Time: 609.5853650737554 \t projected finish in: 169.92192051430936\n",
      "Processing task  2500  Elapsed Time: 633.0598138999194 \t projected finish in: 144.0844136436217\n",
      "Processing task  2600  Elapsed Time: 657.3386294171214 \t projected finish in: 118.57377584485766\n",
      "Processing task  2700  Elapsed Time: 680.7946591693908 \t projected finish in: 93.04193675315003\n",
      "Processing task  2800  Elapsed Time: 704.6340392474085 \t projected finish in: 67.69519877055473\n",
      "Processing task  2900  Elapsed Time: 728.1869187969714 \t projected finish in: 42.43572044023733\n",
      "Processing task  3000  Elapsed Time: 753.4122733678669 \t projected finish in: 17.32848228746093\n"
     ]
    }
   ],
   "source": [
    "# Set up query for the jobs relevant to the trains\n",
    "start_time = timeit.default_timer()\n",
    "taskCounter = 0\n",
    "jobIndex = \"jobs_archive_2018*,jobs_archive_2017*\"\n",
    "\n",
    "to_read = quantities\n",
    "to_read.append('jeditaskid')\n",
    "to_read.append('transformation')\n",
    "\n",
    "for item in trainsAndIDs: # Loop over trains\n",
    "    #taskCounterPerTrain = 0\n",
    "    for theId in item[1]: # Loop over tasks for that train\n",
    "        taskCounter += 1\n",
    "        #taskCounterPerTrain += 1\n",
    "        #if taskCounterPerTrain > 10: break\n",
    "        if taskCounter % 100 == 0: \n",
    "            et = timeit.default_timer() - start_time\n",
    "            print(\"Processing task \",taskCounter,\" Elapsed Time:\", et, '\\t projected finish in:',et/taskCounter*matchingTasks - et )\n",
    "        jobQuery = {\n",
    "            \"_source\": to_read,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"term\":{\"jeditaskid\": theId}},\n",
    "                        {\"term\":{\"jobstatus\": \"finished\"}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # query the jobs\n",
    "        jobs = scan(es, query=jobQuery, index=jobIndex, scroll='5m', timeout=\"5m\", size=1000)\n",
    "        for res in jobs: # Loop over jobs from that task\n",
    "            for quantity in quantities: # get the relevant quantities\n",
    "                if quantity in res['_source'].keys(): \n",
    "                    if quantity not in item[2].keys(): # store the quantities in a long list\n",
    "                        item[2][quantity] = [res['_source'][quantity]]\n",
    "                    else:\n",
    "                        item[2][quantity].append(res['_source'][quantity])\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/ipykernel_launcher.py:65: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cpuconsumptiontime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9eb8aeafecf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainsAndIDs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     cpuconsumptiontime = (np.asarray(train[2]['cpuconsumptiontime'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n\u001b[0m\u001b[1;32m     23\u001b[0m                           \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cpuconsumptiontime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"DAODMerge_tf.py\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                          )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cpuconsumptiontime'"
     ]
    }
   ],
   "source": [
    "# Calculate the quantities required for plotting from the raw information\n",
    "# Note that for this cell and beyond the code is not generic, e.g. it has to be manually configured for\n",
    "# whatever variables are needed in the plot\n",
    "meanCPUPerEvent = ([],[]) # Reco, Merge\n",
    "meanCPU = ([],[]) # (Reco, Merge)\n",
    "totalCPU = ([],[]) # (Reco, Merge)\n",
    "totalDuration = ([],[]) # (Reco, Merge)\n",
    "totalWalltime = ([],[]) # (Reco, Merge)\n",
    "totalTimeSetup = ([],[])\n",
    "totalTimeGetJob = ([],[]) # (Reco, Merge)\n",
    "totalTimeExe = ([],[]) # (Reco, Merge)\n",
    "totalTimeStageIn = ([],[]) # (Reco, Merge)\n",
    "totalTimeStageOut = ([],[]) # (Reco, Merge)\n",
    "cpuOverDuration = ([],[]) # (Reco, Merge)\n",
    "cpuOverWalltime = ([],[]) # (Reco, Merge)\n",
    "walltimeOverDuration = ([],[]) # (Reco, Merge)\n",
    "actualcorecount = ([],[]) # (Reco, Merge)\n",
    "cpuIntensity = ([],[]) # (Reco, Merge)\n",
    "#maxPSSPerCore = ([],[]) # (Reco, Merge)\n",
    "\n",
    "for train in trainsAndIDs:\n",
    "    cpuconsumptiontime = (np.asarray(train[2]['cpuconsumptiontime'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                          np.asarray(train[2]['cpuconsumptiontime'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                         )\n",
    "    nevents = (np.asarray(train[2]['nevents'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "               np.asarray(train[2]['nevents'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "              )\n",
    "    starttimes = (np.array(train[2]['starttime'],dtype='datetime64')[np.array(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                  np.array(train[2]['starttime'],dtype='datetime64')[np.array(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                 )\n",
    "    endtimes = (np.array(train[2]['endtime'],dtype='datetime64')[np.array(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                np.array(train[2]['endtime'],dtype='datetime64')[np.array(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "               )\n",
    "    setupTimes = (np.asarray(train[2]['timeSetup'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                  np.asarray(train[2]['timeSetup'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                 )\n",
    "    getJobTimes = (np.asarray(train[2]['timeGetJob'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                   np.asarray(train[2]['timeGetJob'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                  )\n",
    "    exeTimes = (np.asarray(train[2]['timeExe'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                np.asarray(train[2]['timeExe'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "               )\n",
    "    stageInTimes = (np.asarray(train[2]['timeStageIn'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                    np.asarray(train[2]['timeStageIn'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                   )\n",
    "    stageOutTimes = (np.asarray(train[2]['timeStageOut'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                     np.asarray(train[2]['timeStageOut'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                    )\n",
    "    actualcorecount = (np.asarray(train[2]['actualcorecount'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                       np.asarray(train[2]['actualcorecount'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                      ) \n",
    "    wall_time = (np.asarray(train[2]['wall_time'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                 np.asarray(train[2]['wall_time'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                )\n",
    "    inputSize = (np.asarray(train[2]['inputfilebytes'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                 np.asarray(train[2]['inputfilebytes'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                )\n",
    "    outputSize = (np.asarray(train[2]['outputfilebytes'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "                  np.asarray(train[2]['outputfilebytes'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "                 )\n",
    "    #Max_PSS_per_core = (np.asarray(train[2]['Max_PSS_per_core'])[np.asarray(train[2]['transformation'])==\"Reco_tf.py\"],\n",
    "    #                    np.asarray(train[2]['Max_PSS_per_core'])[np.asarray(train[2]['transformation'])==\"DAODMerge_tf.py\"]\n",
    "    #                   )\n",
    "    \n",
    "    cpuPerEvent = (cpuconsumptiontime[0]/nevents[0],cpuconsumptiontime[1]/nevents[1])\n",
    "    cpuPerEvent = (cpuPerEvent[0][np.isfinite(cpuPerEvent[0])],cpuPerEvent[1][np.isfinite(cpuPerEvent[1])])\n",
    "    meanCPUPerEvent[0].append(cpuPerEvent[0].mean())\n",
    "    meanCPUPerEvent[1].append(cpuPerEvent[1].mean())\n",
    "    meanCPU[0].append(cpuconsumptiontime[0].mean())\n",
    "    meanCPU[1].append(cpuconsumptiontime[1].mean())\n",
    "    totalCPU[0].append(cpuconsumptiontime[0].sum())\n",
    "    totalCPU[1].append(cpuconsumptiontime[1].sum())\n",
    "    totalDuration[0].append((endtimes[0]-starttimes[0]).sum())\n",
    "    totalDuration[1].append((endtimes[1]-starttimes[1]).sum())\n",
    "    totalWalltime[0].append(wall_time[0].sum())\n",
    "    totalWalltime[1].append(wall_time[1].sum())\n",
    "    totalTimeSetup[0].append(setupTimes[0].sum())\n",
    "    totalTimeSetup[1].append(setupTimes[1].sum())\n",
    "    totalTimeGetJob[0].append(getJobTimes[0].sum())\n",
    "    totalTimeGetJob[1].append(getJobTimes[1].sum())\n",
    "    totalTimeExe[0].append(exeTimes[0].sum())\n",
    "    totalTimeExe[1].append(exeTimes[1].sum())\n",
    "    totalTimeStageIn[0].append(stageInTimes[0].sum())\n",
    "    totalTimeStageIn[1].append(stageInTimes[1].sum())\n",
    "    totalTimeStageOut[0].append(stageOutTimes[0].sum())\n",
    "    totalTimeStageOut[1].append(stageOutTimes[1].sum())\n",
    "    cpuOverDuration[0].append((cpuconsumptiontime[0]/actualcorecount[0]).sum()/((endtimes[0]-starttimes[0]).sum()/np.timedelta64(1, 's')))\n",
    "    cpuOverDuration[1].append((cpuconsumptiontime[1]/actualcorecount[1]).sum()/((endtimes[1]-starttimes[1]).sum()/np.timedelta64(1, 's')))\n",
    "    cpuOverWalltime[0].append((cpuconsumptiontime[0]/actualcorecount[0]).sum()/wall_time[0].sum())\n",
    "    cpuOverWalltime[1].append((cpuconsumptiontime[1]/actualcorecount[1]).sum()/wall_time[1].sum())\n",
    "    walltimeOverDuration[0].append(wall_time[0].sum()/((endtimes[0]-starttimes[0]).sum()/np.timedelta64(1, 's')))\n",
    "    walltimeOverDuration[1].append(wall_time[1].sum()/((endtimes[1]-starttimes[1]).sum()/np.timedelta64(1, 's')))\n",
    "    cpuIntensity[0].append( ((inputSize[0]+outputSize[0]).sum()/1000000.0)/wall_time[0].sum() )\n",
    "    cpuIntensity[1].append( ((inputSize[1]+outputSize[1]).sum()/1000000.0)/wall_time[1].sum() )\n",
    "    #maxPSSPerCore[0].append( Max_PSS_per_core[0].sum())\n",
    "    #maxPSSPerCore[1].append( Max_PSS_per_core[1].sum())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up flat names for the tick labels\n",
    "train_names = []\n",
    "for item in trainsAndIDs:\n",
    "    names = item[0]\n",
    "    longName = ''\n",
    "    for name in names:\n",
    "        name = name.strip(\"DAOD_\")\n",
    "        longName += name+' '\n",
    "    train_names.append(longName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - mean CPU per event by train type\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "ax.barh(y_pos, meanCPUPerEvent[0], align='center',color='lightblue',label=\"Reco_tf\")\n",
    "ax.barh(y_pos, meanCPUPerEvent[1], align='center',color='crimson',left=meanCPUPerEvent[0],label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Mean CPU per event (seconds)',fontsize=12)\n",
    "ax.set_title('Mean CPU per event by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting - mean CPU per job by train type\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "ax.barh(y_pos, meanCPU[0], align='center',color='lightblue',label=\"Reco_tf\")\n",
    "ax.barh(y_pos, meanCPU[1], align='center',color='crimson',left=meanCPU[0],label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Mean CPU per job (seconds)',fontsize=12)\n",
    "ax.set_title('Mean CPU per job by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - total CPU by train type\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "ax.barh(y_pos, totalCPU[0], align='center',color='lightblue',label=\"Reco_tf\")\n",
    "ax.barh(y_pos, totalCPU[1], align='center',color='crimson',left=totalCPU[0],label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total CPU (seconds)',fontsize=12)\n",
    "ax.set_title('Total CPU by train type, AthDerivation-21.2.9.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "ax.barh(y_pos, totalDuration[0]/np.timedelta64(1, 's'), align='center',color='lightblue',label=\"Reco_tf\")\n",
    "ax.barh(y_pos, totalDuration[1]/np.timedelta64(1, 's'), align='center',color='crimson',left=totalDuration[0]/np.timedelta64(1, 's'),label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total duration (seconds)',fontsize=12)\n",
    "ax.set_title('Total job duration (end time - start time) by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - total wall time\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "width=0.35\n",
    "rects1 = ax.barh(y_pos, totalWalltime[0], width,color='lightblue',label=\"Reco_tf\")\n",
    "rects2 = ax.barh(y_pos+width, totalWalltime[1], width,color='crimson',label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total wall time (seconds)',fontsize=12)\n",
    "ax.set_title('Total wall time by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - total CPU / total wall time\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "width=0.35\n",
    "rects1 = ax.barh(y_pos, cpuOverWalltime[0], width,color='lightblue',label=\"Reco_tf\")\n",
    "rects2 = ax.barh(y_pos+width, cpuOverWalltime[1], width,color='crimson',label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total CPU / total wall time (seconds)',fontsize=12)\n",
    "ax.set_title('Total CPU / total wall time by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - total wall time / total duration\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "width=0.35\n",
    "rects1 = ax.barh(y_pos, walltimeOverDuration[0], width,color='lightblue',label=\"Reco_tf\")\n",
    "rects2 = ax.barh(y_pos+width, walltimeOverDuration[1], width,color='crimson',label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total wall time / total duration',fontsize=12)\n",
    "ax.set_title('Total wall time over total duration by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - total CPU / total duration\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "width=0.35\n",
    "rects1 = ax.barh(y_pos, cpuOverDuration[0], width,color='lightblue',label=\"Reco_tf\")\n",
    "rects2 = ax.barh(y_pos+width, cpuOverDuration[1], width,color='crimson',label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total CPU / total duration',fontsize=12)\n",
    "ax.set_title('Total CPU / total job duration  by train type, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - I/O intensity\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "width=0.35\n",
    "rects1 = ax.barh(y_pos, cpuIntensity[0], width,color='lightblue',label=\"Reco_tf\")\n",
    "rects2 = ax.barh(y_pos+width, cpuIntensity[1], width,color='crimson',label=\"DAODMerge_tf\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('I/O intensity, MB/s',fontsize=12)\n",
    "ax.set_title('I/O intensity: (input size + output size)/walltime, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting - max PSS per core\n",
    "#fig, ax = plt.subplots(figsize=(10,10))\n",
    "#y_pos = np.arange(len(train_names))\n",
    "#width=0.35\n",
    "#rects1 = ax.barh(y_pos, maxPSSPerCore[0], width,color='lightblue',label=\"Reco_tf\")\n",
    "#rects2 = ax.barh(y_pos+width, maxPSSPerCore[1], width,color='crimson',label=\"DAODMerge_tf\")\n",
    "#ax.set_yticks(y_pos)\n",
    "#ax.set_yticklabels(train_names,fontsize=12)\n",
    "#ax.invert_yaxis() \n",
    "#ax.set_xlabel('Max PSS per core, GB',fontsize=12)\n",
    "#ax.set_title('Maxx PSS per core, AthDerivation-21.2.8.0',fontsize=12)\n",
    "#ax.tick_params(axis='x', labelsize=12)\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Colours\n",
    "import matplotlib.cm as cm\n",
    "viridis = cm.viridis(np.linspace(0, 1, 5))\n",
    "magma = cm.magma(np.linspace(0, 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "ax.barh(y_pos, totalTimeSetup[0], align='center',color=viridis[0],label=\"Setup\")\n",
    "ax.barh(y_pos, totalTimeGetJob[0], align='center',color=viridis[1],left=totalTimeSetup[0],label=\"GetJob\")\n",
    "ax.barh(y_pos, totalTimeStageIn[0], align='center',color=viridis[2],left=totalTimeSetup[0],label=\"Stage in\")\n",
    "ax.barh(y_pos, totalTimeExe[0], align='center',color=viridis[3],left=totalTimeSetup[0],label=\"Execute\")\n",
    "ax.barh(y_pos, totalTimeStageOut[0], align='center',color=viridis[4],left=totalTimeSetup[0],label=\"Stage out\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total time (seconds)',fontsize=12)\n",
    "ax.set_title('Total time spent in various activities within jobs, Reco_tf, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "#ax.set_xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "y_pos = np.arange(len(train_names))\n",
    "ax.barh(y_pos, totalTimeSetup[1], align='center',color=magma[0],label=\"Setup\")\n",
    "ax.barh(y_pos, totalTimeGetJob[1], align='center',color=magma[1],left=totalTimeSetup[0],label=\"GetJob\")\n",
    "ax.barh(y_pos, totalTimeStageIn[1], align='center',color=magma[2],left=totalTimeSetup[0],label=\"Stage in\")\n",
    "ax.barh(y_pos, totalTimeExe[1], align='center',color=magma[3],left=totalTimeSetup[0],label=\"Execute\")\n",
    "ax.barh(y_pos, totalTimeStageOut[1], align='center',color=magma[4],left=totalTimeSetup[0],label=\"Stage out\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(train_names,fontsize=12)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Total time (seconds)',fontsize=12)\n",
    "ax.set_title('Total time spent in various activities within jobs, DAODMerge_tf, AthDerivation-21.2.10.0',fontsize=12)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "#ax.set_xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
