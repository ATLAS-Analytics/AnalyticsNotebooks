{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_host      = 'atlas-kibana.mwt2.org'\n",
    "db_port      = 9200\n",
    "db_index     = 'jobs_archive_2018-04-0*'\n",
    "\n",
    "query_source = [\n",
    "    'jobstatus',\n",
    "    'jobname',\n",
    "    'computingsite', \n",
    "    'taskid', \n",
    "    'pandaid', \n",
    "    'attemptnr',\n",
    "    'exeerrordiag',\n",
    "    'piloterrordiag',\n",
    "    'jobdispatchererrordiag',\n",
    "    'transformation'\n",
    "]\n",
    "\n",
    "query_should = [              \n",
    "    {'term':{'jobstatus': 'finished'}},\n",
    "    {'term':{'jobstatus': 'failed'}},\n",
    "    {'term':{'jobstatus': 'closed'}},\n",
    "]\n",
    "\n",
    "query_must_not = [\n",
    "    {'term':{'gShare': 'Analysis'}},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf> index:  jobs_archive_2018-04-0*\n",
      "inf> processed 100000 events\n",
      "inf> processed 200000 events\n",
      "inf> processed 300000 events\n",
      "inf> processed 400000 events\n",
      "inf> processed 500000 events\n",
      "inf> processed 600000 events\n",
      "inf> processed 700000 events\n",
      "inf> processed 800000 events\n",
      "inf> processed 900000 events\n",
      "inf> processed 1000000 events\n",
      "inf> processed 1100000 events\n",
      "inf> processed 1200000 events\n",
      "inf> processed 1300000 events\n",
      "inf> processed 1400000 events\n",
      "inf> processed 1500000 events\n",
      "inf> processed 1600000 events\n",
      "inf> processed 1700000 events\n",
      "inf> processed 1800000 events\n",
      "inf> processed 1900000 events\n",
      "inf> processed 2000000 events\n",
      "inf> processed 2100000 events\n",
      "inf> processed 2200000 events\n",
      "inf> processed 2300000 events\n",
      "inf> processed 2400000 events\n",
      "inf> processed 2500000 events\n",
      "inf> processed 2600000 events\n",
      "inf> processed 2700000 events\n",
      "inf> processed 2800000 events\n",
      "inf> processed 2900000 events\n",
      "inf> processed 3000000 events\n",
      "inf> processed 3100000 events\n",
      "inf> processed 3200000 events\n",
      "inf> processed 3300000 events\n",
      "inf> processed 3400000 events\n",
      "inf> processed 3500000 events\n",
      "inf> processed 3600000 events\n",
      "inf> processed 3700000 events\n",
      "inf> processed 3800000 events\n",
      "inf> processed 3900000 events\n",
      "inf> processed 4000000 events\n",
      "inf> processed 4100000 events\n",
      "inf> processed 4200000 events\n",
      "inf> processed 4300000 events\n",
      "inf> processed 4400000 events\n",
      "inf> processed 4500000 events\n",
      "inf> processed 4600000 events\n",
      "inf> processed 4700000 events\n",
      "inf> processed 4800000 events\n",
      "inf> processed 4900000 events\n",
      "inf> total 9175 tasks queried\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers \n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "es = Elasticsearch([{'host':db_host, 'port':db_port}], timeout=60)\n",
    "\n",
    "indices = es.cat.indices(index=db_index, h='index', \n",
    "                         request_timeout=600).split('\\n')\n",
    "indices = sorted(indices)\n",
    "indices = [ii for ii in indices if ii != '']\n",
    "indices = ','.join(indices)\n",
    "\n",
    "job_query = {\n",
    "    'size'   : 0, \n",
    "    '_source': query_source, \n",
    "    'query':   {'bool': {'should': query_should, \n",
    "                         'minimum_should_match':1, \n",
    "                         'must_not': query_must_not}}\n",
    "}\n",
    "\n",
    "scroll = scan(client=es, index=indices, query=job_query)\n",
    "\n",
    "results = {}\n",
    "count = 1\n",
    "\n",
    "print ('inf> index: ', db_index)\n",
    "\n",
    "for res in scroll:\n",
    "    r = res['_source']\n",
    "    job_info = []\n",
    "    \n",
    "    for source in query_source:\n",
    "        job_info.append(r[source])\n",
    "            \n",
    "    if r['taskid'] in results.keys():\n",
    "        results[r['taskid']].append(job_info)\n",
    "    else:\n",
    "        results[r['taskid']] = [job_info]\n",
    "    \n",
    "    if (count % 100000) == 0:\n",
    "        print ('inf> processed', count, 'events')\n",
    "    count += 1\n",
    "   \n",
    "print ('inf> total', len(results), 'tasks queried')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf> start labeling\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print ('inf> start labeling')\n",
    "output = open('error.txt', 'w')\n",
    "\n",
    "for key, values in results.items():\n",
    "        \n",
    "    job_initial = {}\n",
    "    job_final   = {}\n",
    "        \n",
    "    for value in values:\n",
    "        \n",
    "        jobstatus      = value[0]\n",
    "        jobname        = '%s %s' % (value[8], value[1])\n",
    "        computingsite  = value[2]\n",
    "        pandaid        = value[4]\n",
    "        attemptnr      = value[5]\n",
    "        exeerrordiag   = value[6]\n",
    "        piloterrordiag = value[7]\n",
    "        \n",
    "        job_info = [jobstatus, computingsite, attemptnr, \n",
    "                    exeerrordiag, piloterrordiag]\n",
    "        \n",
    "        if not jobname in job_initial.keys():\n",
    "            job_initial[jobname] = job_info\n",
    "            job_final[jobname] = job_info\n",
    "        else:\n",
    "            if attemptnr < job_initial[jobname][2]:\n",
    "                job_initial[jobname] = job_info\n",
    "            if attemptnr > job_final[jobname][2]:\n",
    "                job_final[jobname] = job_info\n",
    "    \n",
    "    job_success = {}\n",
    "    job_fail    = {}\n",
    "    \n",
    "    for name, info in job_initial.items():\n",
    "        if job_final[name][0] == 'finished':\n",
    "            if job_final[name][1] in job_success.keys():\n",
    "                job_success[job_final[name][1]] += 1\n",
    "            else:\n",
    "                job_success[job_final[name][1]] = 1\n",
    "                    \n",
    "        if job_final[name][0] == 'failed':\n",
    "            if job_final[name][1] in job_fail.keys():\n",
    "                job_fail[job_final[name][1]] += 1\n",
    "            else:\n",
    "                job_fail[job_final[name][1]] = 1\n",
    "                \n",
    "\n",
    "    for name, info in job_initial.items():\n",
    "    \n",
    "        jobstatus_init  = info[0]\n",
    "        computing_init  = info[1]\n",
    "        jobstatus_final = job_final[name][0]\n",
    "        computing_final = job_final[name][1]\n",
    "        exeerr_init     = info[3]\n",
    "        piloterr_init   = info[4]\n",
    "        \n",
    "        # good job\n",
    "        if jobstatus_init == 'finished':\n",
    "            continue\n",
    "    \n",
    "        # OK?\n",
    "        if (exeerr_init == 'OK'):\n",
    "            continue\n",
    "    \n",
    "        # no error\n",
    "        if (exeerr_init == None) and (piloterr_init == None):\n",
    "            continue\n",
    "        \n",
    "        # 0=unkown, 1=resubmit, 2=athena problem, 3=site problem\n",
    "        label = ''\n",
    "        if jobstatus_final != 'finished':\n",
    "            if (computing_final in job_success.keys()) and (job_success[computing_final] > 5): \n",
    "                label = 'resubmit'\n",
    "            elif info[1] != job_final[name][1]:\n",
    "                label = 'athena'\n",
    "            else:\n",
    "                nfails = 0\n",
    "                for site in job_fail.keys():\n",
    "                    if site == computing_final:\n",
    "                        continue\n",
    "                    nfails += job_fail[site]\n",
    "                \n",
    "                if nfails > 5:\n",
    "                    label = 'athena'\n",
    "                else:\n",
    "                    label = 'site'\n",
    "        else:\n",
    "            if (computing_init in job_success.keys()) and (job_success[computing_init] > 5):\n",
    "                label = 'resubmit'\n",
    "            elif info[1] != job_final[name][1]:\n",
    "                label = 'site'\n",
    "            else:\n",
    "                label = 'resubmit'\n",
    "                \n",
    "        if exeerr_init != None:\n",
    "            exeerr_init = exeerr_init.replace('\\n', '')\n",
    "        if piloterr_init != None:\n",
    "            piloterr_init = piloterr_init.replace('\\n', '')\n",
    "        \n",
    "        output.write('%s,,,,%s,,,,%s,,,,%s\\n' % (computing_init, exeerr_init, piloterr_init, label))\n",
    "print ('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
