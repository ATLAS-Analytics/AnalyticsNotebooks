{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from elasticsearch.helpers import bulk\n",
    "es = Elasticsearch(['atlas-kibana.mwt2.org:9200'],timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### find all the combinations of sources and destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = \"network_weather_2-2016.8.*\"\n",
    "\n",
    "my_query = \\\n",
    "{\n",
    "  \"size\": 0,\n",
    "  \"aggs\": {      \n",
    "        \"type_counts\": {\n",
    "            \"terms\": { \"field\": \"_type\" },\n",
    "            \"aggs\": {      \n",
    "                \"src_counts\": {\n",
    "                     \"terms\": { \"field\": \"src\" , \"size\": 10000},\n",
    "                     \"aggs\": {      \n",
    "                        \"dest_counts\": {\n",
    "                        \"terms\": { \"field\": \"dest\" , \"size\": 10000}\n",
    "                        }\n",
    "                     }\n",
    "                  }\n",
    "              }\n",
    "        }\n",
    "  }\n",
    "}\n",
    "response = es.search(index=ind, body=my_query, request_timeout=1200)\n",
    "#print(response['aggregations']['type_counts']['buckets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for cleaning specific links\n",
    "limitTo=['192.41.230.60','192.41.236.32','192.41.230.59','192.41.236.31','128.142.223.247','128.142.223.246']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packet_loss_rate\n",
      "latency\n",
      "traceroute\n",
      "final delete 410\n",
      "final delete 271\n",
      "final delete 807\n",
      "final delete 727\n",
      "final delete 658\n",
      "final delete 220\n",
      "final delete 450\n",
      "final delete 356\n",
      "final delete 65\n",
      "final delete 170\n",
      "final delete 137\n",
      "final delete 154\n",
      "final delete 55\n",
      "final delete 69\n",
      "final delete 67\n",
      "final delete 76\n",
      "final delete 69\n",
      "final delete 61\n",
      "final delete 77\n",
      "final delete 82\n",
      "final delete 79\n",
      "final delete 74\n",
      "final delete 80\n",
      "final delete 79\n",
      "final delete 72\n",
      "final delete 84\n",
      "final delete 75\n",
      "final delete 77\n",
      "final delete 79\n",
      "final delete 67\n",
      "traceroute Pairs: 7456\n",
      "throughput\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "types = response['aggregations']['type_counts']['buckets']\n",
    "\n",
    "for t in types:\n",
    "    allPairs=0\n",
    "    print(t['key'])\n",
    "    if not t['key']=='traceroute': continue\n",
    "    for s in t['src_counts']['buckets']:\n",
    "        bulk_body = []\n",
    "        dcounter=0\n",
    "        for d in s['dest_counts']['buckets']:\n",
    "            allPairs += 1\n",
    "            src=s['key']\n",
    "            dest=d['key']\n",
    "            #if src not in limitTo and dest not in limitTo: continue\n",
    "            #print ('s: ',src,'d: ', dest, '\\t docs:', d['doc_count'])\n",
    "            myscroll={\n",
    "            \"size\":100000,\n",
    "#             \"fields\": [\"timestamp\", \"delay_mean\",\"throughput\",\"packet_loss\",\"src\",\"dest\"],\n",
    "            \"fields\": [\"timestamp\"],\n",
    "            \"query\":{\n",
    "                \"filtered\": {\n",
    "                    \"query\": {\n",
    "                         \"match_all\": {} \n",
    "                        },\n",
    "                        \"filter\" : { \n",
    "                            \"bool\" : {\n",
    "                                \"must\" : [\n",
    "                                    {\"term\": {\"_type\": {\"value\": t['key'] } } },\n",
    "                                    {\"term\": {\"src\":   {\"value\": src } } },\n",
    "                                    {\"term\": {\"dest\":  {\"value\": dest } } }\n",
    "                                ]\n",
    "                            }\n",
    "\n",
    "                        }\n",
    "                }\n",
    "            }\n",
    "            ,\"sort\": [{ \"timestamp\": { \"order\": \"asc\" }}]\n",
    "            }\n",
    "            \n",
    "            toDelete={}\n",
    "#             oldres=0\n",
    "            oldtime=\"\"\n",
    "            counter=0\n",
    "            scroll = scan(es, query=myscroll, index=ind, size=5000, scroll='240s', request_timeout=120)\n",
    "            for res in scroll:\n",
    "#                 if counter==0: print(res)\n",
    "                if res['fields']['timestamp'][0]==oldtime:# and res['fields']['src'][0]==src and res['fields']['dest'][0]==dest and res['fields']['packet_loss'][0]==oldres:\n",
    "                    toDelete[res['_id']]=res['_index']\n",
    "#                     print(res['fields'])\n",
    "#                 oldres=res['fields']['packet_loss'][0]\n",
    "                oldtime=res['fields']['timestamp'][0]\n",
    "                counter+=1\n",
    "#                if counter>200:\n",
    "#                    break\n",
    "#                 if counter%100==0: \n",
    "                \n",
    "            #print (\"records:\", counter, '\\tto delete:',len(toDelete))\n",
    "            for x in toDelete:\n",
    "                if dcounter>100000:\n",
    "                    dcounter=0\n",
    "                    es.bulk('\\n'.join(bulk_body), request_timeout=1200)\n",
    "                    bulk_body=[]\n",
    "                    print('deleted 100k')\n",
    "                dcounter+=1\n",
    "                bulk_body.append('{\"delete\": {\"_id\": \"'+x+'\", \"_index\":\"'+toDelete[x]+'\", \"_type\":\"'+t['key']+'\"}}')\n",
    "#             print (bulk_body)\n",
    "        if len(toDelete):\n",
    "            print(\"final delete\", dcounter)\n",
    "            es.bulk('\\n'.join(bulk_body), request_timeout=1200)\n",
    "\n",
    "    print(t['key'],'Pairs:',allPairs)\n",
    "print ('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
