{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a predictive model for the completion time of a file, based on network parameters (throughput measure, # of files queued according to activity, and # of files completed over the previous hour) and per-file parameters (filesize, file activity type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines set up inline plotting, and apply a standard size\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "matplotlib.rc('font', **{'size': 15})\n",
    "\n",
    "\n",
    "src = 'CERN-PROD'\n",
    "dst = 'BNL-ATLAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = '''{\n",
    "    \n",
    "  \"size\": 10000,\n",
    "  \"sort\": [\n",
    "    {\n",
    "      \"@timestamp\": {\n",
    "        \"order\": \"random\",\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"query\": {\n",
    "    \"filtered\": {\n",
    "      \"query\": {\n",
    "        \"query_string\": {\n",
    "          \"query\": \"payload.src-rse:CERN* AND payload.dst-rse:BNL* AND event_type:transfer-done\",\n",
    "          \"analyze_wildcard\": True,\n",
    "          \"lowercase_expanded_terms\": False\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                  \"gte\": %i,\n",
    "                  \"lte\": %i,\n",
    "                  \"format\": \"epoch_millis\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          ],\n",
    "          \"must_not\": []\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"fielddata_fields\": [\n",
    "    \"@timestamp\",\"duration\"\n",
    "  ]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Initialize convenient time intervals in milliseconds\n",
    "milliWeek = 7*24*60*60*1000\n",
    "milliTwoWeek = 2*7*24*60*60*1000\n",
    "milliThreeWeek = 3*7*24*60*60*1000\n",
    "milliFourWeek = 4*7*24*60*60*1000\n",
    "milliSixHour = 12*60*60*1000\n",
    "milliMonth = 30*24*60*60*1000\n",
    "milliDay = 24*60*60*1000\n",
    "milliHalfDay = 12*60*60*1000\n",
    "milliHour = 60*60*1000\n",
    "\n",
    "currentMilliTime = int(round(time.time()*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this analysis, three weeks of network data will be taken. Since on the order of 1 million files are processed a day over a link, looking at data over a 4-week period quickly becomes unwieldy. Because of the volume, per-file level data is acquired in chunks, and 100 random samples are taken from each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "#Get four weeks of data\n",
    "start = currentMilliTime - milliFourWeek\n",
    "end = currentMilliTime\n",
    "\n",
    "hourSeg = 24*7*4\n",
    "\n",
    "#Initialize lists\n",
    "fileSize = []\n",
    "activity = []\n",
    "startedAt = []\n",
    "submittedAt = []\n",
    "transferredAt = []\n",
    "duration = []\n",
    "\n",
    "\n",
    "\n",
    "totalTimestamp = []\n",
    "totalFileSize = []\n",
    "totalActivity = []\n",
    "totalStartedAt = []\n",
    "totalSubmittedAt = []\n",
    "totalTransferredAt = []\n",
    "\n",
    "startPart = start\n",
    "endPart = start\n",
    "\n",
    "tic = time.time()\n",
    "#Step through day, getting all data in range\n",
    "for i in range(hourSeg):\n",
    "    endPart += milliHour\n",
    "    \n",
    "    #Create instance of ElasticSearch using the Atlas host server\n",
    "    es1 = Elasticsearch(hosts=[{'host':'atlas-kibana-2.mwt2.org', 'port':9200}],timeout=20,retry_on_timeout = True)\n",
    "    response = es1.search(index=\"rucio-events*\", body=ast.literal_eval(query1%(startPart,endPart)),\n",
    "                          request_timeout=20)\n",
    "    startPart = endPart + 1\n",
    "    \n",
    "    #Get important fields\n",
    "    for r in response['hits']['hits']:\n",
    "        fileSize.append(r['_source']['payload']['file-size'])\n",
    "        activity.append(r['_source']['payload']['activity'])\n",
    "        startedAt.append(r['_source']['payload']['started_at'])\n",
    "        submittedAt.append(r['_source']['payload']['submitted_at'])\n",
    "        transferredAt.append(r['_source']['payload']['transferred_at'])\n",
    "\n",
    "    #Get random indices in range of length of data vectors\n",
    "    if len(fileSize) < 400:\n",
    "        #Grab all files if there are less than 400\n",
    "        ind = np.random.choice(len(fileSize),size=len(fileSize),replace=False)\n",
    "        \n",
    "    elif len(fileSize) >= 400:\n",
    "        #Otherwise just grab 400 randomly\n",
    "        ind = np.random.choice(len(fileSize),size=400,replace=False)  \n",
    "        \n",
    "    np.random.seed()\n",
    "    \n",
    "    #Append each random sample from the hour's data\n",
    "    totalFileSize += [ fileSize[i] for i in ind]\n",
    "    totalActivity += [ activity[i] for i in ind]\n",
    "    totalStartedAt += [ startedAt[i] for i in ind]\n",
    "    totalSubmittedAt += [ submittedAt[i] for i in ind]\n",
    "    totalTransferredAt += [ transferredAt[i] for i in ind]\n",
    "\n",
    "print(\"Done. Took %f seconds\"%(time.time()-tic))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we acquire data from the ddm-metrics, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryTemp = '''{\n",
    "  \"size\": 0,\n",
    "  \"query\": {\n",
    "    \"filtered\": {\n",
    "      \"query\": {\n",
    "        \"query_string\": {\n",
    "          \"query\": \"src:%s AND dst:%s\",\n",
    "          \"analyze_wildcard\": True\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"timestamp\": {\n",
    "                  \"gte\": %i,\n",
    "                  \"lte\": %i,\n",
    "                  \"format\": \"epoch_millis\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          ],\n",
    "          \"must_not\": []\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"aggs\": {\n",
    "    \"2\": {\n",
    "      \"date_histogram\": {\n",
    "        \"field\": \"timestamp\",\n",
    "        \"interval\": \"20m\",\n",
    "        \"time_zone\": \"Europe/Berlin\",\n",
    "        \"min_doc_count\": 1,\n",
    "        \"extended_bounds\": {\n",
    "          \"min\": %i,\n",
    "          \"max\": %i\n",
    "        }\n",
    "      },\n",
    "  \"aggs\": {\n",
    "    \"1\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-total'\n",
    "          }\n",
    "        },\n",
    "        \"2\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-User_Subscriptions'\n",
    "          }\n",
    "        },\n",
    "        \"3\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-T0_Export'\n",
    "          }\n",
    "        },\n",
    "        \"4\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-Data_Consolidation'\n",
    "          }\n",
    "        },\n",
    "        \"5\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-Functional_Test'\n",
    "          }\n",
    "        },\n",
    "        \"6\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-Express'\n",
    "          }\n",
    "        },\n",
    "        \"7\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-Production_Input'\n",
    "          }\n",
    "        },\n",
    "        \"8\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-Production_Output'\n",
    "          }\n",
    "        },\n",
    "        \"9\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'queued-Staging'\n",
    "          }\n",
    "        },\n",
    "        \"10\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-total-1h'\n",
    "          }\n",
    "        },\n",
    "        \"11\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-User_Subscriptions-1h'\n",
    "          }\n",
    "        },\n",
    "        \"12\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-T0_Export-1h'\n",
    "          }\n",
    "        },\n",
    "        \"13\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-Data_Consolidation-1h'\n",
    "          }\n",
    "        },\n",
    "        \"14\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-Functional_Test-1h'\n",
    "          }\n",
    "        },\n",
    "        \"15\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-Express-1h'\n",
    "          }\n",
    "        },\n",
    "        \"16\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-Production_Input-1h'\n",
    "          }\n",
    "        },\n",
    "        \"17\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-Production_Output-1h'\n",
    "          }\n",
    "        },\n",
    "        \"18\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'done-Staging-1h'\n",
    "              }\n",
    "            },\n",
    "        \n",
    "        \n",
    "        \"19\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'packetloss'\n",
    "              }\n",
    "            },\n",
    "        \"20\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'mbps-dashb-1h'\n",
    "              }\n",
    "            },\n",
    "        \"21\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'mbps-dashb-1d'\n",
    "              }\n",
    "            },\n",
    "        \"22\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'mbps-fts-1h'\n",
    "              }\n",
    "            },\n",
    "        \"23\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'mbps-fts-1d'\n",
    "              }\n",
    "            },\n",
    "        \"24\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'mbps-fax-1h'\n",
    "              }\n",
    "            },\n",
    "        \"25\": {\n",
    "          \"avg\": {\n",
    "            \"field\": 'mbps-fax-1d'\n",
    "              }\n",
    "            }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "src = 'CERN-PROD'\n",
    "dst = 'BNL-ATLAS'\n",
    "\n",
    "#I have to break the data acquisition up into pieces so that Elasticsearch doesn't crash.\n",
    "startPart = start\n",
    "endPart = start\n",
    "numDays = 4*7\n",
    "response = []\n",
    "for i in range(numDays):\n",
    "    \n",
    "    es2 = Elasticsearch(hosts=[{'host':'es-atlas.cern.ch', 'port':9202}],\n",
    "                   http_auth=('es-atlas', 'v~$&<J8/cG9]*eQ@'),\n",
    "                   timeout=20,retry_on_timeout=True)\n",
    "    \n",
    "    print(\"Loading data from Day %i...\"%(i+1))\n",
    "    endPart += milliDay  \n",
    "    query = ast.literal_eval(queryTemp%(src,dst,startPart,endPart,startPart,endPart))\n",
    "    response.append(es2.search(index=\"atlas_ddm-metrics*\", body=query, request_timeout=20))\n",
    "    startPart = endPart+1\n",
    "    \n",
    "print(\"Done. Aggregation took %f seconds\"%(time.time()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to gather data about network health and activity volumes from ddm-metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestamp = []\n",
    "\n",
    "#Get current queued data according to activity\n",
    "queuedTotal = []\n",
    "queuedUserSubscriptions = []\n",
    "queuedT0Export = []\n",
    "queuedStaging = []\n",
    "queuedDataConsolidation = []\n",
    "queuedFunctionalTest = []\n",
    "queuedExpress = []\n",
    "queuedProductionInput = []\n",
    "queuedProductionOutput = []\n",
    "queuedStaging = []\n",
    "\n",
    "#Get historical completion data according to activity\n",
    "doneTotal = []\n",
    "doneUserSubscriptions = []\n",
    "doneT0Export = []\n",
    "doneStaging = []\n",
    "doneDataConsolidation = []\n",
    "doneFunctionalTest = []\n",
    "doneExpress = []\n",
    "doneProductionInput = []\n",
    "doneProductionOutput = []\n",
    "doneStaging = []\n",
    "\n",
    "#Get other parameters\n",
    "packetloss = []\n",
    "dashThroughput1h = []\n",
    "dashThroughput1d = []\n",
    "ftsThroughput1h = []\n",
    "ftsThroughput1d = []\n",
    "faxThroughput1h = []\n",
    "faxThroughput1d = []\n",
    "for i in range(numDays):\n",
    "    for r in response[i]['aggregations']['2']['buckets']:\n",
    "        queuedTotal.append(r['1']['value'])\n",
    "        queuedUserSubscriptions.append(r['2']['value'])\n",
    "        queuedT0Export.append(r['3']['value'])\n",
    "        queuedDataConsolidation.append(r['4']['value'])\n",
    "        queuedFunctionalTest.append(r['5']['value'])\n",
    "        queuedExpress.append(r['6']['value'])\n",
    "        queuedProductionInput.append(r['7']['value'])\n",
    "        queuedProductionOutput.append(r['8']['value'])\n",
    "        queuedStaging.append(r['9']['value'])\n",
    "\n",
    "        doneTotal.append(r['10']['value'])\n",
    "        doneUserSubscriptions.append(r['11']['value'])\n",
    "        doneT0Export.append(r['12']['value'])\n",
    "        doneDataConsolidation.append(r['13']['value'])\n",
    "        doneFunctionalTest.append(r['14']['value'])\n",
    "        doneExpress.append(r['15']['value'])\n",
    "        doneProductionInput.append(r['16']['value'])\n",
    "        doneProductionOutput.append(r['17']['value'])\n",
    "        doneStaging.append(r['18']['value'])\n",
    "\n",
    "        packetloss.append(r['19']['value'])\n",
    "        dashThroughput1h.append(r['20']['value'])\n",
    "        dashThroughput1d .append(r['21']['value'])\n",
    "        ftsThroughput1h.append(r['22']['value'])\n",
    "        ftsThroughput1d.append(r['23']['value'])\n",
    "        faxThroughput1h.append(r['24']['value'])\n",
    "        faxThroughput1d.append(r['25']['value'])                              \n",
    "\n",
    "        timestamp.append(r['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "#Some useful functions for date conversion\n",
    "\n",
    "def convertDateToDatetime(dates):\n",
    "    '''\n",
    "    Converts string dates to datetime objects\n",
    "    '''\n",
    "    datetimeDates = []\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        #Use decimal version if the date string contains a decimal point\n",
    "        if dates[i].find('.')==-1:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        datetimeDates.append(epoch)\n",
    "    return datetimeDates\n",
    "\n",
    "def convertDateToMilliEpoch(dates):\n",
    "    '''\n",
    "    Converts string dates to milliEpoch values.\n",
    "    '''\n",
    "    epochDates = []\n",
    "    zeroEpoch = datetime.utcfromtimestamp(0)    \n",
    "    for i in range(len(dates)):\n",
    "        #Use decimal version if the date string contains a decimal point\n",
    "        if dates[i].find('.')==-1:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            \n",
    "        epochDates.append((epoch - zeroEpoch).total_seconds() * 1000.0)\n",
    "    return epochDates\n",
    "        \n",
    "        \n",
    "    return datetimeDates\n",
    "\n",
    "def getSecondsDifference(epochDatesStart,epochDatesFinal):\n",
    "    diff = []\n",
    "    for i in range(len(epochDatesStart)):\n",
    "        diff.append((epochDatesFinal[i] - epochDatesStart[i]).total_seconds())\n",
    "    \n",
    "    return diff\n",
    "\n",
    "#Convenience function that converts lists with None values to arrays with zeroes in their place.\n",
    "def NoneToNum(noneList):\n",
    "    numArray = np.nan_to_num(np.array(noneList,dtype=float))\n",
    "    return numArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert string timestamps to usable quantities (each per-file, BTW)\n",
    "\n",
    "timeInQueue = getSecondsDifference(convertDateToDatetime(totalSubmittedAt),\n",
    "                                   convertDateToDatetime(totalStartedAt))\n",
    "timeTotal = getSecondsDifference(convertDateToDatetime(totalSubmittedAt),\n",
    "                                 convertDateToDatetime(totalTransferredAt))\n",
    "timeInTransfer = getSecondsDifference(convertDateToDatetime(totalStartedAt),\n",
    "                                      convertDateToDatetime(totalTransferredAt))\n",
    "\n",
    "timeInTransfer = np.array(timeInTransfer)\n",
    "timeTotal = np.array(timeTotal)\n",
    "timeInQueue = np.array(timeInQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Get times that each file is submitted at\n",
    "submittedTimes = np.array(convertDateToMilliEpoch(totalSubmittedAt))\n",
    "\n",
    "#Get indices for sorting (use these indices to sort any ddm-metrics variables)\n",
    "timeIndex = np.argsort(submittedTimes)\n",
    "\n",
    "submittedTimes = submittedTimes[timeIndex]\n",
    "\n",
    "#Sort according to monotonically increasing file-submitted timestamp\n",
    "activity = np.array(totalActivity)[timeIndex]\n",
    "filesize = np.array(totalFileSize)[timeIndex]\n",
    "timeTotal = timeTotal[timeIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to regress categorical data, the categorical variable is represented as an n-vector, where n is the number of categories for the variable. For each observation, the index corresponding to the appropriate category holds a value of 1, and the rest of the indices are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "##Convert to panda dataframe, convert from categorical to numerical\n",
    "#activity = {'activity':np.array(totalActivity)[timeIndex]}\n",
    "#df = pd.DataFrame(activity)\n",
    "#dummyActivity = np.array(pd.get_dummies(df['activity']))\n",
    "\n",
    "uniqueActivities = np.unique(totalActivity)\n",
    "num = len(uniqueActivities)\n",
    "\n",
    "vec = np.zeros(num)\n",
    "\n",
    "dummyActivity = []\n",
    "for i in range(len(totalActivity)):\n",
    "    vecNew = vec.copy()\n",
    "    vecNew[np.where(uniqueActivities == totalActivity[i])] = 1\n",
    "    dummyActivity.append(vecNew)\n",
    "dummyActivity = np.array(dummyActivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert from list to array\n",
    "timestamp = np.array(timestamp, dtype=float)\n",
    "\n",
    "#Get indices that relate the two timescales between metrics\n",
    "binIndex = np.digitize(submittedTimes,timestamp)-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Make the bin sort\n",
    "\n",
    "\n",
    "#Convert lists to arrays, and None objects to NaNs, for queued and done data. In these cases, NaNs represent the absence\n",
    "#of files in the category, rather than actual missing data as is often the case.\n",
    "queuedTotal = NoneToNum(queuedTotal)[binIndex]\n",
    "queuedUserSubscriptions = NoneToNum(queuedUserSubscriptions)[binIndex]\n",
    "queuedT0Export = NoneToNum(queuedT0Export)[binIndex]\n",
    "queuedStaging = NoneToNum(queuedStaging)[binIndex]\n",
    "queuedDataConsolidation = NoneToNum(queuedDataConsolidation)[binIndex]\n",
    "queuedFunctionalTest = NoneToNum(queuedFunctionalTest)[binIndex]\n",
    "queuedExpress = NoneToNum(queuedExpress)[binIndex]\n",
    "queuedProductionInput = NoneToNum(queuedProductionInput)[binIndex]\n",
    "queuedProductionOutput = NoneToNum(queuedProductionOutput)[binIndex]\n",
    "queuedStaging = NoneToNum(queuedStaging)[binIndex]\n",
    "\n",
    "doneTotal = NoneToNum(doneTotal)[binIndex]\n",
    "doneUserSubscriptions = NoneToNum(doneUserSubscriptions)[binIndex]\n",
    "doneT0Export = NoneToNum(doneT0Export)[binIndex]\n",
    "doneStaging = NoneToNum(doneStaging)[binIndex]\n",
    "doneDataConsolidation = NoneToNum(doneDataConsolidation)[binIndex]\n",
    "doneFunctionalTest = NoneToNum(doneFunctionalTest)[binIndex]\n",
    "doneExpress = NoneToNum(doneExpress)[binIndex]\n",
    "doneProductionInput = NoneToNum(doneProductionInput)[binIndex]\n",
    "doneProductionOutput = NoneToNum(doneProductionOutput)[binIndex]\n",
    "doneStaging = NoneToNum(doneStaging)[binIndex]\n",
    "\n",
    "#Likewise, the packet loss parameter is only taken when it rises above a certain value, so empty values can safely be\n",
    "#set to zero.\n",
    "loss = NoneToNum(packetloss)[binIndex]\n",
    "\n",
    "#Missing timestamps, however, are serious indeed. We do not want to fit these, so we will be careful to maintain the NaNs\n",
    "#until the final step a few cells down, where we will ignore any observations that contain a NaN.\n",
    "times = timestamp[binIndex]\n",
    "\n",
    "\n",
    "dashThroughput1h = NoneToNum(dashThroughput1h)[binIndex]\n",
    "dashThroughput1d = NoneToNum(dashThroughput1d)[binIndex]\n",
    "ftsThroughput1h = NoneToNum(ftsThroughput1h)[binIndex]\n",
    "ftsThroughput1d = NoneToNum(ftsThroughput1d)[binIndex]\n",
    "faxThroughput1h = NoneToNum(faxThroughput1h)[binIndex]\n",
    "faxThroughput1d = NoneToNum(faxThroughput1d)[binIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertSecsToCat(data):\n",
    "    timeCategory = np.zeros(len(data),dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        if data[i] < 600: #Less than 10 min\n",
    "            timeCategory[i] = 0\n",
    "        elif data[i] >= 600 and data[i] < 1200:#(10-20 min)\n",
    "            timeCategory[i] = 1\n",
    "        elif (data[i] >= 1200 and data[i] < 3600): #(between 20 min and 1 hour)\n",
    "            timeCategory[i] = 2\n",
    "        elif data[i] >=3600 and data[i] < 7200: #(between 1 and 2 hr)\n",
    "            timeCategory[i] = 3\n",
    "        elif data[i] >= 7200 and data[i] < 18000: #(2-5 hour)\n",
    "            timeCategory[i] = 4\n",
    "        elif data[i] >= 18000: #Greater than or equal to 5 hours\n",
    "            timeCategory[i] = 5\n",
    "    return timeCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Form input by combining various variables\n",
    "inputSample = np.squeeze(np.hstack((submittedTimes[:,None],queuedUserSubscriptions[:,None],\n",
    "                                    queuedT0Export[:,None],queuedStaging[:,None],\n",
    "                                    queuedDataConsolidation[:,None],queuedFunctionalTest[:,None],\n",
    "                                    queuedExpress[:,None],queuedProductionInput[:,None],\n",
    "                                    queuedProductionOutput[:,None],queuedStaging[:,None],\n",
    "                                    doneUserSubscriptions[:,None],\n",
    "                                    doneT0Export[:,None],doneStaging[:,None],\n",
    "                                    doneDataConsolidation[:,None],doneFunctionalTest[:,None],\n",
    "                                    doneExpress[:,None],doneProductionInput[:,None],\n",
    "                                    doneProductionOutput[:,None],doneStaging[:,None],filesize[:,None],\n",
    "                                    dashThroughput1h[:,None],ftsThroughput1h[:,None],\n",
    "                                    dummyActivity)))\n",
    "outputSample = timeTotal\n",
    "\n",
    "#Find rows of data that contain at least one missing value\n",
    "missing = np.isnan(np.sum(inputSample,axis=1))\n",
    "\n",
    "reducedInput = inputSample[~missing]\n",
    "reducedOutput = outputSample[~missing]\n",
    "activity = activity[~missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tic = time.time()\n",
    "sp = int(round(0.8*len(reducedOutput)))\n",
    "\n",
    "X_train = reducedInput[:sp,:]\n",
    "Y_train = reducedOutput[:sp]\n",
    "X_test = reducedInput[sp:]\n",
    "Y_test = reducedOutput[sp:]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "Xtrain_ = poly.fit_transform(X_train[:,1:])\n",
    "Xtest_ = poly.fit_transform(X_test[:,1:])\n",
    "\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(Xtrain_,Y_train)\n",
    "\n",
    "Y_predict = clf.predict(Xtest_)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"This took %f seconds\"%(toc-tic))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(X_test[:,0],Y_predict)\n",
    "plt.plot(X_test[:,0],Y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (LinearRegression,BayesianRidge,Lars,Lasso,TheilSenRegressor,\n",
    "                                  PassiveAggressiveRegressor,RANSACRegressor,SGDRegressor)\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import (GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,\n",
    "                              ExtraTreesRegressor)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.dates as dt\n",
    "\n",
    "#First 3/4 of data to train on, and test on the last 1/4\n",
    "sp = int(round(0.75*len(reducedOutput)))\n",
    "\n",
    "X_train = reducedInput[:sp,:]\n",
    "Y_train = reducedOutput[:sp]\n",
    "X_test = reducedInput[sp:]\n",
    "Y_test = reducedOutput[sp:]\n",
    "\n",
    "model = BaggingRegressor()\n",
    "Y_predict = model.fit(X_train[:,1:],Y_train).predict(X_test[:,1:])\n",
    "\n",
    "dateTime = dt.epoch2num(X_test[:,0]/1000.)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(X_test[:,0],Y_test,'r.',label='Test data')\n",
    "plt.plot(X_test[:,0],Y_predict,'b.',label='Model prediction')\n",
    "#plt.errorbar(X_test[:,0],Y_predict,yerr=np.abs(Y_predict-Y_test),linestyle='None',label='predicted')\n",
    "\n",
    "ax = plt.gca()\n",
    "#Tedious date/time formatting\n",
    "xfmt = dt.DateFormatter('%d-%m-%Y %Hhr')\n",
    "ax.xaxis.set_major_formatter(xfmt)\n",
    "ax.xaxis_date()\n",
    "\n",
    "start, end = ax.get_xlim()\n",
    "\n",
    "plt.xticks(np.arange(start, end,10000),rotation= 90 )\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"loss\": ['ls','lad'],\n",
    "              \"learning_rate\": [0.1,0.2,0.3,0.5],\n",
    "              \"n_estimators\": [10,100,],\n",
    "              \"max_depth\": [1,3,4,5],\n",
    "              \"min_samples_leaf\": [1, 3, 10]}\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid)\n",
    "start = time.time()\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time.time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for u in range(len(uniqueActivities)):\n",
    "    plt.subplot(3,3,u+1,sharex=axd)\n",
    "    currentIndex = np.array(activity[sp:])==uniqueActivities[u]\n",
    "    plt.plot(X_test[currentIndex,0],Y_test[currentIndex],'r.')\n",
    "    plt.plot(X_test[currentIndex,0],Y_predict[currentIndex],'b.')\n",
    "    plt.title(uniqueActivities[u])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error is reported as the RMSE per TTC category of a particular activity, taken with respect to its corresponding actual result. This is carried out after the data has been converted from continuous to categorical. Since categories are integers that increase as the corresponding time interval of the category increase, the RMSE can still be found and gives a reasonable estimator of the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get performance statistics, broken down according to activity\n",
    "\n",
    "rmseTotal = []\n",
    "\n",
    "#For each activity, get RMSE of the predicted values\n",
    "yPredictCat = convertSecsToCat(Y_predict)\n",
    "yTestCat = convertSecsToCat(Y_test)\n",
    "\n",
    "predictSize = np.zeros(len(uniqueActivities))\n",
    "for i in range(len(uniqueActivities)):\n",
    "    currentIndex = np.array(activity[sp:])==uniqueActivities[i]\n",
    "    currentActivityTest = yTestCat[currentIndex]\n",
    "    currentActivityPredict = yPredictCat[currentIndex]\n",
    "    predictSize[i] = len(currentActivityPredict)\n",
    "    rmse = np.zeros(6)\n",
    "    \n",
    "    #For each TTC category, find the predicted files with that TTC index and compared to test data, taking the RMS\n",
    "    for j in range(6):\n",
    "        index = (currentActivityPredict==j)\n",
    "        rmse[j] = np.sqrt(np.mean((currentActivityPredict[index] - currentActivityTest[index])**2.))\n",
    "    rmseTotal.append(rmse)\n",
    "\n",
    "\n",
    "#Get total RMSE as well, without separating out the activities \n",
    "rmse = np.zeros(6)\n",
    "for j in range(6):\n",
    "    index = (yPredictCat==j)\n",
    "    rmse[j] = np.sqrt(np.mean((yPredictCat[index] - yTestCat[index])**2.))\n",
    "rmseTotal.append(rmse)\n",
    "\n",
    "#x-labels for bar plot\n",
    "timeStr = ['0-10 m','10-20 m','20-60 m', '1-2 h','2-5 h', '5+ h']\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax1 = plt.subplot(4,3,len(uniqueActivities)+1)\n",
    "plt.bar(range(6),rmseTotal[len(uniqueActivities)],align='center')\n",
    "for u,v in enumerate(rmseTotal[-1]):\n",
    "    if np.logical_not(np.isnan(v)):\n",
    "        ax1.text(u-0.25, v+0.1, str(np.round(v,2)), color='blue', fontweight='bold')\n",
    "plt.xticks(range(6),timeStr)\n",
    "plt.title('All Activities',y=1.08)\n",
    "\n",
    "for i in range(len(uniqueActivities)):\n",
    "    plt.subplot(4,3,i+1,sharex=ax1,sharey=ax1)\n",
    "    \n",
    "    #Place a \"NO DATA\" label on plot if there is no info for that activity\n",
    "    if np.sum(np.isnan(rmseTotal[i])) == 6:\n",
    "        plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "        plt.annotate('NO DATA', xy=(2.5, 0.5),size=20,ha=\"center\", va=\"center\")\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        plt.bar(range(6),rmseTotal[i],width=0.9,align='center')\n",
    "        plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        for u,v in enumerate(rmseTotal[i]):\n",
    "            if np.logical_not(np.isnan(v)):\n",
    "                ax.text(u-0.25, v+0.1, str(np.round(v,2)), color='blue', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Error per activity, according to time',size=30,y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binNorm = 1/(600)*np.array([10,10,40,60,180,300])\n",
    "rmseTotalNormed = rmseTotal*binNorm\n",
    "#x-labels for bar plot\n",
    "timeStr = ['0-10 m','10-20 m','20-60 m', '1-2 h','2-5 h', '5+ h']\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax1 = plt.subplot(4,3,len(uniqueActivities)+1)\n",
    "plt.bar(range(6),rmseTotalNormed[len(uniqueActivities)],align='center')\n",
    "for u,v in enumerate(rmseTotalNormed[-1]):\n",
    "    if np.logical_not(np.isnan(v)):\n",
    "        ax1.text(u-0.25, v+0.1, str(np.round(v,2)), color='blue', fontweight='bold')\n",
    "plt.xticks(range(6),timeStr)\n",
    "plt.title('All Activities',y=1.08)\n",
    "\n",
    "for i in range(len(uniqueActivities)):\n",
    "    plt.subplot(4,3,i+1,sharex=ax1,sharey=ax1)\n",
    "    \n",
    "    #Place a \"NO DATA\" label on plot if there is no info for that activity\n",
    "    if np.sum(np.isnan(rmseTotal[i])) == 6:\n",
    "        plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "        plt.annotate('NO DATA', xy=(2.5, 0.5),size=20,ha=\"center\", va=\"center\")\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        plt.bar(range(6),rmseTotalNormed[i],width=0.9,align='center')\n",
    "        plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        for u,v in enumerate(rmseTotalNormed[i]):\n",
    "            if np.logical_not(np.isnan(v)):\n",
    "                ax.text(u-0.25, v+0.1, str(np.round(v,2)), color='blue', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Normalized error per activity, according to time',size=30,y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get performance statistics, broken down according to activity\n",
    "\n",
    "rmseTotal = []\n",
    "uniqueActivites = np.array(['Data Brokering', 'Data Consolidation', 'Express',\n",
    "       'Functional Test', 'Production Input', 'Production Output',\n",
    "       'Staging', 'T0 Export', 'User Subscriptions'])\n",
    "#For each activity, get RMSE of the predicted values\n",
    "yPredictCat = convertSecsToSpacedCat(Y_predict)\n",
    "yTestCat = convertSecsToSpacedCat(Y_test)\n",
    "\n",
    "predictSize = np.zeros(len(uniqueActivities))\n",
    "numInBin = np.zeros((len(uniqueActivities)+1,25))\n",
    "\n",
    "binCenters = np.arange(10,25*20,20)\n",
    "\n",
    "for i in range(len(uniqueActivities)+1):\n",
    "\n",
    "    rmse = np.zeros(25)\n",
    "    \n",
    "    #For each TTC category, find the predicted files with that TTC index and compared to test data, taking the RMS\n",
    "    for j in range(25):\n",
    "        if i < len(uniqueActivities):\n",
    "            currentIndex = np.array(activity[sp:])==uniqueActivities[i]\n",
    "            currentActivityTest = yTestCat[currentIndex]\n",
    "            currentActivityPredict = yPredictCat[currentIndex]\n",
    "            predictSize[i] = len(currentActivityPredict)\n",
    "            \n",
    "            index = (currentActivityPredict==binCenters[j])\n",
    "            rmse[j] = np.sqrt(np.mean((currentActivityPredict[index] - currentActivityTest[index])**2.))\n",
    "            numInBin[i,j] = len(currentActivityPredict[index])\n",
    "        #Get total RMSE as well, without separating out the activities \n",
    "        elif i == len(uniqueActivities):\n",
    "            index = (yPredictCat==binCenters[j])\n",
    "            rmse[j] = np.sqrt(np.mean((yPredictCat[index] - yTestCat[index])**2.))   \n",
    "            numInBin[i,j] = len(yPredictCat[index])\n",
    "    rmseTotal.append(rmse)\n",
    "\n",
    "\n",
    "#x-labels for bar plot\n",
    "timeStr = np.arange(10,26*20,20)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax1 = plt.subplot(2,2,len(uniqueActivities))\n",
    "plt.bar(range(25),rmseTotal[len(uniqueActivities)],align='center')\n",
    "\n",
    "for j in range(25):\n",
    "    if np.logical_not(np.isnan(rmseTotal[-1][j])):\n",
    "        ax1.text(j, rmseTotal[-1][j]+.25, str(int(numInBin[-1,j])),size = 12,color='blue',\n",
    "                fontweight='bold',ha='center')\n",
    "\n",
    "\n",
    "plt.xticks(range(25),timeStr)\n",
    "plt.title('All Activities',y=1.08)\n",
    "\n",
    "for i in range(len(uniqueActivities)):\n",
    "    plt.subplot(2,2,i+1,sharex=ax1,sharey=ax1)\n",
    "    \n",
    "    #Place a \"NO DATA\" label on plot if there is no info for that activity\n",
    "    if np.sum(np.isnan(rmseTotal[i])) == 25:\n",
    "        plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "        plt.annotate('NO DATA', xy=(2.5, 0.5),size=20,ha=\"center\", va=\"center\")\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        plt.bar(range(25),rmseTotal[i],width=0.9,align='center')\n",
    "        plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "        plt.xticks(range(25),timeStr)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        for j in range(25):\n",
    "            if np.logical_not(np.isnan(rmseTotal[i][j])):\n",
    "                ax.text(j, rmseTotal[i][j]+.25, str(int(numInBin[i,j])),size = 12,color='blue',\n",
    "                        fontweight='bold',ha='center')\n",
    "\n",
    "plt.suptitle('Error per activity, according to time',size=30,y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get performance statistics, broken down according to activity\n",
    "uniqueActivities = ['Data Consolidation','Production Input','T0 Export']\n",
    "\n",
    "rmseTotal = []\n",
    "\n",
    "\n",
    "#Initialize bin edges, ranging from 0 to 10 hours in half hour intervals, with the last bin between 10 and 24 hours\n",
    "binEdges = np.append(np.linspace(0,10,num=21),24)*(60.**2)\n",
    "\n",
    "#Initialize for putting number of predicted objects in each time bin\n",
    "predictSize = np.zeros(len(uniqueActivities))\n",
    "\n",
    "binNumber = len(binEdges)-1\n",
    "numInBin = np.zeros((len(uniqueActivities)+1,binNumber))\n",
    "\n",
    "\n",
    "for i in range(len(uniqueActivities)+1):\n",
    "\n",
    "    rmse = np.zeros(binNumber)\n",
    "    \n",
    "    #For each TTC category, find the predicted files with that TTC index and compared to test data, taking the RMS\n",
    "    for j in range(binNumber):\n",
    "        \n",
    "        if i < len(uniqueActivities):\n",
    "            \n",
    "            #Get indices for a particular activity\n",
    "            currentIndex = np.array(activity[sp:])==uniqueActivities[i]\n",
    "            currentActivityTest = Y_test[currentIndex]\n",
    "            currentActivityPredict = Y_predict[currentIndex]\n",
    "            predictSize[i] = len(currentActivityPredict)\n",
    "            \n",
    "            #Get data in that activity that corresponds to a value within a bin\n",
    "            index = np.where((currentActivityPredict >= binEdges[j])&(currentActivityPredict < binEdges[j+1]))\n",
    "            rmse[j] = np.sqrt(np.mean((currentActivityPredict[index] - currentActivityTest[index])**2.))\n",
    "            numInBin[i,j] = len(currentActivityPredict[index])\n",
    "            \n",
    "        #Get total RMSE as well, without separating out the activities \n",
    "        elif i == len(uniqueActivities):\n",
    "            index = np.where((Y_predict >= binEdges[j])&(Y_predict < binEdges[j+1]))\n",
    "            rmse[j] = np.sqrt(np.mean((Y_predict[index] - Y_test[index])**2.))   \n",
    "            numInBin[i,j] = len(Y_predict[index])\n",
    "    rmseTotal.append(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,40))\n",
    "\n",
    "ax1 = plt.subplot(5,2,len(uniqueActivities)+1)\n",
    "\n",
    "binCenters = binEdges[:len(binEdges)-1]\n",
    "plt.bar(range(binNumber),rmseTotal[len(uniqueActivities)],align='center')\n",
    "\n",
    "for j in range(binNumber):\n",
    "    if np.logical_not(np.isnan(rmseTotal[-1][j])):\n",
    "        ax1.text(j, rmseTotal[-1][j]+.25, str(int(numInBin[-1,j])),size = 12,color='blue',\n",
    "                fontweight='bold',ha='center')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('All Activities',y=1.08)\n",
    "\n",
    "for i in range(len(uniqueActivities)):\n",
    "    plt.subplot(5,2,i+1,sharex=ax1,sharey=ax1)\n",
    "    \n",
    "\n",
    "    plt.bar(binCenters,rmseTotal[i],width=0.9,align='center')\n",
    "    plt.title('%s, # of objects = %i'%(str(uniqueActivities[i]), predictSize[i]),y=1.08)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for j in range(binNumber):\n",
    "        if np.logical_not(np.isnan(rmseTotal[i][j])):\n",
    "            ax.text(range(j), rmseTotal[i][j]+.25, str(int(numInBin[i,j])),size = 12,color='blue',\n",
    "                    fontweight='bold',ha='center')\n",
    "\n",
    "plt.suptitle('Error per activity, according to time',size=30,y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "#Get performance statistics, broken down according to activity\n",
    "uniqueActivities = ['Data Consolidation','Production Input','T0 Export']\n",
    "rmseTotal = []\n",
    "\n",
    "#For each activity, get RMSE of the predicted values\n",
    "yPredictCat = convertSecsToSpacedCat(Y_predict)\n",
    "yTestCat = convertSecsToSpacedCat(Y_test)\n",
    "\n",
    "numInBin = np.zeros((len(uniqueActivities)+1,25))\n",
    "\n",
    "binCenters = np.arange(10,25*20,20)\n",
    "\n",
    "for i in range(len(uniqueActivities)+1):\n",
    "\n",
    "    rmse = np.zeros(25)\n",
    "    \n",
    "    #For each TTC category, find the predicted files with that TTC index and compared to test data, taking the RMS\n",
    "    for j in range(25):\n",
    "        if i < len(uniqueActivities):\n",
    "            currentIndex = np.array(activity[sp:])==uniqueActivities[i]\n",
    "            currentActivityTest = yTestCat[currentIndex]\n",
    "            currentActivityPredict = yPredictCat[currentIndex]\n",
    "            \n",
    "            index = (currentActivityPredict==binCenters[j])\n",
    "            rmse[j] = np.sqrt(np.mean((currentActivityPredict[index] - currentActivityTest[index])**2.))\n",
    "            numInBin[i,j] = len(currentActivityPredict[index])\n",
    "        #Get total RMSE as well, without separating out the activities \n",
    "        elif i == len(uniqueActivities):\n",
    "            index = (yPredictCat==binCenters[j])\n",
    "            rmse[j] = np.sqrt(np.mean((yPredictCat[index] - yTestCat[index])**2.))   \n",
    "            numInBin[i,j] = len(yPredictCat[index])\n",
    "    rmseTotal.append(rmse)\n",
    "\n",
    "\n",
    "\n",
    "#x-labels for bar plot\n",
    "timeStr = range(25)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "color=iter(cm.jet(np.linspace(0,1,len(uniqueActivities))))\n",
    "\n",
    "for i in range(len(uniqueActivities)):\n",
    "    c=next(color)\n",
    "    plt.scatter(np.arange(10,25*20,20),rmseTotal[i],s=(numInBin[i,:]*0.2),c=c)\n",
    "    plt.plot(np.arange(10,25*20,20),rmseTotal[i],c=c,linewidth=3,label = uniqueActivities[i])\n",
    "ax = plt.gca()\n",
    "#ax.set_yscale('log')    \n",
    "#plt.xlim(0,500)\n",
    "#plt.ylim(0,np.nanmax(rmseTotal))\n",
    "plt.xticks(np.arange(0,500,20))\n",
    "#plt.yticks([0.01,0.1,1,10,100])\n",
    "plt.grid()\n",
    "plt.xlabel('Time-to-complete [minutes]')\n",
    "plt.ylabel('RMS error [minutes]')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((yPredictCat-yTestCat)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,3.5,7])\n",
    "b = np.array([0,3,6,9])\n",
    "np.where((x>b[0])&(x < b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier,\n",
    "                              AdaBoostClassifier,ExtraTreesClassifier)\n",
    "\n",
    "#Form input by combining various variables\n",
    "inputSample = np.squeeze(np.hstack((submittedTimes[:,None],queuedUserSubscriptions[:,None],\n",
    "                                    queuedT0Export[:,None],queuedStaging[:,None],\n",
    "                                    queuedDataConsolidation[:,None],queuedFunctionalTest[:,None],\n",
    "                                    queuedExpress[:,None],queuedProductionInput[:,None],\n",
    "                                    queuedProductionOutput[:,None],queuedStaging[:,None],\n",
    "                                    doneUserSubscriptions[:,None],\n",
    "                                    doneT0Export[:,None],doneStaging[:,None],\n",
    "                                    doneDataConsolidation[:,None],doneFunctionalTest[:,None],\n",
    "                                    doneExpress[:,None],doneProductionInput[:,None],\n",
    "                                    doneProductionOutput[:,None],doneStaging[:,None],filesize[:,None],\n",
    "                                    dashThroughput1h[:,None],dummyActivity)))\n",
    "outputSample = timeTotal\n",
    "\n",
    "#Find rows of data that contain at least one missing value\n",
    "missing = np.isnan(np.sum(inputSample,axis=1))\n",
    "\n",
    "reducedInput = inputSample[~missing]\n",
    "reducedOutput = outputSample[~missing]\n",
    "\n",
    "sp = int(round(0.8*len(reducedOutput)))\n",
    "\n",
    "X_train = reducedInput[:sp,:]\n",
    "Y_train = convertSecsToCat(reducedOutput[:sp])\n",
    "X_test = reducedInput[sp:]\n",
    "Y_test = convertSecsToCat(reducedOutput[sp:])\n",
    "\n",
    "model1 = ExtraTreesClassifier()\n",
    "\n",
    "results = model1.fit(X_train[:,1:],Y_train)\n",
    "\n",
    "Y_predict = results.predict(X_test[:,1:])\n",
    "Y_proba = results.predict_proba(X_test[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get performance statistics, broken down according to activity\n",
    "\n",
    "rmseTotal = []\n",
    "\n",
    "#For each activity, get RMSE of the predicted values\n",
    "yPredictCat = Y_predict\n",
    "yTestCat = Y_test\n",
    "for i in range(len(uniqueActivities)):\n",
    "    currentIndex = np.array(activity[sp:])==uniqueActivities[i]\n",
    "    currentActivityTest = yTestCat[currentIndex]\n",
    "    currentActivityPredict = yPredictCat[currentIndex]\n",
    "    rmse = np.zeros(6)\n",
    "    for j in range(6):\n",
    "        index = (currentActivityPredict==j)\n",
    "        rmse[j] = np.sqrt(np.mean((currentActivityPredict[index] - currentActivityTest[index])**2))\n",
    "    rmseTotal.append(rmse)\n",
    "\n",
    "\n",
    "#Get total RMSE as well, without separating out the activities \n",
    "rmse = np.zeros(6)\n",
    "for j in range(6):\n",
    "    index = (yPredictCat==j)\n",
    "    rmse[j] = np.sqrt(np.mean((yPredictCat[index] - yTestCat[index])**2))\n",
    "rmseTotal.append(rmse)\n",
    "\n",
    "#Bar plot x-labels\n",
    "timeStr = ['0-10 m','10-20 m','20-60 m', '1-2 h','2-5 h', '5+ h']\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax1 = plt.subplot(4,3,len(uniqueActivities)+1)\n",
    "plt.bar(range(6),rmseTotal[len(uniqueActivities)],align='center')\n",
    "plt.xticks(range(6),timeStr)\n",
    "plt.title('All Activities')\n",
    "\n",
    "for i in range(len(uniqueActivities)):\n",
    "    plt.subplot(4,3,i+1,sharex=ax1,sharey=ax1)\n",
    "    \n",
    "    #Place a \"NO DATA\" label on plot if there is no info for that activity\n",
    "    if np.sum(np.isnan(rmseTotal[i])) == 6:\n",
    "        plt.title(str(uniqueActivities[i]))\n",
    "        plt.annotate('NO DATA', xy=(2.5, 0.5),size=20,ha=\"center\", va=\"center\")\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        plt.bar(range(6),rmseTotal[i],width=0.9,align='center')\n",
    "        plt.title(str(uniqueActivities[i]))\n",
    "        plt.xticks(range(6),timeStr)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        for u,v in enumerate(rmseTotal[i]):\n",
    "            if np.logical_not(np.isnan(v)):\n",
    "                ax.text(u-0.25, v+0.1, str(np.round(v,2)), color='blue', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Error per activity, according to time',size=30,y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
