{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These lines set up inline plotting, and apply a standard size\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "from elasticsearch import Elasticsearch, exceptions as es_exceptions\n",
    "from elasticsearch.helpers import scan\n",
    "matplotlib.rc('font', **{'size': 15})\n",
    "\n",
    "\n",
    "src = 'AGLT2'\n",
    "src = 'CERN-PROD'\n",
    "dst = 'BNL-ATLAS'\n",
    "\n",
    "\n",
    "#Create instance of ElasticSearch using the Atlas host server\n",
    "es1 = Elasticsearch(hosts=[{'host':'atlas-kibana-2.mwt2.org', 'port':9200}],timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = '''{\n",
    "    \n",
    "  \"size\": 10000,\n",
    "  \"sort\": [\n",
    "    {\n",
    "      \"@timestamp\": {\n",
    "        \"order\": \"random\",\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"query\": {\n",
    "    \"filtered\": {\n",
    "      \"query\": {\n",
    "        \"query_string\": {\n",
    "          \"query\": \"payload.src-rse:CERN* AND payload.dst-rse:BNL* AND event_type:transfer-done\",\n",
    "          \"analyze_wildcard\": True,\n",
    "          \"lowercase_expanded_terms\": False\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                  \"gte\": %i,\n",
    "                  \"lte\": %i,\n",
    "                  \"format\": \"epoch_millis\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          ],\n",
    "          \"must_not\": []\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"fielddata_fields\": [\n",
    "    \"@timestamp\",\"duration\"\n",
    "  ]\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import time\n",
    "\n",
    "#Initialize convenient time intervals in milliseconds\n",
    "currentMilliTime = int(round(time.time()*1000))\n",
    "milliWeek = 7*24*60*60*1000\n",
    "milliTwoWeek = 2*7*24*60*60*1000\n",
    "milliSixHour = 12*60*60*1000\n",
    "milliMonth = 30*24*60*60*1000\n",
    "milliDay = 24*60*60*1000\n",
    "milliHalfDay = 12*60*60*1000\n",
    "\n",
    "milliHour = 60*60*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n",
      "Hey!\n"
     ]
    }
   ],
   "source": [
    "#Get files day interval (stagger a little from current time to get more interesting data)\n",
    "start = currentMilliTime - milliTwoWeek\n",
    "end = currentMilliTime\n",
    "\n",
    "#Initialize lists\n",
    "fileSize = []\n",
    "activity = []\n",
    "startedAt = []\n",
    "submittedAt = []\n",
    "transferredAt = []\n",
    "duration = []\n",
    "fileTimestamp = []\n",
    "\n",
    "startPart = start\n",
    "endPart = start\n",
    "hourSeg = 24*7*2\n",
    "\n",
    "weekTimestamp = []\n",
    "weekFileSize = []\n",
    "weekActivity = []\n",
    "weekStartedAt = []\n",
    "weekSubmittedAt = []\n",
    "weekTransferredAt = []\n",
    "\n",
    "#Step through day, getting all data in range\n",
    "for i in range(hourSeg):\n",
    "    endPart += milliHour\n",
    "    response = es1.search(index=\"rucio-events*\", body=ast.literal_eval(query1%(startPart,endPart)), request_timeout=600)\n",
    "    startPart = endPart + 1\n",
    "    \n",
    "    #Get important fields\n",
    "    for r in response['hits']['hits']:\n",
    "        fileTimestamp.append(r['fields']['@timestamp'])\n",
    "        fileSize.append(r['_source']['payload']['file-size'])\n",
    "        activity.append(r['_source']['payload']['activity'])\n",
    "        startedAt.append(r['_source']['payload']['started_at'])\n",
    "        submittedAt.append(r['_source']['payload']['submitted_at'])\n",
    "        transferredAt.append(r['_source']['payload']['transferred_at'])\n",
    "    \n",
    "    \n",
    "    #Get random indices in range of length of data vectors\n",
    "    ind = np.random.choice(len(fileSize),size=50,replace=False)\n",
    "    np.random.seed()\n",
    "    \n",
    "    #Use this to obtain random sample of data, and save these to disk\n",
    "    weekTimestamp += [ fileTimestamp[i] for i in ind]\n",
    "    weekFileSize += [ fileSize[i] for i in ind]\n",
    "    weekActivity += [ activity[i] for i in ind]\n",
    "    weekStartedAt += [ startedAt[i] for i in ind]\n",
    "    weekSubmittedAt += [ submittedAt[i] for i in ind]\n",
    "    weekTransferredAt += [ transferredAt[i] for i in ind]\n",
    "\n",
    "    if i%24 == 0:\n",
    "        print('Hey!')\n",
    "\n",
    "    \n",
    "#fileSize = np.array(fileSize)\n",
    "#activity = np.array(activity)\n",
    "#startedAt = np.array(startedAt)\n",
    "#submittedAt = np.array(submittedAt)\n",
    "#transferredAt = np.array(transferredAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_ddm_data(es,src,dst,start,end,agg,field,timestamp=False):\n",
    "    queryTemp = '''{\n",
    "      \"size\": 0,\n",
    "      \"query\": {\n",
    "        \"filtered\": {\n",
    "          \"query\": {\n",
    "            \"query_string\": {\n",
    "              \"query\": \"src:%s AND dst:%s\",\n",
    "              \"analyze_wildcard\": True\n",
    "            }\n",
    "          },\n",
    "          \"filter\": {\n",
    "            \"bool\": {\n",
    "              \"must\": [\n",
    "                {\n",
    "                  \"range\": {\n",
    "                    \"timestamp\": {\n",
    "                      \"gte\": %i,\n",
    "                      \"lte\": %i,\n",
    "                      \"format\": \"epoch_millis\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ],\n",
    "              \"must_not\": []\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"2\": {\n",
    "          \"date_histogram\": {\n",
    "            \"field\": \"timestamp\",\n",
    "            \"interval\": \"1h\",\n",
    "            \"time_zone\": \"Europe/Berlin\",\n",
    "            \"min_doc_count\": 1,\n",
    "            \"extended_bounds\": {\n",
    "              \"min\": %i,\n",
    "              \"max\": %i\n",
    "            }\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"1\": {\n",
    "              \"%s\": {\n",
    "                \"field\": \"%s\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }'''\n",
    "\n",
    "    query = ast.literal_eval(queryTemp%(src,dst,start,end,start,end,agg,field))\n",
    "    response = es.search(index=\"atlas_ddm-metrics*\", body=query, request_timeout=600)\n",
    "    \n",
    "    data = []\n",
    "    if timestamp == False:\n",
    "        for r in response['aggregations']['2']['buckets']:\n",
    "            data.append(r['1']['value'])\n",
    "        return np.array(data,dtype=np.float)\n",
    "    \n",
    "    elif timestamp == True:\n",
    "        timestamp = []\n",
    "        for r in response['aggregations']['2']['buckets']:\n",
    "            data.append(r['1']['value'])\n",
    "            timestamp.append(r['key'])\n",
    "        return np.array(data,dtype=np.float),np.array(timestamp,dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished aggregating in 78.403914 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "#start and end left over from previous definition\n",
    "#Get fields from another host\n",
    "\n",
    "es2 = Elasticsearch(hosts=[{'host':'es-atlas.cern.ch', 'port':9202}],\n",
    "                   http_auth=('es-atlas', 'v~$&<J8/cG9]*eQ@'),\n",
    "                   timeout=60)\n",
    "\n",
    "agg = 'avg'\n",
    "\n",
    "#Get ddm-metrics data\n",
    "queuedTotal,timestamp = get_ddm_data(es2,src,dst,start,end,agg,'queued-total',timestamp=True)\n",
    "queuedUserSubscriptions = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-User_Subscriptions')\n",
    "queuedT0Export = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-T0_Export')\n",
    "queuedStaging = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Staging')\n",
    "queuedDataConsolidation = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Data_Consolidation')\n",
    "queuedFunctionalTest = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Functional_Test')\n",
    "queuedExpress = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Express')\n",
    "queuedProductionInput = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Production_Input')\n",
    "queuedProductionOutput = get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Production_Output')\n",
    "queuedStaging= get_ddm_data(es2,src,dst,start,end,agg,\n",
    "                                                 'queued-Staging')\n",
    "\n",
    "\n",
    "packetloss = get_ddm_data(es2,src,dst,start,end,agg,'packetloss')\n",
    "dashThroughput1h = get_ddm_data(es2,src,dst,start,end,agg,'mbps-dashb-1h')\n",
    "dashThroughput1d = get_ddm_data(es2,src,dst,start,end,agg,'mbps-dashb-1d')\n",
    "ftsThroughput1h = get_ddm_data(es2,src,dst,start,end,agg,'mbps-fts-1h')\n",
    "ftsThroughput1d = get_ddm_data(es2,src,dst,start,end,agg,'mbps-fts-1d')\n",
    "faxThroughput1h = get_ddm_data(es2,src,dst,start,end,agg,'mbps-fax-1h')\n",
    "faxThroughput1d = get_ddm_data(es2,src,dst,start,end,agg,'mbps-fax-1d')\n",
    "doneTotal1h = get_ddm_data(es2,src,dst,start,end,agg,'done-total-1h')\n",
    "\n",
    "\n",
    "print('Finished aggregating in %f seconds'%(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "#Some useful functions for date conversion\n",
    "\n",
    "def convertDateToDatetime(dates):\n",
    "    '''\n",
    "    Converts string dates to datetime objects\n",
    "    '''\n",
    "    datetimeDates = []\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        #Use decimal version if the date string contains a decimal point\n",
    "        if dates[i].find('.')==-1:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        datetimeDates.append(epoch)\n",
    "    return datetimeDates\n",
    "\n",
    "def convertDateToMilliEpoch(dates):\n",
    "    '''\n",
    "    Converts string dates to milliEpoch values.\n",
    "    '''\n",
    "    epochDates = []\n",
    "    zeroEpoch = datetime.utcfromtimestamp(0)    \n",
    "    for i in range(len(dates)):\n",
    "        #Use decimal version if the date string contains a decimal point\n",
    "        if dates[i].find('.')==-1:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            epoch = datetime.strptime(dates[i], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            \n",
    "        epochDates.append((epoch - zeroEpoch).total_seconds() * 1000.0)\n",
    "    return epochDates\n",
    "        \n",
    "        \n",
    "    return datetimeDates\n",
    "\n",
    "def getSecondsDifference(epochDatesStart,epochDatesFinal):\n",
    "    diff = []\n",
    "    for i in range(len(epochDatesStart)):\n",
    "        diff.append((epochDatesFinal[i] - epochDatesStart[i]).total_seconds())\n",
    "    \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert string timestamps to usable quantities (each per-file, BTW)\n",
    "\n",
    "timeInQueue = getSecondsDifference(convertDateToDatetime(weekSubmittedAt),\n",
    "                                   convertDateToDatetime(weekStartedAt))\n",
    "timeTotal = getSecondsDifference(convertDateToDatetime(weekSubmittedAt),\n",
    "                                 convertDateToDatetime(weekTransferredAt))\n",
    "timeInTransfer = getSecondsDifference(convertDateToDatetime(weekStartedAt),\n",
    "                                      convertDateToDatetime(weekTransferredAt))\n",
    "\n",
    "timeInTransfer = np.array(timeInTransfer)\n",
    "timeTotal = np.array(timeTotal)\n",
    "timeInQueue = np.array(timeInQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get times that each file is submitted at\n",
    "\n",
    "submittedTimes = np.array(convertDateToMilliEpoch(weekSubmittedAt))\n",
    "\n",
    "#Get indices for sorting (use these indices to sort any ddm-metrics variables)\n",
    "timeIndex = np.argsort(submittedTimes)\n",
    "submittedTimes = submittedTimes[timeIndex]\n",
    "\n",
    "#Sort according to monotonically increasing file-submitted timestamp\n",
    "filesize = np.array(weekFileSize)[timeIndex]\n",
    "activity = {'activity':np.array(weekActivity)[timeIndex]}\n",
    "timeTotal = timeTotal[timeIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Convert to panda dataframe, convert categorical, and convert back\n",
    "df = pd.DataFrame(activity)\n",
    "dummyActivity = np.array(pd.get_dummies(df['activity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get indices that relate the two timescales between metrics\n",
    "binIndex = np.digitize(submittedTimes,timestamp)-1\n",
    "\n",
    "#Make the bin sort\n",
    "#Convert zeros to NaNs for all the queued datasets\n",
    "queuedTotal = np.nan_to_num(queuedTotal)[binIndex]\n",
    "queuedUserSubscriptions = np.nan_to_num(queuedUserSubscriptions)[binIndex]\n",
    "queuedT0Export = np.nan_to_num(queuedT0Export)[binIndex]\n",
    "queuedStaging = np.nan_to_num(queuedStaging)[binIndex]\n",
    "queuedDataConsolidation = np.nan_to_num(queuedDataConsolidation)[binIndex]\n",
    "queuedFunctionalTest = np.nan_to_num(queuedFunctionalTest)[binIndex]\n",
    "queuedExpress = np.nan_to_num(queuedExpress)[binIndex]\n",
    "queuedProductionInput = np.nan_to_num(queuedProductionInput)[binIndex]\n",
    "queuedProductionOutput = np.nan_to_num(queuedProductionOutput)[binIndex]\n",
    "queuedStaging = np.nan_to_num(queuedStaging)[binIndex]\n",
    "\n",
    "loss = packetloss[binIndex]\n",
    "times = timestamp[binIndex]\n",
    "dash_through1h = dashThroughput1h[binIndex]\n",
    "dash_through1d = dashThroughput1d[binIndex]\n",
    "fts_through1d = ftsThroughput1d[binIndex]\n",
    "fts_through1h = ftsThroughput1h[binIndex]\n",
    "done_total1h = doneTotal1h[binIndex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputSample = np.squeeze(np.hstack((submittedTimes[:,None],queuedUserSubscriptions[:,None],\n",
    "                                    queuedT0Export[:,None],queuedStaging[:,None],queuedTotal[:,None],\n",
    "                                    queuedDataConsolidation[:,None],queuedFunctionalTest[:,None],\n",
    "                                    queuedExpress[:,None],queuedProductionInput[:,None],\n",
    "                                    queuedProductionOutput[:,None],queuedStaging[:,None],\n",
    "                                    done_total1h[:,None],filesize[:,None],\n",
    "                                    dash_through1h[:,None],dummyActivity)))\n",
    "outputSample = timeTotal\n",
    "\n",
    "#Find rows of data that contain at least one missing value\n",
    "missing = np.isnan(np.sum(inputSample,axis=1))\n",
    "\n",
    "reducedInput = inputSample[~missing]\n",
    "reducedOutput = outputSample[~missing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "\n",
    "X = reducedInput\n",
    "Y = reducedOutput\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "    X, Y,test_size=0.7, random_state=1)\n",
    "\n",
    "clf = svm.SVR(kernel='linear', C=1)\n",
    "\n",
    "Y_predict = clf.fit(X_train,Y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = reducedInput\n",
    "Y = reducedOutput\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "    X, Y,test_size=0.7, random_state=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "Y_predict = model.fit(X_train[:,1:],Y_train).predict(X_test[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = len(Y_predict)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.semilogy(X_test[a:b,0],Y_predict[a:b],'b.',label='Model result')\n",
    "plt.semilogy(X_test[a:b,0],Y_test[a:b],'r.',label='Actual result')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('Total Time [s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(((Y_predict - Y_test) ** 2).mean())\n",
    "nrmse = rmse/(Y_test.max()-Y_test.min())\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
