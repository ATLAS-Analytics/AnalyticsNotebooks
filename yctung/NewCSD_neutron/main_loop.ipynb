{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from sklearn import metrics\n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "import time\n",
    "import pandas\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "splitting data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('loading data')\n",
    "\n",
    "dataframe = pandas.read_csv(\"train_newcsd_neutron.txt\", delimiter=\",\", header=None)\n",
    "dataset=dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:33]\n",
    "Y = dataset[:,33]\n",
    "\n",
    "print('splitting data')\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15, random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building network\n"
     ]
    }
   ],
   "source": [
    "print('building network')\n",
    "\n",
    "\n",
    "\n",
    "class_weight = {0: 1.0,\n",
    "                1: 1.0,\n",
    "               }\n",
    "# used in unbalanced datasets \n",
    "\n",
    "\n",
    "neurons=256\n",
    "\n",
    "def create_model(drop=0.0,l2val=0.0):  #drop and l2val are going to be tuned later \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=33, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))    \n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    epochs = 300;\n",
    "    learning_rate = 0.001\n",
    "    decay_rate = learning_rate / epochs * 10\n",
    "    adam=optimizers.Adam(lr=learning_rate, decay=decay_rate, beta_1=0.9, beta_2=0.999, epsilon=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish building\n"
     ]
    }
   ],
   "source": [
    "l2val=[0.0001,0.0005,0.00005]\n",
    "drop=[0.1,0.3,0.5]\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=1024, verbose=0)\n",
    "\n",
    "\n",
    "param_grid = dict(l2val=l2val,drop=drop)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "#grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5) ## the defualt cross validation is 3\n",
    "\n",
    "print('finish building')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start tuning\n",
      "please do not disturb\n",
      "Best: 0.972910 using {'drop': 0.1, 'l2val': 0.0001}\n",
      "0.972910 (0.000234) with: {'drop': 0.1, 'l2val': 0.0001}\n",
      "0.972519 (0.000640) with: {'drop': 0.1, 'l2val': 0.0005}\n",
      "0.972445 (0.000908) with: {'drop': 0.1, 'l2val': 5e-05}\n",
      "0.966504 (0.002104) with: {'drop': 0.3, 'l2val': 0.0001}\n",
      "0.969237 (0.000940) with: {'drop': 0.3, 'l2val': 0.0005}\n",
      "0.968087 (0.001101) with: {'drop': 0.3, 'l2val': 5e-05}\n",
      "0.851452 (0.010874) with: {'drop': 0.5, 'l2val': 0.0001}\n",
      "0.883660 (0.010154) with: {'drop': 0.5, 'l2val': 0.0005}\n",
      "0.839410 (0.013836) with: {'drop': 0.5, 'l2val': 5e-05}\n"
     ]
    }
   ],
   "source": [
    "print('start tuning')\n",
    "print('please do not disturb')\n",
    "\n",
    "grid_result = grid.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']   #the arverage accuracy among the 3 folds \n",
    "stds = grid_result.cv_results_['std_test_score']   # the standard deviation. \n",
    "params = grid_result.cv_results_['params']   \n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    \n",
    "    \n",
    "# usually the highest mean with lowest std is the best\n",
    "# higher accuray -> better performance \n",
    "# higher std -> more likely overtraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neurons=256\n",
    "#lr is learning rate  # beta_1 is momentum, this one is not that important, you can only search for lr\n",
    "drop=0.5\n",
    "l2val=0.0005 #(use the values you just determined by last grid search )\n",
    "\n",
    "def create_model(lr=0.0,beta_1=0.0):  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=33, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    adam=optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=0.999, epsilon=0.001, decay=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = [0.001, 0.0008, 0.0006, 0.0005, 0.0004]\n",
    "beta_1 = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "model = KerasClassifier(build_fn=create_model, epochs=600, batch_size=2048, verbose=0)\n",
    "\n",
    "param_grid = dict(lr=lr, beta_1=beta_1)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############  you can also tune the number of neurons in each layer \n",
    "############  but this is usually the same as tuning l2 and drop (they are both directly related to the complexity of the strucutre)\n",
    "############  I think people usually fix the number of neurons and layers and reduce the complexity (reduce over-training) by tuning l2 and drop\n",
    "\n",
    "\n",
    "drop=0.5\n",
    "l2val=0.0005 \n",
    "lr=???\n",
    "beta_1 =???\n",
    "\n",
    "def create_model(neurons=128):  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=184, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(neurons, activation='relu',kernel_regularizer=regularizers.l2(l2val)))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    \n",
    "neurons = [64,128,256,512,1024]\n",
    "model = KerasClassifier(build_fn=create_model, epochs=600, batch_size=1024, verbose=0)\n",
    "\n",
    "param_grid = dict(neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
