{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chance: 0.96 \tcut: 0.962\n",
      "Processing: 192.101.161.186.h5\n",
      "(221098, 7)\n",
      "(3000, 7)\n",
      "2017-05-10 00:00:00 2017-05-12 02:01:00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 14)                112       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 105       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 225\n",
      "Trainable params: 225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "\n",
    "\n",
    "# tuning parameters\n",
    "ref = 24\n",
    "sub = 1\n",
    "path='../'\n",
    "\n",
    "chance = ref/(sub+ref)\n",
    "cut = chance + (1-chance) * 0.05\n",
    "print('chance:',chance, '\\tcut:', cut)\n",
    "ref = ref * Hour()\n",
    "sub = sub * Hour()\n",
    "\n",
    "\n",
    "def scaled_accuracy(accuracy, ref_samples, sub_samples):\n",
    "    chance = float(ref_samples)/(ref_samples+sub_samples)\n",
    "    rescale = 1/(1 - chance)\n",
    "    return (accuracy-chance)*rescale\n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, df, auc_df):\n",
    "        self.n_series = df.shape[1]\n",
    "        self.df = df\n",
    "        self.auc_df = auc_df\n",
    "        \n",
    "        self.nn = Sequential()\n",
    "        self.nn.add(Dense(units=self.n_series*2, input_shape=(self.n_series,), activation='relu' ))\n",
    "#       self.nn.add(Dropout(0.5))\n",
    "        self.nn.add(Dense(units=self.n_series, activation='relu'))\n",
    "#       self.nn.add(Dropout(0.5))\n",
    "        self.nn.add(Dense(units=1, activation='sigmoid'))\n",
    "#       self.nn.compile(loss='hinge', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "#       self.nn.compile(loss='mse',optimizer='rmsprop', metrics=['accuracy'])\n",
    "        self.nn.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "#       self.nn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "        self.nn.summary()\n",
    "    \n",
    "    def check_for_anomaly(self,ref, sub, count):\n",
    "    \n",
    "        y_ref = pd.Series([0] * ref.shape[0])\n",
    "        X_ref = ref\n",
    "    \n",
    "        y_sub = pd.Series([1] * sub.shape[0])\n",
    "        X_sub = sub\n",
    "        \n",
    "        # separate Reference and Subject into Train and Test\n",
    "        X_ref_train, X_ref_test, y_ref_train, y_ref_test = train_test_split(X_ref, y_ref, test_size=0.3, random_state=42)\n",
    "        X_sub_train, X_sub_test, y_sub_train, y_sub_test = train_test_split(X_sub, y_sub, test_size=0.3, random_state=42)\n",
    "    \n",
    "        # combine training ref and sub samples\n",
    "        X_train = pd.concat([X_ref_train, X_sub_train])\n",
    "        y_train = pd.concat([y_ref_train, y_sub_train])\n",
    "\n",
    "        # combine testing ref and sub samples\n",
    "        X_test = pd.concat([X_ref_test, X_sub_test])\n",
    "        y_test = pd.concat([y_ref_test, y_sub_test])\n",
    "    \n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "    \n",
    "        X_train_s, y_train_s = shuffle(X_train, y_train)\n",
    "    \n",
    "#         with tf.device('/gpu:0'):\n",
    "        hist = self.nn.fit(X_train_s.values, y_train_s.values, epochs=100, verbose=0, shuffle=True, batch_size=10)\n",
    "        loss_and_metrics = self.nn.evaluate(X_test.values, y_test.values)#, batch_size=256)\n",
    "        print(loss_and_metrics)\n",
    "        \n",
    "        return scaled_accuracy(loss_and_metrics[1], ref.shape[0], sub.shape[0])\n",
    "    \n",
    "    \n",
    "    def loop_over_intervals(self):\n",
    "        lstart = self.df.index.min()\n",
    "        lend = self.df.index.max()\n",
    "\n",
    "        #round start \n",
    "        lstart.seconds=0\n",
    "        lstart.minutes=0\n",
    "\n",
    "        # loop over them\n",
    "        ti = lstart + ref + sub\n",
    "        count = 0\n",
    "        while ti < lend + 1 * Minute():\n",
    "            print(count)\n",
    "            startt = time()\n",
    "            ref_start = ti-ref-sub\n",
    "            ref_end = ti-sub\n",
    "            ref_df = self.df[(self.df.index >= ref_start) & (self.df.index < ref_end)]\n",
    "            sub_df = self.df[(self.df.index >= ref_end) & (self.df.index < ti)]\n",
    "            if sub_df.shape[0]>60 * 0.7 and ref_df.shape[0]>24*60*0.7:\n",
    "                score = self.check_for_anomaly(ref_df, sub_df, count)\n",
    "            else:\n",
    "                score = 0\n",
    "            self.auc_df.loc[(self.auc_df.index >= ref_end) & (self.auc_df.index < ti), ['score']]  = score\n",
    "            print('\\n',ti,\"\\trefes:\" , ref_df.shape, \"\\tsubjects:\", sub_df.shape, '\\tacc:', score)\n",
    "            ti = ti + sub\n",
    "            print(\"took:\", time()-startt)\n",
    "            count = count + 1\n",
    "            #if count>2: break    \n",
    "    \n",
    "    \n",
    "while True:\n",
    "    objs = listdir(path)\n",
    "    toProcess=''\n",
    "    for o in objs:\n",
    "        if o.endswith('.h5') and \"res_\"+o not in objs and \"proc_\"+o not in objs:\n",
    "            toProcess=o\n",
    "            f  = open(path + 'proc_' + o, 'w')\n",
    "            f.write(str(time()))\n",
    "            f.close()\n",
    "            break\n",
    "    if toProcess=='': \n",
    "        break\n",
    "    \n",
    "    print('Processing:', toProcess)\n",
    "    full_df = pd.read_hdf(path+toProcess,'data')\n",
    "    print(full_df.shape)\n",
    "    full_df = full_df[:3000]\n",
    "    print(full_df.shape)\n",
    "    print (full_df.index.min(), full_df.index.max())\n",
    "    full_df.fillna(0, inplace=True)\n",
    "    auc_df = pd.DataFrame(np.nan, index=full_df.index, columns=['score'])\n",
    "    ann = ANN(full_df, auc_df)\n",
    "    ann.loop_over_intervals()\n",
    "                          \n",
    "    hdf = pd.HDFStore( 'res_' + o)\n",
    "    hdf.put('result', auc_df, format='table', data_columns=True)\n",
    "    hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
